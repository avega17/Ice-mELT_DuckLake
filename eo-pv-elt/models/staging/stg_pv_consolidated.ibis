"""
dbt-ibis staging model for consolidated PV installations.

This model instantiates a Hamilton driver to create staging tables if they don't exist,
then returns the consolidated table as an Ibis expression for dbt materialization.

This approach keeps everything within the dbt build process while leveraging Hamilton
for the complex staging transformations.
"""

from dbt_ibis import depends_on, source
import sys
import os
from pathlib import Path

# Add repo root to path for Hamilton imports
repo_root = Path(os.getenv('REPO_ROOT', '.')).resolve()
sys.path.insert(0, str(repo_root))

@depends_on()
def model():
    """
    Create consolidated PV table using Hamilton driver within dbt-ibis.

    This approach:
    1. Checks if staging tables exist in DuckLake catalog
    2. If not, runs Hamilton consolidation to create them
    3. Returns the consolidated table as an Ibis expression

    Returns:
        Ibis table expression with consolidated PV installations
    """
    import ibis
    from hamilton import driver
    from hamilton.execution import executors

    print("üöÄ dbt-ibis staging: Creating consolidated table with Hamilton integration...")

    try:
        # Use dbt's existing connection (which has DuckLake attached via on-run-start hooks)
        con = ibis.get_backend()

        # Ensure DuckLake catalog is attached and active
        try:
            # Check if eo_pv_lakehouse database is available
            databases = con.list_databases()
            if 'eo_pv_lakehouse' not in databases:
                # Attach DuckLake catalog if not already attached
                catalog_path = str(repo_root / "db" / "ducklake_catalog.sqlite")
                data_path = str(repo_root / "db" / "ducklake_data")

                con.raw_sql("INSTALL ducklake")
                con.raw_sql("LOAD ducklake")
                con.raw_sql(f"ATTACH 'ducklake:sqlite:{catalog_path}' AS eo_pv_lakehouse (DATA_PATH '{data_path}/')")
                print("   üîó Attached DuckLake catalog to dbt connection")

            # Use the eo_pv_lakehouse database
            con.raw_sql("USE eo_pv_lakehouse")
            print("   üìä Using eo_pv_lakehouse database")

        except Exception as e:
            print(f"   ‚ö†Ô∏è  DuckLake setup warning: {e}")

        # Check if staging tables exist
        staging_tables = [
            "stg_chn_med_res_pv_2024",
            "stg_global_harmonized_large_solar_farms_2020",
            "stg_global_pv_inventory_sent2_spot_2021",
            "stg_ind_pv_solar_farms_2022",
            "stg_uk_crowdsourced_pv_2020",
            "stg_usa_cali_usgs_pv_2016"
        ]

        available_tables = con.list_tables()
        existing_staging_tables = [t for t in staging_tables if t in available_tables]

        print(f"   üìã Found {len(existing_staging_tables)} existing staging tables: {existing_staging_tables}")

        # If no staging tables exist, run Hamilton consolidation to create them
        if len(existing_staging_tables) == 0:
            print("   üîß No staging tables found, running Hamilton consolidation...")

            # Import Hamilton staging modules
            from dataflows.stg.consolidation import stg_doi_pv_consolidation

            # Determine the correct database path based on dbt target
            # Check if we're running against MotherDuck (prod) or local (dev)
            try:
                current_db_path = str(con.con.execute("SELECT current_database()").fetchone()[0])
                print(f"   üîç Current database path: {current_db_path}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Could not detect current database: {e}")
                current_db_path = ""

            # Check environment variables to determine target
            ducklake_name = os.getenv('DUCKLAKE_NAME', '')
            motherduck_token = os.getenv('MOTHERDUCK_TOKEN', '')

            print(f"   üîç DUCKLAKE_NAME: {ducklake_name}")
            print(f"   üîç MOTHERDUCK_TOKEN: {'***' if motherduck_token else 'not set'}")

            if motherduck_token and ('md:' in current_db_path or 'motherduck' in current_db_path.lower() or ducklake_name):
                # Production: Use MotherDuck + Neon PostgreSQL catalog
                database_path = os.getenv('DUCKLAKE_PROD_CONN')
                if not database_path:
                    # Fallback to constructing from environment
                    pg_conn = os.getenv('NEON_PG_CONN')
                    if pg_conn:
                        database_path = f"ducklake:postgres:{pg_conn.replace('postgresql://', '')}"
                    else:
                        database_path = 'ducklake:postgres:dbname=eo_pv_lakehouse host=ep-broad-rain-a4tdwnxn-pooler.us-east-1.aws.neon.tech port=5432 user=neondb_owner sslmode=require'
                print(f"   üåê Using production catalog: MotherDuck + Neon PostgreSQL")
                print(f"   üîó Database path: {database_path[:50]}...")
            else:
                # Development: Use local SQLite catalog
                database_path = f"ducklake:sqlite:{str(repo_root / 'db' / 'ducklake_catalog.sqlite')}"
                print(f"   üíª Using development catalog: Local SQLite")
                print(f"   üîó Database path: {database_path}")

            # Create Hamilton configuration
            config = {
                'database_path': database_path,
                'target_table': 'stg_pv_consolidated',
                'h3_resolution': 8,
                'overlap_threshold': 0.75,
                'buffer_distance_m': 100,
                'memory_limit_gb': 8,
                'enable_spatial_index': True,
                'enable_hive_partitioning': True,
                'export_geoparquet': True,
                'export_csv': False,
                'compression': 'snappy',
                'repo_root': str(repo_root),
                'output_dir': str(repo_root / "output" / "staging"),
                'log_level': 'INFO',
                'verbose': False,
                'execution_mode': 'sequential'  # Use sequential for dbt integration
            }

            print(f"   üìä Hamilton config database_path: {config['database_path']}")

            # Create Hamilton driver
            dr = driver.Builder()\
                .with_modules(stg_doi_pv_consolidation)\
                .with_config(config)\
                .build()

            # Run Hamilton consolidation
            final_outputs = ['staging_table_created']
            results = dr.execute(final_outputs)

            print(f"   ‚úÖ Hamilton consolidation completed: {results}")

            # Refresh available tables list
            available_tables = con.list_tables()
            existing_staging_tables = [t for t in staging_tables if t in available_tables]
            print(f"   üìã After Hamilton: {len(existing_staging_tables)} staging tables available")

        # Create union of all available staging tables
        if not existing_staging_tables:
            raise ValueError("No staging tables available after Hamilton consolidation")

        tables_to_union = []
        for table_name in existing_staging_tables:
            table = con.table(table_name, database="eo_pv_lakehouse.main")
            tables_to_union.append(table)
            print(f"      - {table_name}: {table.count().execute():,} records")

        # Union all tables
        if len(tables_to_union) == 1:
            consolidated_table = tables_to_union[0]
        else:
            consolidated_table = tables_to_union[0]
            for table in tables_to_union[1:]:
                consolidated_table = consolidated_table.union(table)

        total_records = consolidated_table.count().execute()
        print(f"   ‚úÖ Consolidated table created: {total_records:,} total records")
        print(f"      - Columns: {len(consolidated_table.columns)}")

        # Return the Ibis table expression for dbt materialization
        return consolidated_table

    except ImportError as e:
        print(f"‚ùå Failed to import Hamilton modules: {e}")
        print(f"   Make sure repo root is in path: {repo_root}")
        raise
    except Exception as e:
        print(f"‚ùå Error in Hamilton consolidation: {e}")
        raise
