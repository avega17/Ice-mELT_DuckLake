{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8564ba95",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "import ipywidgets as widgets\n",
    "from jupyter_bbox_widget import BBoxWidget\n",
    "from ipywidgets import Layout, interact\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from branca.colormap import linear\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm # To specify colormaps\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "import geopandas as gpd\n",
    "import pycountry\n",
    "from shapely import wkt, Polygon\n",
    "import folium\n",
    "import lonboard\n",
    "import pydeck as pdk\n",
    "# import openeo \n",
    "# import pystac_client\n",
    "# import easystac\n",
    "# import cubo\n",
    "\n",
    "import duckdb\n",
    "import datahugger\n",
    "import sciencebasepy\n",
    "from seedir import seedir\n",
    "\n",
    "# python libraries\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import urllib.parse\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import tempfile\n",
    "import shutil\n",
    "import pprint as pp\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from zipfile import ZipFile\n",
    "import random\n",
    "from typing import Optional, List, Dict, Tuple, Any\n",
    "\n",
    "# Import refactored utility functions\n",
    "from utils.fetch_and_preprocess import (\n",
    "    fetch_dataset_files, \n",
    "    filter_gdf_duplicates, \n",
    "    process_vector_geoms, \n",
    "    geom_db_consolidate_dataset,\n",
    "    ddb_filter_duplicates\n",
    ")\n",
    "from utils.visualizations import (\n",
    "    format_dataset_info,\n",
    "    create_folium_cluster_map,\n",
    "    create_folium_choropleth,\n",
    "    create_folium_heatmap,\n",
    "    create_pydeck_scatterplot,\n",
    "    create_pydeck_polygons,\n",
    "    create_pydeck_heatmap\n",
    ")\n",
    "\n",
    "from utils.st_context_processing import (\n",
    "    add_h3_index_to_pv_labels,\n",
    "    ddb_alter_table_add_h3,\n",
    "    ddb_save_div_matches,\n",
    "    ddb_save_subtype_geoms,\n",
    "    get_duckdb_connection,\n",
    "    group_pv_by_h3_cells,\n",
    "    spatial_join_stac_items_with_h3,\n",
    "    create_h3_stac_fetch_plan,\n",
    "    fetch_overture_maps_theme,\n",
    "    spatial_join_pv_overture_duckdb\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f645e0",
   "metadata": {},
   "source": [
    "# Fetch Open, Published Datasets of PV Solar Panel locations across the world\n",
    "\n",
    "Many of these datasets are located in [Zenodo](https://zenodo.org/), a general-purpose open-access repository developed under the European OpenAIRE program and operated by CERN. Others are hosted in figshare, a web-based platform for sharing research data and other types of content. The rest are hosted in GitHub repositories or other open-access data platforms.\n",
    "\n",
    "The datasets are available in a variety of formats, including CSV, GeoJSON, GeoPackage, ESRI shapefiles, raw raster masks, and *[GeoParquet](#Intro-to-GeoParquet)*. We'll be using open-source Python libraries to download and process them into properly georeferenced geoparquet files that we'll manage using [dbt-core+duckdb](https://motherduck.com/blog/motherduck-duckdb-dbt/) (see these short videos on [this stack](https://www.youtube.com/watch?v=asxGh2TrNyI) and [MotherDuck](https://www.youtube.com/watch?v=OuCY7_DzCTA) to scale compute and storage in the cloud *with the same tools and codebase*.\n",
    "\n",
    "\n",
    "<!-- prev note for Georectified USA (Note: these correspond to PV _facilities_ rather than individual panel arrays or objects and need filtering of duplicates with other datasets and further processing to extract the PV arrays in the facility) -->\n",
    "\n",
    "Here we list the dataset titles of publications alongside their first author, DOI links, and their number of labels:\n",
    "- **\"Distributed solar photovoltaic array location and extent dataset for remote sensing object identification\"** - K. Bradbury, 2016 | [paper DOI](https://doi.org/10.1038/sdata.2016.106) | [dataset DOI](https://doi.org/10.6084/m9.figshare.3385780.v4) | polygon annotations for 19,433 PV modules in 4 cities in California, USA\n",
    "- \"A solar panel dataset of very high resolution satellite imagery to support the Sustainable Development Goals\" - C. Clark et al, 2023 | [paper DOI](https://doi.org/10.1038/s41597-023-02539-8) | [dataset DOI](https://doi.org/10.6084/m9.figshare.22081091.v3) | 2,542 object labels (per spatial resolution)\n",
    "- **\"A harmonised, high-coverage, open dataset of solar photovoltaic installations in the UK\" - D. Stowell et al, 2020** | [paper DOI](https://doi.org/10.1038/s41597-020-00739-0) | [dataset DOI](https://zenodo.org/records/4059881) | 265,418 data points (over 255,000 are stand-alone installations, 1067 solar farms, and rest are subcomponents within solar farms)\n",
    "- \"Georectified polygon database of ground-mounted large-scale solar photovoltaic sites in the United States\" - K. Sydny, 2023 | [paper DOI](https://doi.org/10.1038/s41597-023-02644-8) | [dataset DOI](https://www.sciencebase.gov/catalog/item/6671c479d34e84915adb7536) | 4186 data points \n",
    "- \"Vectorized solar photovoltaic installation dataset across China in 2015 and 2020\" - J. Liu et al, 2024 | [paper DOI](https://doi.org/10.1038/s41597-024-04356-z) | [dataset link](https://github.com/qingfengxitu/ChinaPV) | 3,356 PV labels (inspect quality!)\n",
    "- \"Multi-resolution dataset for photovoltaic panel segmentation from satellite and aerial imagery\" - H. Jiang, 2021 | [paper DOI](https://doi.org/10.5194/essd-13-5389-2021) | [dataset DOI](https://doi.org/10.5281/zenodo.5171712) | 3,716 samples of PV data points\n",
    "- \"A crowdsourced dataset of aerial images with annotated solar photovoltaic arrays and installation metadata\" - G. Kasmi, 2023 | [paper DOI](https://doi.org/10.1038/s41597-023-01951-4) | [dataset DOI](https://doi.org/10.5281/zenodo.6865878) | > 28K points of PV installations; 13K+ segmentation masks for PV arrays; metadata for 8K+ installations\n",
    "- **\"An Artificial Intelligence Dataset for Solar Energy Locations in India\"** - A. Ortiz, 2022 | [paper DOI](https://doi.org/10.1038/s41597-022-01499-9) | [dataset link 1](https://researchlabwuopendata.blob.core.windows.net/solar-farms/solar_farms_india_2021.geojson) or [dataset link 2](https://raw.githubusercontent.com/microsoft/solar-farms-mapping/refs/heads/main/data/solar_farms_india_2021_merged_simplified.geojson) | 117 geo-referenced points of solar installations across India\n",
    "- \"GloSoFarID: Global multispectral dataset for Solar Farm IDentification in satellite imagery\" - Z. Yang, 2024** | [paper DOI](https://doi.org/10.48550/arXiv.2404.05180) | [dataset DOI](https://github.com/yzyly1992/GloSoFarID/tree/main/data_coordinates) | 6,793 PV samples across 3 years (double counting of samples)\n",
    "- **\"A global inventory of photovoltaic solar energy generating units\" - L. Kruitwagen et al, 2021** | [paper DOI](https://doi.org/10.1038/s41586-021-03957-7) | [dataset DOI](https://doi.org/10.5281/zenodo.5005867) | 50,426 for training, cross-validation, and testing; 68,661 predicted polygon labels \n",
    "- **\"Harmonised global datasets of wind and solar farm locations and power\" - S. Dunnett et al, 2020** | [paper DOI](https://doi.org/10.1038/s41597-020-0469-8) | [dataset DOI](https://doi.org/10.6084/m9.figshare.11310269.v6) | 35272 PV installations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04686807",
   "metadata": {},
   "source": [
    "# Dataset Metadata and Preparation\n",
    "\n",
    "We will use datahugger, sciencebasepy, and osf-client to manage official dataset records published in open access scientific data repositories.\n",
    "We also implement some ad-hoc functions to download some data assets hosted in GitHub repositories. \n",
    "We will also use geopandas, rasterio, and pyproj to process the datasets into georeferenced geoparquet files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3aad185",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load environment variables\n",
    "load_dotenv()\n",
    "DATASET_DIR = Path(os.getenv('DATA_PATH'))\n",
    "# read dataset metadata from json file\n",
    "with open('dataset_metadata.json', 'r') as f:\n",
    "    dataset_metadata = json.load(f)\n",
    "\n",
    "dataset_choices = [\n",
    "    'global_harmonized_large_solar_farms_2020',\n",
    "    # 'global_pv_inventory_sent2_2024',\n",
    "    'global_pv_inventory_sent2_spot_2021',\n",
    "    # 'fra_west_eur_pv_installations_2023',\n",
    "    'ind_pv_solar_farms_2022',\n",
    "    'usa_cali_usgs_pv_2016',\n",
    "    # 'chn_med_res_pv_2024',\n",
    "    # 'usa_eia_large_scale_pv_2023',\n",
    "    'uk_crowdsourced_pv_2020',\n",
    "    # 'deu_maxar_vhr_2023'   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6162a17b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize a list to store selected datasets\n",
    "# mostly gen by github copilot with Claude 3.7 model\n",
    "selected_datasets = dataset_choices.copy()\n",
    "\n",
    "# Create an accordion to display selected datasets with centered layout\n",
    "dataset_accordion = widgets.Accordion(\n",
    "    children=[widgets.HTML(format_dataset_info(ds)) for ds in selected_datasets],\n",
    "    layout=Layout(width='50%', margin='0 auto')\n",
    ")\n",
    "for i, ds in enumerate(selected_datasets):\n",
    "    dataset_accordion.set_title(i, ds)\n",
    "\n",
    "# Define a function to add or remove datasets\n",
    "def manage_datasets(action, dataset=None):\n",
    "    global selected_datasets, dataset_accordion\n",
    "    \n",
    "    if action == 'add' and dataset and dataset not in selected_datasets:\n",
    "        selected_datasets.append(dataset)\n",
    "    elif action == 'remove' and dataset and dataset in selected_datasets:\n",
    "        selected_datasets.remove(dataset)\n",
    "    \n",
    "    # Update the accordion with current selections\n",
    "    dataset_accordion.children = [widgets.HTML(format_dataset_info(ds)) for ds in selected_datasets]\n",
    "    for i, ds in enumerate(selected_datasets):\n",
    "        dataset_accordion.set_title(i, ds)\n",
    "    \n",
    "    f\"Currently selected datasets: {len(selected_datasets)}\"\n",
    "\n",
    "# Create dropdown for available datasets\n",
    "dataset_dropdown = widgets.Dropdown(\n",
    "    options=list(dataset_metadata.keys()),\n",
    "    description='Dataset:',\n",
    "    disabled=False,\n",
    "    layout=Layout(width='70%', margin='20 20 auto 20 20')\n",
    ")\n",
    "\n",
    "# Create buttons for actions\n",
    "add_button = widgets.Button(description=\"Add Dataset\", button_style='success')\n",
    "remove_button = widgets.Button(description=\"Remove Dataset\", button_style='danger')\n",
    "\n",
    "# Define button click handlers\n",
    "def on_add_clicked(b):\n",
    "    manage_datasets('add', dataset_dropdown.value)\n",
    "\n",
    "def on_remove_clicked(b):\n",
    "    manage_datasets('remove', dataset_dropdown.value)\n",
    "\n",
    "# Link buttons to handlers\n",
    "add_button.on_click(on_add_clicked)\n",
    "remove_button.on_click(on_remove_clicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f44fde4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Dataset Selection Interface\n",
    "### Use the dropdown and buttons below to customize which solar panel datasets will be fetched and processed.\n",
    "- Select a dataset from the dropdown:\n",
    "    - Click \"Add Dataset\" to include it in processing\n",
    "    - Click \"Remove Dataset\" to exclude it\n",
    "- View metadata table for each selected dataset by clicking on it's row in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb943c7b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f92aeff1cc4462a88b36f7d25fc266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Dropdown(description='Dataset:', layout=Layout(margin='20 20 auto 20 20', width='70%'), options…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "016a5b3e624e418789c7a2021d176d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(HTML(value='\\n    <style>\\n    .dataset-table {\\n        border-collapse: collapse;\\n     …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the widgets\n",
    "display(widgets.HBox([dataset_dropdown, add_button, remove_button]))\n",
    "display(dataset_accordion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a78984",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Fetching and Organizing datasets for later-preprocessing\n",
    "\n",
    "Using:\n",
    "- [datahugger](https://j535d165.github.io/datahugger/) to fetch datasets hosted in Zenodo, figshare.\n",
    "- sciencebasepy for the dataset hosted in the USGS ScienceBase Catalog.\n",
    "- github datasets will be fetched using ad hoc functions.\n",
    "<!-- We will use osf-client to fetch datasets hosted in the Open Science Framework (OSF). -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eece7e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# iterate through the selected datasets and fetch files\n",
    "# iterate through the selected datasets and fetch files\n",
    "ds_trees = {}\n",
    "max_mb = int(os.getenv('MAX_LABEL_MB', 100))\n",
    "\n",
    "# Create widgets for controlling the fetching process\n",
    "fetch_output = widgets.Output(\n",
    "    layout=widgets.Layout(\n",
    "        width='80%', \n",
    "        border='1px solid #ddd', \n",
    "        padding='10px',\n",
    "        overflow='auto'\n",
    "    )\n",
    ")\n",
    "# Apply direct CSS styling for text wrapping (Note: unvalidated)\n",
    "display(widgets.HTML(\"\"\"\n",
    "<style>\n",
    ".jupyter-widgets-output-area pre {\n",
    "    white-space: pre-wrap !important;       /* CSS3 */\n",
    "    word-wrap: break-word !important;        /* Internet Explorer 5.5+ */\n",
    "    overflow-wrap: break-word !important;\n",
    "    max-width: 100%;\n",
    "}\n",
    "</style>\n",
    "\"\"\"))\n",
    "control_panel = widgets.VBox(layout=widgets.Layout(width='20%', padding='10px', overflow='auto', word_wrap='break-word'))\n",
    "fetch_button = widgets.Button(description=\"Fetch Next Dataset\", button_style=\"primary\")\n",
    "progress_label = widgets.HTML(\"Waiting to start...\")\n",
    "dataset_index = 0\n",
    "\n",
    "# Function to fetch the next dataset\n",
    "def fetch_next_dataset(button=None):\n",
    "    global dataset_index\n",
    "    global dataset_metadata\n",
    "    \n",
    "    if dataset_index >= len(selected_datasets):\n",
    "        with fetch_output:\n",
    "            print(\"All datasets have been fetched!\")\n",
    "            progress_label.value = f\"<b>Completed:</b> {dataset_index}/{len(selected_datasets)} datasets\"\n",
    "        fetch_button.disabled = True\n",
    "        return\n",
    "    \n",
    "    dataset = selected_datasets[dataset_index]\n",
    "    progress_label.value = f\"<b>Fetching:</b> {dataset_index+1}/{len(selected_datasets)}<br><b>Current:</b> {dataset}\"\n",
    "    \n",
    "    with fetch_output:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Fetching dataset files for {dataset} using DOI/URL:\\n {dataset_metadata[dataset]['doi']}\")\n",
    "        ds_tree = fetch_dataset_files(dataset, dataset_metadata[dataset], max_mb=max_mb, force=force_download_checkbox.value)\n",
    "        \n",
    "        \n",
    "        if ds_tree:\n",
    "            ds_trees[dataset] = ds_tree\n",
    "            # update metadata dict with local filesystem info\n",
    "            dataset_metadata[dataset]['output_folder'] = ds_tree['output_folder']\n",
    "            dataset_metadata[dataset]['files'] = ds_tree['files']\n",
    "            dataset_metadata[dataset]['fs_tree'] = ds_tree['fs_tree']\n",
    "            # print the dataset file tree\n",
    "        else:\n",
    "            print(f\"Failed to fetch dataset {dataset}\")\n",
    "    \n",
    "    dataset_index += 1\n",
    "    progress_label.value = f\"<b>Completed:</b> {dataset_index}/{len(selected_datasets)}<br><b>Next:</b> {selected_datasets[dataset_index] if dataset_index < len(selected_datasets) else 'Done'}\"\n",
    "\n",
    "# Add a checkbox for force download option\n",
    "force_download_checkbox = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Force Download',\n",
    "    tooltip='If checked, download will be forced even if files exist locally',\n",
    "    layout=widgets.Layout(width='auto')\n",
    ")\n",
    "\n",
    "# Configure the button callback\n",
    "fetch_button.on_click(fetch_next_dataset)\n",
    "\n",
    "# Create the control panel\n",
    "dataset_progress = widgets.HTML(f\"Datasets selected: {len(selected_datasets)}\")\n",
    "fetch_status = widgets.HTML(\n",
    "    f\"Status: Ready to begin\",\n",
    "    layout=widgets.Layout(margin=\"10px 0\")\n",
    ")\n",
    "\n",
    "# Create the control panel with left alignment\n",
    "control_panel.children = [\n",
    "    widgets.HTML(\"<h3 style='align:left;'>Fetch Control</h3>\"), \n",
    "    dataset_progress,\n",
    "    force_download_checkbox,\n",
    "    widgets.HTML(\"<hr style='margin:10px 0'>\"),\n",
    "    progress_label,\n",
    "    fetch_button\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f56a89",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Dataset Fetching UI widget \n",
    "\n",
    "- You can sequentially go through the selected datasets above \n",
    "- Initiate the download with the fetch button \n",
    "- Files that are already downloaded will be skipped unless the force redownload checkbox is checked \n",
    "- You can inspect each download's std out and stderr logs in the output area below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa103f33",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add custom CSS to ensure alignment\n",
    "display(widgets.HTML(\"\"\"\n",
    "<style>\n",
    ".widget-html {\n",
    "    text-align: left !important;\n",
    "}\n",
    ".widget-checkbox {\n",
    "    align-items: left !important;\n",
    "}\n",
    ".widget-button {\n",
    "    width: 100% !important;\n",
    "}\n",
    "</style>\n",
    "\"\"\"))\n",
    "# Display the widget layout\n",
    "display(widgets.HBox([fetch_output, control_panel]))\n",
    "\n",
    "# Set up for first fetch\n",
    "if selected_datasets:\n",
    "    progress_label.value = f\"<b>Ready to start:</b><br>First dataset: {selected_datasets[0]}\"\n",
    "else:\n",
    "    progress_label.value = \"<b>No datasets selected</b>\"\n",
    "    fetch_button.disabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ddc7a4",
   "metadata": {},
   "source": [
    "### Fetching selected datasets and visualizing metadata and file structure\n",
    "\n",
    "Use the simple UI rendered below to click the \"Next Dataset\" button to initiate the fetching of the selected datasets above. \n",
    "The datasets will avoid redownloading existing files, but the user can force a re-download by checking the \"Force Re-download\" checkbox. \n",
    "\n",
    "We will store the datasets in the `DATA_PATH` variable configured in the repo's `.env` file in the `raw/` subdirectory (to denote data fetched and saved as-is or with minimal processing).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e2f6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if files and fs_tree are empty for selected datasets (ie user did not use fetch UI above or there was an error with one of the fetches)\n",
    "processed_ds_keys = ['output_folder', 'files', 'fs_tree']\n",
    "missing_ds_fetches = [ds for ds in selected_datasets if any(key not in dataset_metadata[ds] for key in processed_ds_keys)]\n",
    "if missing_ds_fetches:\n",
    "    print(f\"Warning: The following datasets were not fetched or processed correctly:\\n{missing_ds_fetches}\")\n",
    "    print(f\"We will attempt to fetch these in a loop in the next cell, but if you did not skip the fetch step, please check the stdout above for errors.\")\n",
    "else:\n",
    "    print(f\"All {len(selected_datasets)} datasets have been fetched and processed correctly (🤞).\")\n",
    "\n",
    "if missing_ds_fetches:\n",
    "    # loop through the datasets that were not fetched and try to fetch them again\n",
    "    for dataset in missing_ds_fetches:\n",
    "        print(f\"Attempting to fetch {dataset} again...\")\n",
    "        ds_tree = fetch_dataset_files(dataset, dataset_metadata[dataset], max_mb=max_mb, force=False)\n",
    "        \n",
    "        if ds_tree:\n",
    "            ds_trees[dataset] = ds_tree\n",
    "            # update metadata dict with local filesystem info\n",
    "            dataset_metadata[dataset]['output_folder'] = ds_tree['output_folder']\n",
    "            dataset_metadata[dataset]['files'] = ds_tree['files']\n",
    "            dataset_metadata[dataset]['fs_tree'] = ds_tree['fs_tree']\n",
    "        else:\n",
    "            print(f\"Failed to fetch dataset {dataset} again; Error is likely in the fetch function above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218e707e",
   "metadata": {},
   "source": [
    "#### Global inventory of solar PV units (Kruitwagen et al, 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee193e4",
   "metadata": {},
   "source": [
    "From Zenodo:\n",
    "```\n",
    "Repository contents:\n",
    "\n",
    "trn_tiles.geojson: 18,570 rectangular areas-of-interest used for sampling training patch data.\n",
    "\n",
    "trn_polygons.geojson: 36,882 polygons obtained from OSM in 2017 used to label training patches.\n",
    "\n",
    "cv_tiles.geojson: 560 rectangular areas-of-interest used for sampling cross-validation data seeded from WRI GPPDB\n",
    "\n",
    "cv_polygons.geojson: 6,281 polygons corresponding to all PV solar generating units present in cv_tiles.geojson at the end of 2018.\n",
    "\n",
    "test_tiles.geojson: 122 rectangular regions-of-interest used for building the test set.\n",
    "\n",
    "test_polygons.geojson: 7,263 polygons corresponding to all utility-scale (>10kW) solar generating units present in test_tiles.geojson at the end of 2018.\n",
    "\n",
    "predicted_polygons.geojson: 68,661 polygons corresponding to predicted polygons in global deployment, capturing the status of deployed photovoltaic solar energy generating capacity at the end of 2018.\n",
    "``` \n",
    "\n",
    "**Regarding predicted_polygons.geojson**: \"The final dataset includes 68,661 detections in 131 countries with a mean detection\n",
    "area of approximately 70,000m2. Country-level aggregates are shown in Supplementary\n",
    "Table 10. Any false positives remaining in the dataset are a product of human error. Final precision statistics in Supplementary Figure 6 and Supplementary Table ?? is reported\n",
    "again the test set. We find human error in hand labelling reduced final precision to approximately 98.6%.\" \n",
    "\n",
    "On the confidence column only in that file: \"Sentinel-2 and SPOT pipeline branches are combined into a final\n",
    "vector dataset using a rules-based filter. Where Sentinel-2 and SPOT polygons intersect\n",
    "with a Jaccard index (Intersection-over-union) in excess of 30%, the geometry of the SPOT\n",
    "polygon is retained, inheriting the installation date from the Sentinel-2 detection (confidence level “A”). Detections from only the SPOT and S2 branches are retained with confidence levels “B” and “C” respectively. Where the IoU does not exceed 30%, the union of both geometries are retained, inheriting the S2 installation date (confidence “D”).\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2873bb-a0c1-4969-add2-3da160ad0226",
   "metadata": {},
   "source": [
    "#### Harmonised, high-coverage PV dataset of UK (Stowell et al, 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505e2535",
   "metadata": {},
   "source": [
    "#### France Western Europe PV Installations 2023 (Kasmi et al, 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d566f477",
   "metadata": {},
   "source": [
    "From [research publication](https://doi.org/10.1038/s41597-023-01951-4): \n",
    "```\n",
    "The Git repository contains the raw crowdsourcing data and all the material necessary to re-generate our training dataset and technical validation.  \n",
    "It is structured as follows: the raw subfolder contains the raw annotation data from the two annotation campaigns and the raw PV installations’ metadata.  \n",
    "The replication subfolder contains the compiled data used to generate our segmentation masks.  \n",
    "The validation subfolder contains the compiled data necessary to replicate the analyses presented in the technical validation section.\n",
    "```\n",
    "\n",
    "We will be using the `replication` subfolder to generate our PV polygons geojson file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503fc6be",
   "metadata": {},
   "source": [
    "### Manual dataset file curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7659bb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# keep subset of metadata dict for selected datasets\n",
    "selected_metadata = {ds: dataset_metadata[ds] for ds in selected_datasets}\n",
    "get_ds_files = lambda ds: dataset_metadata[ds]['files']\n",
    "get_ds_dir = lambda ds: dataset_metadata[ds]['output_folder']\n",
    "is_ds_ftype = lambda ds, fname: fname.endswith(f\".{dataset_metadata[ds]['label_fmt']}\")\n",
    "get_full_ds_path = lambda ds: DATASET_DIR / 'raw' / 'labels' / ds\n",
    "fra_ds_folder = 'replication'\n",
    "\n",
    "# TODO: refactor this to a function as it'll quickly get out of hand with more datasets and pruning required\n",
    "# make a manual selection of the set of files we'll use from each dataset\n",
    "selected_ds_files = {ds : [f for f in get_ds_files(ds) if is_ds_ftype(ds, f)] for ds in selected_datasets}\n",
    "\n",
    "# ad hoc selection of files for testing (keep files that contain 'solar' and 'WGS84' in filename)\n",
    "selected_ds_files['global_harmonized_large_solar_farms_2020'] = [f for f in selected_ds_files['global_harmonized_large_solar_farms_2020'] if 'solar' in f.split('/')[-1] and 'WGS84' in f and not os.path.isdir(f)]\n",
    "# prediction dataset was human verified thoroughly and meant for downstream applications; only use this file for now\n",
    "selected_ds_files['global_pv_inventory_sent2_spot_2021'] = [f for f in selected_ds_files['global_pv_inventory_sent2_spot_2021'] if 'predicted' in os.path.basename(f)]\n",
    "print(f\"Selected {len(selected_ds_files['global_pv_inventory_sent2_spot_2021'])} files for {selected_datasets[0]}:\\n{selected_ds_files['global_pv_inventory_sent2_spot_2021']}\")\n",
    "\n",
    "# only include files that were not filtered out\n",
    "include_files = [os.path.basename(f) for ds in selected_datasets for f in selected_ds_files[ds]]\n",
    "# don't print out unused directories\n",
    "exclude_folders = [os.path.basename(dir) for dir in os.listdir(DATASET_DIR / 'raw' / 'labels') if dir not in selected_datasets]\n",
    "\n",
    "# build and output tree for selected datasets\n",
    "selected_ds_dirs = [get_ds_dir(ds) for ds in selected_datasets]\n",
    "print(\"All selected datasets have been fetched with the following file tree:\\n\")\n",
    "# TODO: fix unwanted dirs in the tree\n",
    "selected_ds_tree = seedir(DATASET_DIR / 'raw' / 'labels', depthlimit=10, printout=True, regex=False, include_files=include_files, exclude_folders=exclude_folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db04ae2e",
   "metadata": {},
   "source": [
    "# Process into deduplicated and consolidated Database and GeoParquet\n",
    "1. We will use the `geopandas` library to read the raw dataset files and convert each dataset into a single geoparquet file\n",
    "2. We will use this list of geoparquet files to create a consolidated duckdb database file\n",
    "3. We will perform spatial deduplication (removing overlapping polygons from different datasets) and save the consolidated geoparquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f011a0",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 60%; margin: 0 auto; padding-left: 1em; padding-right: 1em; text-align: justify;\">\n",
    "<h2 style=\"text-align: left\">GeoParquet: Intro</h2>\n",
    "\n",
    "<p>GeoParquet is <a href=\"https://geoparquet.org/\">an incubating Open Geospatial Consortium (OGC) standard</a> that simply adds compatible geospatial <a href=\"https://docs.safe.com/fme/html/FME-Form-Documentation/FME-ReadersWriters/geoparquet/Geometry-Support.htm\">geometry types</a> (Point, Line, Polygon, etc) to the mature and widely adopted <a href=\"https://parquet.apache.org/\">Apache Parquet format</a>, a popular columnar storage file format commonly used in big data processing and modern data engineering pipelines and analytics. This is analogous to how the GeoTIFF raster format adds geospatial metadata to the longstanding TIFF standard. GeoParquet is designed to be a simple and efficient way to store geospatial <em>vector</em> data in a columnar format, and is designed to be compatible with existing Parquet tools and libraries to enable Cloud <em>Data Warehouse</em> Interoperability.</p>\n",
    "\n",
    "<figure style=\"text-align: center\">\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*QEQJjtnDb3JQ2xqhzARZZw.png\" style=\"width:70%; height:auto;\">\n",
    "<figcaption align = \"center\"> Visualization of the layout of a Parquet file </figcaption>\n",
    "</figure>\n",
    "\n",
    "<p>Parquet is optimized for analytical workloads (i.e. <strong>not transactional</strong>) and is designed to work well with large-scale data processing frameworks like Apache Spark, Dask, and Apache Airflow. It is a <em>popular choice for storing large datasets using modern cloud-centric DBMS architectures</em> like data lakes and data warehouses. Parquet files are <strong>designed to be highly compressed</strong>, which reduces storage costs and improves performance when reading and writing data. The columnar format allows for efficient compression algorithms to be <em>applied to each column independently</em>, resulting in better compression ratios compared to traditional row-based formats like CSV or JSON.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3a3226",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 60%; margin: 0 auto; padding-left: 1em; padding-right: 1em; text-align: justify;\">\n",
    "<h2 style=\"text-align: left\">GeoParquet: Features & Performance</h2>\n",
    "\n",
    "<p>These files are organized in a set of file chunks called \"row groups\". Row groups are logical groups of columns with the same number of rows. Each of these columns is actually a \"column chunk\" which is a contiguous block of data for that column. The schema across row groups must be consistent, i.e. the data types and number of columns must be the same for every row group. The new geospatial standard adds some relevant additional metadata such as the geometry's Coordinate Reference System (CRS), additional metadata for geometry columns, and <a href=\"https://medium.com/radiant-earth-insights/geoparquet-1-1-coming-soon-9b72c900fbf2\">support for spatial indexing in v1.1</a>.\n",
    "</div>\n",
    "\n",
    "<figure style=\"text-align: center\">\n",
    "<img src=\"https://guide.cloudnativegeo.org/images/geoparquet_layout.png\" style=\"width:40%; height:auto;\">\n",
    "<figcaption align = \"center\"> GeoParquet has the same layout with additional metadata </figcaption>\n",
    "</figure>\n",
    "\n",
    "<!-- GeoParquet is only the latest in a long line of cloud-native file formats  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9997ee",
   "metadata": {},
   "source": [
    "<!-- \n",
    "\n",
    "### \n",
    "\n",
    "For a much more in-depth exploration of the lower-level details that enable GeoParquet's (and the related memory layout specification, [GeoArrow](https://geoarrow.org/#relationship-with-geoparquet)) performance and scalability improvements, I recommend this [in-depth blog exploring the Apache Arrow format](https://iomete.com/resources/blog/apache-arrow-format) (which GeoParquet can natively take advantage of by, for example, encoding geometry metadata as GeoArrow instead of traditional WKB/WKT). More briefly, we highlight some of the key geoparquet-specific performance improvements and limitations below:\n",
    "\n",
    "\n",
    "  \n",
    "Beyond the file data itself, Parquet also stores metadata at the end of the file that describes the internal \"chunking\" of the file, byte ranges of every column chunks, several column statistics, among other things. \n",
    "We discuss some details below, but the main benefits to using GeoParquet are:\n",
    "1. it's a cloud-native file format which allows for efficient data access and processing in the cloud -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a02d4b-d4e7-4a45-909b-6082a16d6e21",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 77%; margin: 0 auto; padding-left: 1em; padding-right: 1em; text-align: justify;\">\n",
    "<h2 style=\"text-align: left\">GeoParquet: Features & Performance</h2>\n",
    "\n",
    "\n",
    "- Efficient storage and compression: \n",
    "    - Internally compressed by default, and can be configured to optimize decompression time or storage size\n",
    "    - columnar format is more efficient for filtering on columns which is common in analytical workloads\n",
    "- Scalability and Efficient data access:\n",
    "    - Spatial indexing, spatial partitioning, and other optimizations enable spatial joins (e.g. intersection, within, overlaps, etc) and [spatial predicate pushdowns](https://medium.com/radiant-earth-insights/geoparquet-1-1-coming-soon-9b72c900fbf2)\n",
    "    - The latter can significantly speed up spatial queries over the network by **applying filters at the storage level** and greatly reducing data movement\n",
    "- Optimized for *read-heavy workflows*: \n",
    "    - Parquet is an immutable file format, which means taking advantage of cheap reads, and efficient filtering and aggregation operations\n",
    "    - Ideal for OLAP (Online Analytical Processing) and BI (Business Intelligence) workloads that revolve around historical and aggregated data that don't require high-frequency updates\n",
    " - Interoperability and wide ecosystem:\n",
    "    - Integrates into existing data pipelines and workflows thanks to compatibility with existing Parquet ecosystem\n",
    "    - Broad and fast adoption across the data engineering and geospatial ecosystems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac0935a",
   "metadata": {},
   "source": [
    "### Convert individual datasets to GeoParquet with GeoPandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11057816",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# These dataset processing functions are now imported from utils.fetch_and_preprocess\n",
    "OVERLAP_THRESH = float(os.getenv('GEOM_OVERLAP_THRESHOLD', 0.5))\n",
    "\n",
    "# random.shuffle(selected_datasets)\n",
    "# go through the selected datasets and process them\n",
    "gdfs = []\n",
    "out_widgets = []\n",
    "\n",
    "for ds in tqdm(selected_datasets, desc=\"Processing datasets\"):\n",
    "    ds_files = selected_ds_files[ds]\n",
    "    ds_dir = get_ds_dir(ds)\n",
    "    ds_tree = dataset_metadata[ds]['fs_tree']\n",
    "    out_dir = DATASET_DIR / 'raw' / 'labels' / 'geoparquet'\n",
    "    # add header and footer separators for each dataset\n",
    "    print(\"\\n\", \"#\" * 50, \"\\n\")\n",
    "    print(f\"Processing dataset {ds} with {len(ds_files)} files in {os.path.relpath(ds_dir)}:\\n{ds_files}\")\n",
    "    \n",
    "    ds_gdf = process_vector_geoms(\n",
    "                geom_files=ds_files,\n",
    "                dataset_name=ds,\n",
    "                output_dir=out_dir\n",
    "    )\n",
    "    print(\"\\n\", \"#\" * 50, \"\\n\")\n",
    "    if ds_gdf is not None:\n",
    "        out = widgets.Output()\n",
    "        with out:\n",
    "            print(f\"Dataset {ds} has {len(ds_gdf)} geometries and {len(ds_gdf.columns)} columns.\")\n",
    "            display(ds_gdf.describe())\n",
    "            # display unique geometry types in each dataset gdf\n",
    "            print(f\"Unique geometry types in the dataset: {ds_gdf['geometry'].apply(lambda x: x.geom_type).unique()}\")\n",
    "            print(f\"Count of Point geometries: {len(ds_gdf[ds_gdf['geometry'].geom_type == 'Point'])}\")\n",
    "            print(f\"Count of Polygon geometries: {len(ds_gdf[ds_gdf['geometry'].geom_type == 'Polygon'])}\")\n",
    "            print(f\"Count of MultiPoint geometries: {len(ds_gdf[ds_gdf['geometry'].geom_type == 'MultiPoint'])}\")\n",
    "            print(f\"Count of MultiPolygon geometries: {len(ds_gdf[ds_gdf['geometry'].geom_type == 'MultiPolygon'])}\")\n",
    "            print(\"See data samples below:\")\n",
    "            display(ds_gdf.sample(3))\n",
    "        out_widgets.append(out)\n",
    "        gdfs.append(ds_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60c3cf3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Organize outputs in tabs with dataset DOIs as tab titles \n",
    "tabs = widgets.Tab(children=out_widgets)\n",
    "for i, ds in enumerate(selected_datasets):\n",
    "    title = '_'.join(ds.split('_')[:3])\n",
    "    tabs.set_title(i, title)\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba037d2",
   "metadata": {},
   "source": [
    "## Dataset Consolidation: single DB table and GPQ file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bf5f1c",
   "metadata": {},
   "source": [
    "### DuckDB: an in-process SQL OLAP RDBMS \n",
    "\n",
    "From their [\"Why DuckDB?\" page](https://duckdb.org/why_duckdb.html):\n",
    "\n",
    "DuckDB is an **in-process analytical data management system (OLAP RDBMS)**. Unlike traditional client-server databases (like PostgreSQL or MySQL), DuckDB runs directly within the host process (e.g., our Python script or Jupyter kernel), similar to SQLite. However, unlike SQLite which is optimized for transactional workloads (OLTP), DuckDB is specifically designed for **analytical queries (OLAP)** involving complex, long-running queries over potentially huge datasets, typical in big data analytics and scientific computing workflows.\n",
    "\n",
    "Key benefits for our workflow include:\n",
    "-   **Simplicity & Portability:** Easy installation (`pip install duckdb`) and no external dependencies or database server management required. Databases are stored as single, portable files (`.duckdb`), making them easy to manage, share, and archive.\n",
    "-   **Direct Data Access:** Can directly query various file formats, including the **Parquet and GeoParquet files** we are generating and (geo)pandas DataFrames(!), without needing a separate, time-consuming ingestion/copy step. This is highly efficient for consolidating data from multiple files, and remote sources (e.g., S3, GCS).\n",
    "-   **Powerful SQL:** Offers a rich, modern SQL dialect, including window functions, complex joins, and support for common table expressions (CTEs), allowing sophisticated data manipulation and analysis directly in SQL.\n",
    "-   **Geospatial Capabilities:** Crucially, DuckDB has a **`spatial` extension** that provides functions for handling and querying geospatial data types (like points, lines, and polygons) using libraries like GEOS. This enables operations such as spatial joins (e.g., `ST_Intersects`, `ST_Contains`), area calculations (`ST_Area`), centroid computation (`ST_Centroid`), and reading/writing WKT/WKB formats directly within the database. This is essential for our tasks like deduplication and integrating PV labels with contextual layers like Overture Maps.\n",
    "-   **Performance:** Its **column-vectorized query execution engine** is optimized for analytical performance, often *significantly faster than row-based systems* and more optimized than *pure Python/Pandas operations* for large datasets that may not fit into memory. \n",
    "-   **Python Integration:** Seamlessly integrates with Python libraries like Pandas and GeoPandas through its client API and tools like `jupysql`, allowing easy data exchange between dataframes and the database directly from our notebooks! \n",
    "\n",
    "In this notebook, we use DuckDB to:\n",
    "1.  Efficiently consolidate multiple GeoParquet files (one per source dataset) into a single database table using its ability to read Parquet directly.\n",
    "2.  Leverage its `spatial` extension for geospatial indexing, filtering, and performing spatial joins with the [Overture Maps divisions](#Overture-Maps-Divisions) data based on [H3 indices](#H3-Geospatial-Indexing-System-and-Spatial-Clustering).\n",
    "3.  Provide a persistent, queryable, and portable database (`.duckdb` file) containing the cleaned, consolidated, and spatially enriched PV label data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eac4643",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ZSTD_COMPRESSION = os.getenv(\"GPQ_ZSTD_COMPRESSION\", 5)\n",
    "TABLE_NAME = os.getenv(\"PV_DB_TABLE\", \"global_consolidated_pv\")\n",
    "\n",
    "# get list of geoparquet files to be consolidated\n",
    "get_full_gpq_path = lambda f: DATASET_DIR / 'raw' / 'labels' / 'geoparquet' / f\n",
    "parquet_files = [get_full_gpq_path(f) for f in os.listdir(DATASET_DIR / 'raw' / 'labels' / 'geoparquet') if any(os.path.splitext(f)[0].startswith(ds) for ds in selected_datasets)]\n",
    "flist = '\\n-'.join([os.path.relpath(f) for f in parquet_files])\n",
    "print(f\"Consolidating these {len(parquet_files)} files:\\n-{flist}\")\n",
    "\n",
    "DB_DIR = Path(os.getenv(\"DUCKDB_DIR\", DATASET_DIR / 'db'))\n",
    "out_consolidated_parquet = DATASET_DIR / 'prepared' / 'labels' / 'geoparquet' / 'global_consolidated_pv.geoparquet'\n",
    "out_consolidated_db = DB_DIR / 'global_consolidated_pv.duckdb'\n",
    "# create the output directories if they don't exist\n",
    "print(f\"Creating output directories: {out_consolidated_parquet.parent}\")\n",
    "os.makedirs(out_consolidated_parquet.parent, exist_ok=True)\n",
    "\n",
    "# consolidate the dataset into a single duckdb database that will also be saved as a geoparquet file\n",
    "# exclude POINT and MULTIPOINT geometries until we have implemented a heuristic to extract a PV polygon label from the points\n",
    "# TODO: look at usability of SAM2 models that perform segmentation from single point input \n",
    "db_file = geom_db_consolidate_dataset(\n",
    "    parquet_files=parquet_files,\n",
    "    table_name=TABLE_NAME,\n",
    "    geom_column=\"geometry\",\n",
    "    keep_geoms=[\"POLYGON\", \"MULTIPOLYGON\"], \n",
    "    spatial_index=True,\n",
    "    out_parquet=out_consolidated_parquet,\n",
    "    printout=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483b118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "%config SqlMagic.feedback = False\n",
    "%config SqlMagic.displaycon = False\n",
    "conn = get_duckdb_connection(db_file)\n",
    "%sql conn --alias duckdb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b9a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display db tables\n",
    "%sql SHOW TABLES;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9814c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql DESCRIBE {{TABLE_NAME}};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98cc66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SUMMARIZE SELECT unified_id, area_m2, centroid_lon, centroid_lat, dataset, bbox FROM {{TABLE_NAME}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b0860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the consolidated geoparquet file into a geopandas for visualization\n",
    "%time ds_gdf = gpd.read_parquet(out_consolidated_parquet)\n",
    "print(f\"Loaded {len(ds_gdf)} geometries from {out_consolidated_parquet}\")\n",
    "# display some stats about our raw combined gdf\n",
    "print(f\"Combined {len(gdfs)} datasets into one gdf with {len(ds_gdf)} rows and {len(ds_gdf.columns)} columns.\")\n",
    "print(f\"Combined gdf has the following columns:\\n{list(ds_gdf.columns)}\")\n",
    "display(ds_gdf.describe())\n",
    "display(ds_gdf.sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dd6230",
   "metadata": {},
   "source": [
    "## Spatial Deduplication with GeoPandas and Spatial Indexing\n",
    "<!-- We will use the `h3` library to perform spatial clustering of neighboring PV panels with our H3 grid, and perform local deduplication of the PV panels in `duckdb` using [spatial functions](https://duckdb.org/docs/stable/extensions/spatial/functions.html). -->\n",
    "\n",
    "<!-- We will compare the performance of the former approach with our initial approach using `geopandas` and their [spatial index](https://geopandas.org/en/stable/docs/reference/sindex.html) methods and spatial predicates. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e52b66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# perform geopandas dedup and check size again\n",
    "pd.set_option('display.max_rows', 10)  # Number of rows per page\n",
    "ds_gdf = filter_gdf_duplicates(ds_gdf)\n",
    "print(f\"After filtering and cleaning, we have {len(ds_gdf)} PV installations\")\n",
    "display(ds_gdf.describe())\n",
    "display(ds_gdf.sample(3))\n",
    "# save the consolidated geoparquet file in our prepared directory\n",
    "consolidated_dedup_parquet = DATASET_DIR / 'prepared' / 'labels' / 'geoparquet' / 'global_consolidated_pv_dedup.geoparquet'\n",
    "# drop existing bbox column to recalculate with reduced geometries\n",
    "if 'bbox' in ds_gdf.columns:\n",
    "    ds_gdf = ds_gdf.drop(columns=['bbox'])\n",
    "# save the processed geoparquet file\n",
    "ds_gdf.to_parquet(consolidated_dedup_parquet, \n",
    "    index=None, \n",
    "    compression='zstd',\n",
    "    # geometry_encoding='geoarrow', \n",
    "    write_covering_bbox=True,\n",
    "    schema_version='1.1.0')\n",
    "# remove our non-deduped geoparquet file\n",
    "if os.path.exists(out_consolidated_parquet):\n",
    "    os.remove(out_consolidated_parquet)\n",
    "    print(f\"Removed {out_consolidated_parquet} to save space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1b0373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display unique geometry types in our gdf\n",
    "print(f\"Unique geometry types in the dataset: {ds_gdf['geometry'].apply(lambda x: x.geom_type).unique()}\")\n",
    "# print sample of geoms that are points\n",
    "print(f\"Count of Point geometries: {len(ds_gdf[ds_gdf['geometry'].geom_type == 'Point'])}\")\n",
    "print(f\"Count of Polygon geometries: {len(ds_gdf[ds_gdf['geometry'].geom_type == 'Polygon'])}\")\n",
    "print(f\"Count of MultiPoint geometries: {len(ds_gdf[ds_gdf['geometry'].geom_type == 'MultiPoint'])}\")\n",
    "print(f\"Count of MultiPolygon geometries: {len(ds_gdf[ds_gdf['geometry'].geom_type == 'MultiPolygon'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f01ce6",
   "metadata": {},
   "source": [
    "# Visualization Functions for PV Data\n",
    "\n",
    "After processing the datasets into standardized geoparquet format, we'll use the following visualization libraries to explore and present the data:\n",
    "\n",
    "- **Folium**: For interactive web maps with various basemaps and markers\n",
    "- **Pydeck**: For high-performance 3D and large-scale visualizations\n",
    "- **Lonboard**: For GPU-accelerated geospatial visualization of large datasets\n",
    "\n",
    "Each library has specific strengths that we'll leverage for different visualization needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a547fa-257b-4f35-aa9f-6434022a61ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source gdf for visualization directly from db in cases where ds_gdf is deleted further below\n",
    "viz_gdf = conn.sql(f\"\"\"\n",
    "SELECT unified_id, dataset, area_m2, centroid_lon, centroid_lat, bbox, ST_AsText(geometry) AS geometry\n",
    "FROM {TABLE_NAME}\"\"\").df()\n",
    "viz_gdf = gpd.GeoDataFrame(viz_gdf, geometry=viz_gdf['geometry'].apply(wkt.loads), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf64feb",
   "metadata": {},
   "source": [
    "## Folium Visualization Functions\n",
    "\n",
    "Folium is excellent for creating interactive web maps with various basemaps and markers. It's particularly useful for visualizing geographic distributions and creating choropleth maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f7991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for creating a \"Show Map\" and \"Clear Output\" button with dedicated output area\n",
    "def create_map_buttons(map_object):\n",
    "    # Create taller buttons with custom styling\n",
    "    button_layout = widgets.Layout(\n",
    "        height='40px',\n",
    "        width='150px',\n",
    "        margin_left='2em',\n",
    "        margin_right='2em',\n",
    "        display='flex',\n",
    "        align_items='center',\n",
    "        justify_content='center'\n",
    "    )\n",
    "    \n",
    "    # Create an output widget for the map\n",
    "    output_area = widgets.Output(\n",
    "        layout=widgets.Layout(\n",
    "            height='500px',\n",
    "            border='2px solid #ddd',\n",
    "            overflow='auto',\n",
    "            align_content='center'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    show_map_button = widgets.Button(description=\"Show Map\", button_style='info', layout=button_layout)\n",
    "    clear_output_button = widgets.Button(description=\"Clear Output\", button_style='warning', layout=button_layout)\n",
    "\n",
    "    def on_show_map_clicked(b):\n",
    "        output_area.clear_output(wait=True)\n",
    "        with output_area:\n",
    "            display(map_object)\n",
    "    \n",
    "    def on_clear_output_clicked(b):\n",
    "        output_area.clear_output(wait=True)\n",
    "    \n",
    "    show_map_button.on_click(on_show_map_clicked)\n",
    "    clear_output_button.on_click(on_clear_output_clicked)\n",
    "    \n",
    "    # Return buttons and output area in a horizontal layout\n",
    "    controls = widgets.HBox([show_map_button, clear_output_button])\n",
    "    return widgets.VBox([controls, output_area])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bafefdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filter to only rows without NaN for folium viz\n",
    "# ds_gdf = ds_gdf[ds_gdf['area_m2'].notna()]\n",
    "# Create Folium maps\n",
    "# re-project geometries to a projected CRS for visualization\n",
    "tmp_gdf = viz_gdf.to_crs(epsg=3857)\n",
    "\n",
    "# render_button = widgets.Button(description=\"Render Map\", button_style='success', layout=Layout(width='auto'))\n",
    "# render_out = widgets.Output()\n",
    "# with render_out:\n",
    "folium_map = create_folium_cluster_map(tmp_gdf, title=f\"{TABLE_NAME} - Cluster Map\")\n",
    "vout = create_map_buttons(folium_map)\n",
    "\n",
    "# def cluster_map_on_click(b):\n",
    "#     clear_output(wait=True)\n",
    "#     display(render_out)\n",
    "\n",
    "\n",
    "# render_button.on_click(cluster_map_on_click)\n",
    "\n",
    "# display(render_button)\n",
    "display(vout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4b42fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "# plot map and scatterplot on top of it\n",
    "fig, ax = plt.subplots(figsize=(8, 12))\n",
    "# world.boundary.plot(ax=ax, linewidth=1)\n",
    "viz_gdf.plot(ax=ax, kind='scatter', x='centroid_lon', y='centroid_lat', alpha=0.5, c='area_m2', cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad9e0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time create_folium_cluster_map(tmp_gdf, title=f\"{TABLE_NAME} - Cluster Map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d04e252",
   "metadata": {},
   "source": [
    "## PyDeck Visualization Functions\n",
    "\n",
    "PyDeck is excellent for high-performance 3D visualizations and handling large datasets. It's particularly useful for creating layered maps with multiple types of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fff6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdk_heatmap = create_pydeck_heatmap(\n",
    "    gdf=viz_gdf,\n",
    "    weight_column='area_m2',\n",
    "    radius_pixels=15,\n",
    "    intensity=1.55,\n",
    "    threshold=0.005,\n",
    "    aggregation='SUM', # SUM or MEAN\n",
    "    tooltip_cols=['dataset', 'area_m2'],\n",
    "    map_style='light',\n",
    "    initial_zoom=2\n",
    ")\n",
    "vout = create_map_buttons(pdk_heatmap)\n",
    "display(vout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5195e2df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # hide pydeck warnings\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pydeck\")\n",
    "# import tempfile\n",
    "# import os\n",
    "# from IPython.display import HTML, display as ipy_display, clear_output\n",
    "# import ipywidgets as widgets\n",
    "# from ipywidgets import Layout\n",
    "\n",
    "# # Global variable to store the path to the temporary HTML file\n",
    "# heatmap_html_path = None\n",
    "\n",
    "# render_button = widgets.Button(description=\"Render Map to File\", button_style='success', layout=Layout(width='auto'))\n",
    "# render_out = widgets.Output(layout=widgets.Layout(border='1px solid #ddd', padding='10px', min_height='50px')) # Added min_height\n",
    "# clear_button = widgets.Button(description=\"Clear Output & File\", button_style='warning', layout=Layout(width='auto'))\n",
    "\n",
    "# def pdk_heatmap_on_click(b):\n",
    "#     global heatmap_html_path\n",
    "#     with render_out:\n",
    "#         clear_output(wait=True)\n",
    "#         print(\"Generating heatmap...\")\n",
    "#         try:\n",
    "#             pdk_heatmap = create_pydeck_heatmap(\n",
    "#                 gdf=viz_gdf,\n",
    "#                 weight_column='area_m2',\n",
    "#                 radius_pixels=15,\n",
    "#                 intensity=1.55,\n",
    "#                 threshold=0.005,\n",
    "#                 aggregation='SUM', # SUM or MEAN\n",
    "#                 tooltip_cols=['dataset', 'area_m2'],\n",
    "#                 map_style='light',\n",
    "#                 initial_zoom=2\n",
    "#             )\n",
    "\n",
    "#             # Create a temporary file to save the HTML\n",
    "#             # Use a more persistent temp directory if needed across sessions/restarts\n",
    "#             temp_dir = tempfile.gettempdir()\n",
    "#             with tempfile.NamedTemporaryFile(delete=False, suffix='.html', dir=temp_dir) as tmp_file:\n",
    "#                 heatmap_html_path = tmp_file.name\n",
    "#                 pdk_heatmap.to_html(heatmap_html_path, notebook_display=False) # Save without displaying\n",
    "#             print(f\"Heatmap successfully generated and saved.\")\n",
    "#             print(f\"Please open the following file in your browser:\")\n",
    "#             print(f\"file://{heatmap_html_path}\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error generating or saving heatmap: {e}\")\n",
    "#             heatmap_html_path = None # Reset path on error\n",
    "\n",
    "\n",
    "# def clear_output_on_click(b):\n",
    "#     global heatmap_html_path\n",
    "#     # with render_out:\n",
    "#     clear_output(wait=True)\n",
    "#     if heatmap_html_path and os.path.exists(heatmap_html_path):\n",
    "#         try:\n",
    "#             os.remove(heatmap_html_path)\n",
    "#             print(f\"Deleted temporary file: {heatmap_html_path}\")\n",
    "#             heatmap_html_path = None\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error deleting temporary file {heatmap_html_path}: {e}\")\n",
    "#     elif heatmap_html_path:\n",
    "#             print(f\"Temporary file path recorded but file not found: file://{heatmap_html_path}\")\n",
    "#             heatmap_html_path = None # Clear the path if file doesn't exist\n",
    "#     else:\n",
    "#         print(\"No temporary file path recorded.\")\n",
    "#     print(\"Output cleared. Click 'Render Map to File' to generate a new heatmap.\")\n",
    "\n",
    "# render_button.on_click(pdk_heatmap_on_click)\n",
    "# clear_button.on_click(clear_output_on_click)\n",
    "\n",
    "# # Display the buttons and the output area\n",
    "# display(widgets.HBox([render_button, clear_button]))\n",
    "# display(render_out)\n",
    "\n",
    "# # Initial message in the output area\n",
    "# with render_out:\n",
    "#     print(\"Click 'Render Map to File' to generate the heatmap HTML and display its path here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b3ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# render_button = widgets.Button(description=\"Render Map\", button_style='success', layout=Layout(width='auto'))\n",
    "# render_out = widgets.Output()\n",
    "\n",
    "# def pdk_scatter_on_click(b):\n",
    "#     with render_out:\n",
    "#         clear_output(wait=True)\n",
    "pdk_scatter = create_pydeck_scatterplot(\n",
    "    gdf=viz_gdf,\n",
    "    color_column='dataset',\n",
    "    size_column='area_m2',\n",
    "    size_scale=1.5,\n",
    "    tooltip_cols=['dataset', 'area_m2'],\n",
    "    map_style='light',\n",
    "    initial_zoom=2\n",
    ")\n",
    "\n",
    "# render_button.on_click(pdk_scatter_on_click)\n",
    "# with render_out:\n",
    "# vout = widgets.VBox([render_button, render_out])\n",
    "vout = create_map_buttons(pdk_scatter)\n",
    "display(vout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba7f427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4343995f",
   "metadata": {},
   "source": [
    "# Fetching vector layers for Spatio-temporal context\n",
    "\n",
    "To better manage our *global* PV datasets for clustered STAC searches with grid traversal,.  \n",
    "We will also fetch some vector layers that enable us to organize the datasets by country, continent, and other relevant spatio-temporal context for our research.\n",
    "\n",
    "**NOTE**: Fetching Overture divisions locally (with no filters) will result in a **~6.4GB file**!;  \n",
    "There is a cell at the end of this notebook to optionally delete the saved duckdb table to save space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f92efd",
   "metadata": {},
   "source": [
    "## Overture Maps: Adding Geospatial Context to our data\n",
    "\n",
    "From their [Division theme guide](https://docs.overturemaps.org/guides/divisions/) and their [brief blog on the history of the project](https://overturemaps.org/blog/2025/overture-maps-foundation-making-open-data-the-winning-choice/):\n",
    "\n",
    "Overture Maps is a collaborative project that aims to create a high-quality, open map datasets for the entire world. The project is a collaboration between several organizations, including Meta, Amazon Web Services (AWS), and Microsoft. Overture distributes its open datasets as GeoParquet files (see above), and can be accessed through the Overture Maps CLI, API or downloaded directly from [their S3](https://docs.overturemaps.org/guides/divisions/#data-access-and-retrieval) buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765b666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"spatio-temporal_metadata.json\", \"r\") as file:\n",
    "    st_metadata = json.load(file)\n",
    "\n",
    "OVERTURE_DIVISION_AREA_S3 = st_metadata[\"overture_divisions_area\"][\"s3_path\"]\n",
    "OVERTURE_LAND_COVER_S3 = st_metadata[\"overture_base_land_cover\"][\"s3_path\"]\n",
    "\n",
    "# --- Step 1: Fetch Overture base and division maps into a DuckDB table for use with our labels ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bba6403",
   "metadata": {},
   "source": [
    "### Overture Maps Divisions\n",
    "\n",
    "From their [Division Overview page](https://docs.overturemaps.org/guides/divisions/#overview):\n",
    "\n",
    "The Overture divisions theme has three feature types: division, **division_area**, and division_boundary.\n",
    "\n",
    "There are more than 5.45 million point, line, and polygon representations of human settlements, such as countries, regions, states, cities, and even neighborhoods. The theme is derived from a conflation of OpenStreetMap data and geoBoundaries data and is available under an ODbL license.\n",
    "\n",
    "Divisions data can be used for many purposes, which can include, but are not limited to:\n",
    "- reverse-geocoding\n",
    "- cartographic styling and map rendering\n",
    "- place labels\n",
    "- choropleth rendering\n",
    "- containment checks (e.g. point-in-polygon analysis)\n",
    "- choosing a geometry based on your use case\n",
    "\n",
    "Their `division_area` subset provides a hierarchical structure of administrative boundaries, including countries, states, and cities. There are more than 5.45 million point, line, and polygon representations of human settlements, such as countries, regions, states, cities, and even neighborhoods. The theme is derived from a conflation of OpenStreetMap data and geoBoundaries data.\n",
    "\n",
    "This dataset is useful for geospatial analysis and can be used to enrich other datasets with location-based information. \n",
    "Divisions data can be used for many purposes, and we include some examples below with those relevant to our work highlighted:\n",
    "\n",
    "- reverse-geocoding\n",
    "- **cartographic styling and map rendering**\n",
    "- place labels\n",
    "- choropleth rendering\n",
    "- **containment checks (e.g. point-in-polygon analysis)**\n",
    "- **choosing a geometry based on your use case**\n",
    "\n",
    "<figure style=\"text-align: center\">\n",
    "<img src=\"https://docs.overturemaps.org/assets/images/divisions-admin0-admin1-coverage-ff1a8d4c6d68c88047b34d1f9c9109be.png\" style=\"width:65%; height:auto;\">\n",
    "<figcaption align = \"center\"> Overture divisions data, styled by subtype: countries in purple, region boundaries as green lines. </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9512c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ov_divisions_table = os.getenv(\"DIVISIONS_DB_TABLE\", \"overture_division_areas\")\n",
    "# only fetch these division types as more granular divisions are not needed for our analysis\n",
    "division_types = [\n",
    "    'dependency',\n",
    "    'country',\n",
    "    'region',\n",
    "    'county'\n",
    "]\n",
    "div_filters = {\"subtype\": (\"IN\", division_types)}\n",
    "\n",
    "print(f\"Fetching Overture divisions from {OVERTURE_DIVISION_AREA_S3} into DuckDB database {out_consolidated_db} …\")\n",
    "fetch_success = fetch_overture_maps_theme(\n",
    "    s3_path=OVERTURE_DIVISION_AREA_S3,\n",
    "    db_file=str(out_consolidated_db),\n",
    "    table_name=ov_divisions_table,\n",
    "    filter_dict=div_filters,\n",
    "    out_gpq_path=None  # no need to export to geoparquet here\n",
    ")\n",
    "if fetch_success:\n",
    "    print(\"Overture divisions fetched and stored in DuckDB successfully.\")\n",
    "else:\n",
    "    print(\"Failed to fetch Overture divisions into DuckDB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093e2c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext sql\n",
    "conn = get_duckdb_connection(db_file)\n",
    "%sql conn --alias duckdb \n",
    "\n",
    "%config SqlMagic.autopandas = False\n",
    "%config SqlMagic.feedback = False\n",
    "%config SqlMagic.displaycon = False\n",
    "%config SqlMagic.displaylimit = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea58d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql DESCRIBE {{ov_divisions_table}};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3276d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT DISTINCT subtype FROM {{ov_divisions_table}} ORDER BY subtype;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60922c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT COUNT(DISTINCT country) AS division_country_count FROM {{ov_divisions_table}};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3d7236",
   "metadata": {},
   "source": [
    "### Overture Maps Land Cover theme \n",
    "\n",
    "Size is pretty large and should be approached as querying from S3 for required AOI and dates.\n",
    "https://docs.overturemaps.org/blog/2024/05/16/land-cover/\n",
    "\n",
    "Land Cover data could be useful to characterize the distribution of PV panels in different land cover types, and the diversity of land cover types in the vicinity of PV installations. This could be useful for understanding in what type of land cover types PV panels are being installed (i.e. is it an appretiate land use for the land cover type?).\n",
    "\n",
    "TBD in Summer and Fall 2025..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5e4a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ov_land_cover_table = \"overture_land_cover\"\n",
    "# # --- Step 1: Fetch Overture base and division maps into a DuckDB table for use with our labels ---\n",
    "# print(f\"Fetching Overture land cover from {OVERTURE_LAND_COVER_S3} into DuckDB database {out_consolidated_db} …\")\n",
    "# fetch_success = fetch_overture_maps_theme(\n",
    "#     s3_path=OVERTURE_LAND_COVER_S3,\n",
    "#     db_file=str(out_consolidated_db),\n",
    "#     table_name=ov_land_cover_table,\n",
    "#     out_gpq_path=None  # no need to export to geoparquet here\n",
    "# )\n",
    "# if fetch_success:\n",
    "#     print(\"Overture divisions fetched and stored in DuckDB successfully.\")\n",
    "# else:\n",
    "#     print(\"Failed to fetch Overture divisions into DuckDB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de11d8ca",
   "metadata": {},
   "source": [
    "## H3: Geospatial Indexing System and Spatial Clustering\n",
    "\n",
    "From their [home page](https://h3geo.org/), [announcement blog](https://www.uber.com/blog/h3/), and [overview page](https://h3geo.org/docs/core-library/overview/):\n",
    "\n",
    "The H3 geospatial indexing system is a discrete global grid system developed at Uber. It was designed for **indexing geographies via multi-precision hexagonal tiling into a hexagonal grid with hierarchical indexes**. Geospatial coordinates can be indexed to *cell IDs* that each represent a unique cell.  \n",
    "Indexed data can be **quickly joined across disparate datasets and aggregated at different levels of precision.**. \n",
    "\n",
    "H3 enables a *range of algorithms and optimizations* based on the grid, including *nearest neighbors, shortest path, gradient smoothing*, and more.\n",
    "\n",
    "The hexagonal grid system is created on the planar faces of [a sphere-circumscribed icosahedron](https://www.researchgate.net/figure/a-Icosahedron-faces-projected-onto-a-sphere-b-Triangular-tessellation-c-Diamond_fig1_339813234) and the grid cells are then projected to the surface of the sphere...\n",
    "It is common to use WGS84/EPSG:4326 CRS data with the H3 library.\n",
    "\n",
    "<figure style=\"text-align: center\">\n",
    "<img src=\"https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=100,onerror=redirect,format=auto/wp-content/uploads/2018/06/Twitter-H3.png\" style=\"width:75%; height:auto;\">\n",
    "<figcaption align = \"center\"> H3 enables users to partition the globe into hexagons for more accurate analysis. </figcaption>\n",
    "</figure>\n",
    "\n",
    "[TODO: Add brief note on rise of Discrete Global Grid Systems (DGGS) in scientific data and computing]\n",
    "\n",
    "See formal citations here:\n",
    "- [XDGGS: A community-developed Xarray package to support planetary DGGS data cube computations](https://doi.org/10.5194/isprs-archives-XLVIII-4-W12-2024-75-2024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a82eaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "# display h3 viewer \n",
    "IFrame(\"https://h3.chotard.com\", width=1080, height=540)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a57ba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Compute updated PV GeoDataFrame with H3 indices ---\n",
    "# h3_resolution = 10\n",
    "\n",
    "import h3\n",
    "# time performance of h3 indexing\n",
    "# t1 = time.time()\n",
    "%time ds_gdf_h3 = add_h3_index_to_pv_labels(ds_gdf)\n",
    "del ds_gdf\n",
    "# t2 = time.time()\n",
    "# print(f\"Time taken to add H3 indices: {t2 - t1:.2f} seconds\")\n",
    "\n",
    "# plot out the distribution of h3 resolutions\n",
    "temp_gdf = ds_gdf_h3[['unified_id', 'h3_index']].copy()\n",
    "# # add temp columns for h3 resolution used and h3 cell area in m2\n",
    "temp_gdf['h3_resolution'] = ds_gdf_h3['h3_index'].apply(lambda x: h3.get_resolution(x))\n",
    "temp_gdf['h3_cell_area_m2'] = ds_gdf_h3['h3_index'].apply(lambda x: h3.cell_area(x, unit='m^2'))\n",
    "display(temp_gdf.sample(3))\n",
    "\n",
    "# see unique h3 resolutions\n",
    "print(f\"Unique H3 resolutions in the dataset: {temp_gdf['h3_resolution'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebde780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display unique geometry types in our gdf\n",
    "print(f\"Unique geometry types in the dataset: {ds_gdf_h3['geometry'].apply(lambda x: x.geom_type).unique()}\")\n",
    "# print sample of geoms that are points\n",
    "print(f\"Count of Point geometries: {len(ds_gdf_h3[ds_gdf_h3['geometry'].geom_type == 'Point'])}\")\n",
    "print(f\"Count of Polygon geometries: {len(ds_gdf_h3[ds_gdf_h3['geometry'].geom_type == 'Polygon'])}\")\n",
    "print(f\"Count of MultiPoint geometries: {len(ds_gdf_h3[ds_gdf_h3['geometry'].geom_type == 'MultiPoint'])}\")\n",
    "print(f\"Count of MultiPolygon geometries: {len(ds_gdf_h3[ds_gdf_h3['geometry'].geom_type == 'MultiPolygon'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1341eda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Export updated PV labels with H3 indices to a geoparquet file for spatial join ---\n",
    "consolidated_dedup_h3_parquet = DATASET_DIR / 'prepared' / 'labels' / 'geoparquet' / 'global_consolidated_pv_dedup_h3.geoparquet'\n",
    "os.makedirs(consolidated_dedup_h3_parquet.parent, exist_ok=True)\n",
    "ds_gdf_h3.to_parquet(\n",
    "    consolidated_dedup_h3_parquet, \n",
    "    index=False, \n",
    "    # encoding='wkt',\n",
    "    compression='zstd',\n",
    "    # write_covering_bbox=True,\n",
    "    schema_version='1.1.0'\n",
    ")\n",
    "print(f\"Exported updated PV labels with H3 indices to {consolidated_dedup_h3_parquet}\")\n",
    "# remove the previous geoparquet file\n",
    "if os.path.exists(consolidated_dedup_parquet):\n",
    "    os.remove(consolidated_dedup_parquet)\n",
    "    print(f\"Removed {consolidated_dedup_parquet} to save space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7696ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update duckdb table with our deduplicated + h3 indexed geoparquet file by dropping the rows not in the new file\n",
    "prev_pv_count = conn.execute(f\"SELECT COUNT(*) FROM {TABLE_NAME};\").fetchone()[0]\n",
    "# change geometry encoding so we can directly query gdf with duckdb\n",
    "ds_gdf_h3['geometry'] = ds_gdf_h3['geometry'].to_wkt()\n",
    "conn.execute(f\"DELETE FROM {TABLE_NAME} WHERE unified_id NOT IN (SELECT unified_id FROM ds_gdf_h3);\")\n",
    "new_pv_count = conn.execute(f\"SELECT COUNT(*) FROM {TABLE_NAME};\").fetchone()[0]\n",
    "print(f\"Updated {TABLE_NAME} table with {new_pv_count} rows from {consolidated_dedup_h3_parquet}\\nand removed {prev_pv_count - new_pv_count} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1542e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# add a common h3 index at the same resolution to both our pv labels and overture divisions for easier spatial joins\n",
    "common_h3_res = 5 # ~250km^2; roughly corresponds to many sensors swath widths, and San Juan, PR is ~200km^2\n",
    "# common_h3_res = 4 # ~1770km^2; children cells can be used for sampling stac items of area ~250km^2; results in ~\n",
    "# apply the common h3 index to both of our tables\n",
    "ddb_alter_table_add_h3(\n",
    "    db_file=out_consolidated_db,\n",
    "    table_name=TABLE_NAME,\n",
    "    h3_resolution=common_h3_res\n",
    ")\n",
    "\n",
    "ddb_alter_table_add_h3(\n",
    "    db_file=out_consolidated_db,\n",
    "    table_name=ov_divisions_table,\n",
    "    h3_resolution=common_h3_res\n",
    ")\n",
    "\n",
    "# close the db connection since we are about to add some new tables that don't automatically reflect in the db object\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747c08dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our util to add the matching overture divisions to our pv labels\n",
    "h3_col = f\"h3_res_{common_h3_res}\"\n",
    "joined_divs = ddb_save_div_matches(\n",
    "    db_file=out_consolidated_db,\n",
    "    labels_table=TABLE_NAME,\n",
    "    divisions_table=ov_divisions_table,\n",
    "    division_subtypes=division_types,\n",
    "    h3_col_name=h3_col\n",
    ")\n",
    "if joined_divs:\n",
    "    print(\"Joined Overture divisions with PV labels successfully.\")\n",
    "else:\n",
    "    print(\"Failed to join Overture divisions with PV labels. See errors above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb418c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the geometries for countries in a separate table with some pv labels aggregate stats\n",
    "saved_country_geoms = ddb_save_subtype_geoms(db_file=db_file, \n",
    "                                             div_table=ov_divisions_table,\n",
    "                                             pv_table=TABLE_NAME,\n",
    "                                             division_type='country')\n",
    "if saved_country_geoms:\n",
    "    print(f\"Saved all country geometries in separate table `ov_divisions_country_geoms`\")\n",
    "else:\n",
    "    print(\"Failed to save country geometries with PV labels. See errors above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0680bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the db connection we closed earlier\n",
    "conn = get_duckdb_connection(db_file)\n",
    "%sql conn --alias duckdb\n",
    "\n",
    "print(f\"Updated columns for DuckDB table `{TABLE_NAME}` with matching overture divisions fields:\\n\")\n",
    "%config SqlMagic.autopandas = False\n",
    "%sql SHOW TABLES;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04233c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n\\nUpdated columns for DuckDB table `{TABLE_NAME}` with matching overture divisions fields:\\n\")\n",
    "%sql DESCRIBE {{TABLE_NAME}};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e6519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n\\nSaved separate table with top level country geometries `{ov_divisions_table}`\\n\")\n",
    "%sql DESCRIBE country_geoms; "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f917a060",
   "metadata": {},
   "source": [
    "## Spatial Context Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e9330c",
   "metadata": {},
   "source": [
    "### Enabling Regional, and National Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23b1eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of countries from the Overture divisions table within given bbox\n",
    "def get_countries_in_bbox(bbox):\n",
    "    \n",
    "    available_countries = conn.execute(f\"\"\"\n",
    "    SELECT DISTINCT country\n",
    "    FROM {ov_divisions_table}\n",
    "    WHERE bbox.xmin >= {bbox['xmin']}\n",
    "        AND bbox.xmax <= {bbox['xmax']}\n",
    "        AND bbox.ymin >= {bbox['ymin']}\n",
    "        AND bbox.ymax <= {bbox['ymax']};\"\"\").fetchall()\n",
    "    available_countries = [row[0] for row in available_countries]\n",
    "    available_countries.sort() # sort alphabetically\n",
    "    # Create a dictionary mapping ISO codes to country names\n",
    "    country_name_to_iso = {}\n",
    "    for iso in available_countries:\n",
    "        try:\n",
    "            country = pycountry.countries.get(alpha_2=iso)\n",
    "            if country:\n",
    "                country_name_to_iso[country.name] = iso\n",
    "            else:\n",
    "                country_name_to_iso[iso] = iso\n",
    "        except:\n",
    "            country_name_to_iso[iso] = iso\n",
    "    return country_name_to_iso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e17a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note this causes duckdb commands to not display correctly\n",
    "%config SqlMagic.autopandas = True\n",
    "from shapely import wkt\n",
    "# obtained by hand from this web tool: https://boundingbox.klokantech.com/\n",
    "region_bboxes = {\n",
    "    'Europe': {'xmin': -10.3537422571, 'ymin': 37.6758453719, 'xmax': 38.9601092499, 'ymax': 69.4103480224},\n",
    "    'North America': {'xmin': -168.9608207601, 'ymin': 14.1919805753, 'xmax': -11.8182066364, 'ymax': 82.358239202},\n",
    "    'South America': {'xmin': -78.832, 'ymin': -53.5, 'xmax': -34.5, 'ymax': 9.5},\n",
    "    'Caribbean and Central America': {'xmin': -92.4356240886, 'ymin': 6.0669772529, 'xmax': -58.9479149275, 'ymax': 22.7606175843},\n",
    "    'Africa': {'xmin': -24.6846926317, 'ymin': -34.1570435723, 'xmax': 59.3700034902, 'ymax': 38.9354816694},\n",
    "    'Asia': {'xmin': 36.4650442099, 'ymin': 5.9216789926, 'xmax': -170.1791801277, 'ymax': 75.65461693326},\n",
    "    'Oceania': {'xmin': 110.7980110873, 'ymin': -47.8675155043, 'xmax': -133.990553329, 'ymax': 5.2797365739}\n",
    "}\n",
    "\n",
    "def get_countries_gdf(country_isos):\n",
    "    # Get the countries in the selected region\n",
    "    cond = f\"country_iso IN ({', '.join([f'\\'{iso}\\'' for iso in country_isos])})\"\n",
    "    countries_gdf = conn.execute(f\"\"\"\n",
    "        SELECT division_id, country_iso, division_name, \n",
    "        CAST(country_pv_count AS INT) as country_pv_count,\n",
    "        CAST(country_pv_area_m2 AS FLOAT) as country_pv_area_m2, ST_AsText(geometry) AS geometry\n",
    "        FROM country_geoms as countries\n",
    "        WHERE {cond}\"\"\").df()\n",
    "    \n",
    "    countries_gdf = gpd.GeoDataFrame(\n",
    "        countries_gdf,\n",
    "        geometry=countries_gdf['geometry'].apply(wkt.loads),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    return countries_gdf\n",
    "\n",
    "def get_folium_region_map(region_gdf, region_bbox):\n",
    "\n",
    "    # total_bounds returns a  (minx, miny, maxx, maxy) tuple of the bounds of the series as a whole\n",
    "    # bounds = region_gdf.total_bounds\n",
    "    bbox = region_bbox\n",
    "    # print(f\"Region bbox: {bbox}\")\n",
    "    # get rough bbox center for init map view\n",
    "    bbox_center = {\n",
    "        'lat': (bbox['ymin'] + bbox['ymax']) / 2,\n",
    "        'lon': (bbox['xmin'] + bbox['xmax']) / 2\n",
    "    }\n",
    "    # get full region name from the gdf ISO 3166-2 region field\n",
    "    # region_gdf['region'] = region_gdf['region'].apply(lambda x: pycountry.subdivisions.get(code=x).name)\n",
    "    # print(f\"Region name: {region_gdf['region'].unique()}\")\n",
    "\n",
    "    # prepare map object\n",
    "    init_map_view = [bbox_center['lat'], bbox_center['lon']]\n",
    "    # Define a style function for the GeoJson layer\n",
    "    style_function = lambda x: {\n",
    "        'fillColor': 'lightblue',\n",
    "        'color': 'black',  # Border color\n",
    "        'weight': 1,       # Border weight\n",
    "        'fillOpacity': 0.5\n",
    "    }\n",
    "\n",
    "    # Define a highlight function for hover effect (optional)\n",
    "    highlight_function = lambda x: {\n",
    "        'fillColor': '#0000FF', # Change color on hover\n",
    "        'color': 'black',\n",
    "        'weight': 2,\n",
    "        'fillOpacity': 0.7\n",
    "    }\n",
    "\n",
    "    # Create a map of the globe and plot the bounding box over it\n",
    "    m = folium.Map(location=init_map_view, zoom_start=2, tiles='cartodb positron')\n",
    "    # from Folium docs: \"[(lat1, lon1), (lat2, lon2)] – Two lat lon pairs marking the two corners of the rectangle.\"\n",
    "    total_envelope_coords = [\n",
    "        (bbox['ymin'], bbox['xmin']),\n",
    "        (bbox['ymax'], bbox['xmax'])\n",
    "    ]\n",
    "    folium.Rectangle(\n",
    "        bounds=total_envelope_coords,\n",
    "        color='red',\n",
    "        fill=False,\n",
    "        weight=2\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Add the GeoDataFrame to the map as a GeoJson layer\n",
    "    # Add the countries to the map\n",
    "    folium.GeoJson(\n",
    "        region_gdf,\n",
    "        name='Countries',\n",
    "        style_function=style_function,\n",
    "        highlight_function=highlight_function,\n",
    "        tooltip=folium.features.GeoJsonTooltip(\n",
    "            fields=['country_iso', 'division_name', 'country_pv_count' ],\n",
    "            aliases=['Country:', 'Name:', 'PV Count:'],\n",
    "            sticky=False,\n",
    "            style=\"background-color: white; color: black; font-family: arial; font-size: 14px; padding: 10px;\"\n",
    "        )\n",
    "    ).add_to(m)\n",
    "    return m\n",
    "\n",
    "# dropdown to choose region bbox\n",
    "region_dropdown = widgets.Dropdown(\n",
    "    options=list(region_bboxes.keys()),\n",
    "    description='Region:',\n",
    "    value='Europe',\n",
    "    disabled=False,\n",
    "    layout=Layout(width='70%', margin='20 20 auto 20 20')\n",
    ")\n",
    "render_out = widgets.Output()\n",
    "\n",
    "bbox = region_bboxes[region_dropdown.value]\n",
    "countries_in_region = conn.execute(f\"\"\"\n",
    "    SELECT DISTINCT country_iso\n",
    "    FROM country_geoms\n",
    "    WHERE ST_Intersects(geometry, ST_MakeEnvelope({bbox['xmin']}, {bbox['ymin']}, {bbox['xmax']}, {bbox['ymax']}));\"\"\").fetchall()\n",
    "countries_in_region = [row[0] for row in countries_in_region]\n",
    "countries_gdf = get_countries_gdf(countries_in_region)\n",
    "\n",
    "@interact(region=region_dropdown)\n",
    "def plot_region_bbox_global(region):\n",
    "    # display(region_dropdown)\n",
    "    bbox = region_bboxes[region]\n",
    "    print(f\"Selected region [{region}] with bbox: {bbox}\")\n",
    "    \n",
    "    # Get the countries in the selected region\n",
    "    countries_in_region = conn.execute(f\"\"\"\n",
    "    SELECT DISTINCT country_iso\n",
    "    FROM country_geoms\n",
    "    WHERE ST_Intersects(geometry, ST_MakeEnvelope({bbox['xmin']}, {bbox['ymin']}, {bbox['xmax']}, {bbox['ymax']}));\"\"\").fetchall()\n",
    "    countries_in_region = [row[0] for row in countries_in_region]\n",
    "    countries_with_pv = conn.execute(f\"\"\"SELECT COUNT(DISTINCT division_country) FROM {TABLE_NAME} WHERE division_country IN ({', '.join([f'\\'{iso}\\'' for iso in countries_in_region])});\"\"\").fetchall()[0][0]\n",
    "    countries_gdf = get_countries_gdf(countries_in_region)\n",
    "    print(f\"Region bbox contains {countries_with_pv} / {len(countries_gdf)} countries with PV labels.\")\n",
    "    # countries_gdf.plot(column=\"country_pv_area_m2\", cmap='OrRd', legend=True)\n",
    "    with render_out:\n",
    "        # Clear previous output\n",
    "        render_out.clear_output(wait=True)\n",
    "        # Create the map\n",
    "        # m = None\n",
    "        m = get_folium_region_map(countries_gdf, bbox)\n",
    "        # display the map\n",
    "        display(m)\n",
    "    display(render_out)\n",
    "# plot_region_bbox_global(region_dropdown.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3867a8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a simple chloropleth map of the countries in the selected region\n",
    "# generate a simple chloropleth map of the countries in the selected region\n",
    "# Common cmap options include: 'viridis', 'plasma', 'inferno', 'magma', 'cividis', \n",
    "# 'Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds', \n",
    "# 'YlOrBr', 'YlOrRd', 'PuRd', 'RdPu', 'BuPu', 'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn'\n",
    "# Diverging: 'coolwarm', 'bwr', 'seismic'\n",
    "# Qualitative: 'tab10', 'tab20', 'Paired', 'Set1', 'Set2', 'Set3'\n",
    "print(region_dropdown.value)\n",
    "# TODO: fix region update\n",
    "bbox = region_bboxes[region_dropdown.value]\n",
    "countries_in_region = conn.execute(f\"\"\"\n",
    "    SELECT DISTINCT country_iso\n",
    "    FROM country_geoms\n",
    "    WHERE ST_Intersects(geometry, ST_MakeEnvelope({bbox['xmin']}, {bbox['ymin']}, {bbox['xmax']}, {bbox['ymax']}));\"\"\").fetchall()\n",
    "countries_in_region = [row[0] for row in countries_in_region]\n",
    "print(countries_in_region)\n",
    "countries_gdf = get_countries_gdf(countries_in_region)\n",
    "countries_gdf.plot(column=\"country_pv_area_m2\", cmap='viridis', legend=True, legend_kwds={'label': \"Total PV Area (m^2)\", 'orientation': \"horizontal\"}, figsize=(10, 6))\n",
    "plt.title(\"Countries colored by total PV area (m^2)\")\n",
    "# remove axes\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c31b377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive plot to use geopandas' plot function\n",
    "country_name_to_iso = get_countries_in_bbox(region_bboxes[region_dropdown.value])\n",
    "random_default = random.choice(list(country_name_to_iso.keys()))\n",
    "country_select = widgets.Dropdown(\n",
    "    options=list(country_name_to_iso.keys()),\n",
    "    description='Country:',\n",
    "    value=random_default,\n",
    "    disabled=False,\n",
    "    layout=Layout(width='70%', margin='20 20 auto 20 20')\n",
    ")\n",
    "\n",
    "# temporarily \n",
    "\n",
    "# TODO: add region geoms so plot is meaningful\n",
    "@interact(country_name=country_select)\n",
    "def plot_country(country_name):\n",
    "    # Get the ISO code for the selected country\n",
    "    iso_code = country_name_to_iso[country_name]\n",
    "    \n",
    "    # Filter the GeoDataFrame for the selected country\n",
    "    filtered_gdf = get_countries_gdf([iso_code])\n",
    "    \n",
    "    # Plot the filtered GeoDataFrame\n",
    "    fig, ax = plt.subplots(figsize=(8, 12))\n",
    "    ax.set_title(f\"Countries colored by count of PV labels\")\n",
    "    ax.set_axis_off()\n",
    "    filtered_gdf.plot(column=\"country_pv_count\", ax=ax, legend=True, cmap='PuRd')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f905467",
   "metadata": {},
   "source": [
    "## Lonboard Visualization Functions\n",
    "\n",
    "Lonboard is a GPU-accelerated geospatial visualization library that's excellent for handling very large datasets. It's particularly useful for creating high-performance interactive visualizations of millions of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd4b2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %config SqlMagic.autopandas = True\n",
    "\n",
    "# 1. Get ISO codes\n",
    "# Countries for filtering\n",
    "country_isos = get_countries_in_bbox(region_bboxes[region_dropdown.value])\n",
    "div_countries = list(country_isos.keys())\n",
    "\n",
    "# valid data for H3ClusterLayer\n",
    "\n",
    "# --- Database Connection and Query ---\n",
    "# db_path = str(out_consolidated_db) # Use db path from CELL 24/49\n",
    "pv_table_name = TABLE_NAME\n",
    "h3_col = f\"h3_res_{common_h3_res}\"\n",
    " \n",
    "\n",
    "# h3_agg_df = pd.DataFrame(columns=[h3_col, 'region' 'pv_count', 'total_area_m2', 'hex_ids'])\n",
    "\n",
    "view_lat, view_lon = 48.0, 9.0  # Default center (Central Europe)\n",
    "    \n",
    "# Create country selection dropdown\n",
    "country_dropdown = widgets.Dropdown(\n",
    "    options=div_countries,\n",
    "    value=div_countries[0] if div_countries else None,\n",
    "    description='Country:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "def get_country_hex_cells(gdf, country_iso, h3_col):\n",
    "    # aggregate our pv labels by h3_col and filter to country selected in dropdown\n",
    "    h3_agg_query = f\"\"\"\n",
    "    SELECT \n",
    "        h3_h3_to_string({h3_col}) as hex, division_region as region,\n",
    "        COUNT(*) AS pv_count, SUM(area_m2) AS total_area_m2\n",
    "    FROM {TABLE_NAME}\n",
    "    -- only aggregate one type of division to avoid double counting pv labels that match with multiple divisions (almost all)\n",
    "    WHERE division_country = '{country_isos[country_dropdown.value]}' and division_subtype = 'region'\n",
    "    GROUP BY {h3_col}, division_region\n",
    "    -- HAVING COUNT(*) > 0\n",
    "    \"\"\"\n",
    "    # conn.install_extension(\"h3\", \"community\")\n",
    "    # conn.load_extension(\"h3\")\n",
    "    results = conn.execute(h3_agg_query).fetchall()\n",
    "    # convert to DataFrame\n",
    "    gdf = pd.DataFrame(results, columns=gdf.columns)\n",
    "    # make sure hex is a string\n",
    "    gdf['hex'] = gdf['hex'].astype(str)\n",
    "    print(gdf['hex'].head())\n",
    "    # gdf = gpd.GeoDataFrame(\n",
    "    #     gdf,\n",
    "    #     geometry=gpd.GeoSeries.from_wkt(gdf['hex'].apply(h3.cell_to_boundary)),\n",
    "    #     crs=\"EPSG:4326\"\n",
    "    # )\n",
    "\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7386f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Helper Functions\n",
    "\n",
    "def get_country_iso_codes() -> dict:\n",
    "    \"\"\"Gets ISO 3166-1 alpha-2 codes for a list of country names.\"\"\"\n",
    "    # (Function remains the same as before)\n",
    "    # Countries for filtering; only include countries that are in our PV labels\n",
    "    div_countries = conn.execute(f\"\"\"SELECT DISTINCT division_country FROM country_geoms;\"\"\").fetchall()\n",
    "    div_country_isos = [row[0] for row in div_countries]\n",
    "    div_countries = []\n",
    "    for iso in div_country_isos:\n",
    "        try:\n",
    "            country = pycountry.countries.get(alpha_2=iso)\n",
    "            if country:\n",
    "                div_countries.append(country.name)\n",
    "            else:\n",
    "                div_countries.append(iso)\n",
    "        except:\n",
    "            div_countries.append(iso)\n",
    "\n",
    "    div_countries = [pycountry.countries.get(alpha_2=iso).name for iso in div_countries]\n",
    "    country_name_to_iso = dict(zip(div_countries, div_country_isos))\n",
    "\n",
    "    return country_name_to_iso\n",
    "\n",
    "def get_country_centroid(country_iso: str, db_conn: duckdb.DuckDBPyConnection) -> tuple | None:\n",
    "    \"\"\"Fetches the centroid coordinates for a given country ISO code.\"\"\"\n",
    "    # (Function remains the same as before)\n",
    "    if not db_conn: return None\n",
    "    try:\n",
    "        centroid_query = f\"\"\"\n",
    "        SELECT ST_Y(country_centroid) AS country_lat, ST_X(country_centroid) AS country_lon\n",
    "        FROM (\n",
    "            SELECT ST_Centroid(ST_MakeEnvelope(div_bbox.xmin, div_bbox.ymin, div_bbox.xmax, div_bbox.ymax)) AS country_centroid\n",
    "            FROM {pv_table_name} WHERE division_country = ? AND division_subtype = 'country' LIMIT 1\n",
    "        ) AS subquery;\n",
    "        \"\"\"\n",
    "        result = db_conn.execute(centroid_query, [country_iso]).fetchone()\n",
    "        return (result[0], result[1]) if result else (48.0, 9.0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching centroid for {country_iso}: {e}\")\n",
    "        return (48.0, 9.0)\n",
    "\n",
    "def get_country_geometry(country_iso: str, db_conn: duckdb.DuckDBPyConnection) -> Polygon | None:\n",
    "    \"\"\"Fetches the primary administrative boundary geometry for a country.\"\"\"\n",
    "    if not db_conn:\n",
    "        print(\"Database connection is not available for geometry fetch.\")\n",
    "        return None\n",
    "    try:\n",
    "        # Fetch all divisions for the country\n",
    "        geom_query = f\"\"\"\n",
    "        SELECT ST_AsWKB(geometry)\n",
    "        FROM {ov_divisions_table}\n",
    "        where country = '{country_iso}' and subtype = 'country';\n",
    "        \"\"\"\n",
    "        result = db_conn.execute(geom_query).fetchone()\n",
    "        if result and result[0]:\n",
    "            # Convert WKB back to Shapely geometry\n",
    "            from shapely import wkb\n",
    "            country_geom = wkb.loads(result[0])\n",
    "            print(f\"Fetched geometry for {country_iso} (Type: {country_geom.geom_type})\")\n",
    "            return country_geom\n",
    "        else:\n",
    "            print(f\"Warning: Could not find geometry for country ISO: {country_iso}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching geometry for {country_iso}: {e}\")\n",
    "        return None\n",
    "\n",
    "def safe_h3_to_polygon(h3_index: str) -> Polygon | None:\n",
    "    \"\"\"Safely converts an H3 index string to a Shapely Polygon.\"\"\"\n",
    "    # (Function remains the same as before)\n",
    "    try:\n",
    "        boundary_lon_lat = h3.cell_to_boundary(h3_index, geo_json=True)\n",
    "        return Polygon(boundary_lon_lat)\n",
    "    except ValueError: return None\n",
    "    except Exception: return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412d764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country_hex_data_as_gdf(country_iso: str, target_h3_res: int, db_conn: duckdb.DuckDBPyConnection) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Gets all H3 cells within a country's boundary using h3.h3_shape_to_cells and __geo_interface__,\n",
    "    joins PV aggregation data, adds geometry using h3pandas,\n",
    "    and returns a GeoDataFrame.\n",
    "    \"\"\"\n",
    "    if not db_conn:\n",
    "        print(\"Database connection is not available.\")\n",
    "        return gpd.GeoDataFrame()\n",
    "\n",
    "    # 1. Get Country Geometry\n",
    "    country_geom = get_country_geometry(country_iso, db_conn)\n",
    "    # print(f\"Country geometry for {country_iso}: {country_geom}\")\n",
    "    if not country_geom:\n",
    "        return gpd.GeoDataFrame()\n",
    "\n",
    "    # 2. Get all H3 cells covering the geometry using h3.h3_shape_to_cells and __geo_interface__\n",
    "    try:\n",
    "        # Get the GeoJSON-like dictionary from the Shapely object\n",
    "        country_geojson_interface = country_geom.__geo_interface__\n",
    "        h3shape = h3.geo_to_h3shape(country_geojson_interface)\n",
    "        # h3shape = h3.geo_to_h3shape(country_geom)\n",
    "        print(f\"Filling geometry with H3 cells using h3_shape_to_cells at resolution {target_h3_res}...\")\n",
    "        # Use geo_json_conformant=True because __geo_interface__ uses [lon, lat] order\n",
    "        all_cells_set = h3.h3shape_to_cells(h3shape, target_h3_res)\n",
    "        # print(all_cells_set)\n",
    "        if not all_cells_set:\n",
    "             print(f\"Warning: h3.h3_shape_to_cells returned no cells for {country_iso}.\")\n",
    "             return gpd.GeoDataFrame()\n",
    "        all_cells_list = list(all_cells_set) # Convert set to list\n",
    "        print(f\"Found {len(all_cells_list)} H3 cells covering {country_iso}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during h3.h3_shape_to_cells for {country_iso}: {e}\")\n",
    "        return gpd.GeoDataFrame()\n",
    "\n",
    "    # 3. Create DataFrame of all H3 cells and set H3 index for h3pandas\n",
    "    all_country_cells_df = pd.DataFrame({'hex_id': all_cells_list})\n",
    "\n",
    "    # 4. Aggregate PV data for these cells from DuckDB\n",
    "    h3_col_in_pv_table = f\"h3_res_5\" # Use the global resolution variable\n",
    "    db_conn.execute(f\"\"\"\n",
    "    CREATE TEMPORARY TABLE df_cells AS\n",
    "    SELECT h3_string_to_h3(hex_id) as hex_id, ST_GeomFromText(h3_cell_to_boundary_wkt(hex_id)) as geometry\n",
    "    FROM all_country_cells_df;\"\"\")\n",
    "\n",
    "    h3_agg_query = f\"\"\"\n",
    "    SELECT\n",
    "        {h3_col_in_pv_table} as hex,\n",
    "        COUNT(*) AS pv_count,\n",
    "        SUM(area_m2) AS total_area_m2\n",
    "    FROM {pv_table_name}\n",
    "    INNER JOIN df_cells ON {pv_table_name}.{h3_col_in_pv_table} = df_cells.hex_id\n",
    "    GROUP BY 1;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Aggregating PV data for relevant cells in {country_iso}...\")\n",
    "        pv_agg_df = db_conn.execute(h3_agg_query).df()\n",
    "        \n",
    "        print(f\"Found PV data for {len(pv_agg_df)} cells.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error aggregating PV data: {e}\")\n",
    "        pv_agg_df = pd.DataFrame(columns=['hex', 'pv_count', 'total_area_m2'])\n",
    "\n",
    "    # 5. Use h3pandas to add geometry boundary to the DataFrame of all cells\n",
    "    print(\"Generating hexagon boundary geometries...\")\n",
    "    try:\n",
    "        # get boundary for each hex cell\n",
    "        all_country_cells_df['geometry'] = all_country_cells_df['hex_id'].apply(safe_h3_to_polygon)\n",
    "        # drop rows where polygon conversion failed\n",
    "        all_country_cells_df = all_country_cells_df.dropna(subset=['geometry'])\n",
    "        # all_country_cells_df = all_country_cells_df.reset_index().rename(columns={'hex_id': 'hex'})\n",
    "        gdf_all_cells = gpd.GeoDataFrame(all_country_cells_df,\n",
    "                                         geometry=all_country_cells_df['geometry'].apply(wkt.loads),\n",
    "                                         crs='EPSG:4326')\n",
    "        print(f\"Generated geometries for {len(gdf_all_cells)} cells.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error using h3 to generate geometries: {e}\")\n",
    "        return gpd.GeoDataFrame()\n",
    "\n",
    "    # 6. Left join the GeoDataFrame with aggregated PV data\n",
    "    print(\"Joining all country cells (with geometry) with PV aggregates...\")\n",
    "    if not pv_agg_df.empty:\n",
    "        pv_agg_df['hex'] = pv_agg_df['hex'].astype(str)\n",
    "    merged_gdf = pd.merge(gdf_all_cells, pv_agg_df, on='hex', how='left')\n",
    "\n",
    "    # 7. Fill NaNs and set CRS\n",
    "    merged_gdf['pv_count'] = merged_gdf['pv_count'].fillna(0).astype(int)\n",
    "    merged_gdf['total_area_m2'] = merged_gdf['total_area_m2'].fillna(0).astype(float)\n",
    "    merged_gdf.crs = 'EPSG:4326'\n",
    "    print(f\"Created final GeoDataFrame with {len(merged_gdf)} polygons for {country_iso}.\")\n",
    "    return merged_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c04ec3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %% Visualization Function (Lonboard)\n",
    "\n",
    "def create_lonboard_viz(gdf: gpd.GeoDataFrame, view_lat: float, view_lon: float) -> lonboard.Map | None:\n",
    "    \"\"\"Creates the Lonboard PolygonLayer visualization.\"\"\"\n",
    "    # (Function remains the same as the last working version)\n",
    "    if gdf.empty:\n",
    "        print(\"No data to visualize.\")\n",
    "        return None\n",
    "    try:\n",
    "        pv_counts_numeric = pd.to_numeric(gdf['pv_count'], errors='coerce').fillna(0)\n",
    "        values_array = pv_counts_numeric.to_numpy(dtype=float)\n",
    "        fill_colors_rgba = lonboard.colormap.apply_continuous_cmap(\n",
    "            values_array, cmap=cm.viridis, alpha=0.75\n",
    "        )\n",
    "        polygon_layer = lonboard.PolygonLayer.from_geopandas(\n",
    "            gdf,\n",
    "            get_fill_color=fill_colors_rgba,\n",
    "            get_line_color=[255, 255, 255, 80],\n",
    "            line_width_min_pixels=0.5,\n",
    "            pickable=True,\n",
    "            auto_highlight=True,\n",
    "        )\n",
    "        view_state_params = {\"latitude\": view_lat, \"longitude\": view_lon, \"zoom\": 6, \"pitch\": 30, \"bearing\": 0}\n",
    "        m = lonboard.Map(layers=[polygon_layer], view_state=view_state_params)\n",
    "        return m\n",
    "    except ImportError:\n",
    "        print(\"Error: Required libraries not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating lonboard visualization: {e}\")\n",
    "        raise e # Re-raise to see full traceback\n",
    "\n",
    "# %% Main Execution Logic (with Widgets)\n",
    "\n",
    "# 1. Get map from Country Name -> ISO code\n",
    "bbox = country_select.value\n",
    "country_name_iso_map = get_countries_in_bbox(region_bboxes[region_dropdown.value])\n",
    "#  filter to countries that have pv_count > 0\n",
    "countries_with_pv = conn.execute(f\"\"\"SELECT DISTINCT division_country FROM {TABLE_NAME};\"\"\").fetchall()\n",
    "countries_with_pv = set(sorted([row[0] for row in countries_with_pv if row[0] is not None]))\n",
    "# print(f\"Countries with PV labels: {countries_with_pv}\")\n",
    "# print(country_name_iso_map)\n",
    "valid_countries = {k: v for k, v in country_name_iso_map.items() if v in countries_with_pv}\n",
    "# print(f\"Valid countries with PV labels: {valid_countries}\")\n",
    "# region_countries = sorted(list(country_name_iso_map.keys()))\n",
    "valid_countries = sorted(list(valid_countries.keys())) # Sort for usability\n",
    "\n",
    "# 2. Create Widgets\n",
    "country_dropdown = widgets.Dropdown(\n",
    "    options=valid_countries,\n",
    "    value=valid_countries[0] if valid_countries else None,\n",
    "    description='Country:',\n",
    "    disabled=False if valid_countries else True,\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "grid_res = widgets.IntSlider(\n",
    "    value=common_h3_res,\n",
    "    min=0,\n",
    "    max=15,\n",
    "    step=1,\n",
    "    description='H3 grid resolution:')\n",
    "\n",
    "# Output widget to display the map\n",
    "map_output = widgets.Output()\n",
    "\n",
    "# 3. Define Update Function for Visualization\n",
    "def update_visualization(selected_country, h3_resolution=5):\n",
    "    \"\"\"Fetches data and updates the map in the output widget.\"\"\"\n",
    "    with map_output:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"--- Updating map for: {selected_country} ---\")\n",
    "        conn = get_duckdb_connection(db_file)\n",
    "        if 'conn' not in locals() or conn is None:\n",
    "             print(\"Database connection error. Cannot update map.\")\n",
    "             return\n",
    "\n",
    "        country_iso = country_name_iso_map.get(selected_country) # Use the map\n",
    "        print(f\"Selected country ISO: {country_iso}\")\n",
    "        if not country_iso:\n",
    "            print(f\"ISO code not found for {selected_country}.\")\n",
    "            return\n",
    "\n",
    "        # Fetch aggregated hex data AS GeoDataFrame using the new logic\n",
    "        country_hex_gdf = get_country_hex_data_as_gdf(country_iso, h3_resolution, conn)\n",
    "        print(f\"Fetched {len(country_hex_gdf)} hex cells for {selected_country}.\")\n",
    "\n",
    "        # Fetch centroid\n",
    "        center_lat, center_lon = get_country_centroid(country_iso, conn)\n",
    "\n",
    "        if center_lat is None or center_lon is None:\n",
    "            print(\"Could not determine map center.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Map centered on {selected_country}: Lat={center_lat:.4f}, Lon={center_lon:.4f}\")\n",
    "\n",
    "        # Create and display the Lonboard visualization\n",
    "        lonboard_map = create_lonboard_viz(country_hex_gdf, center_lat, center_lon)\n",
    "\n",
    "        if lonboard_map:\n",
    "            print(f\"Displaying Lonboard map for {selected_country}...\")\n",
    "            display(lonboard_map)\n",
    "        else:\n",
    "            print(f\"No visualization generated for {selected_country}.\")\n",
    "        print(f\"--- Finished update for: {selected_country} ---\")\n",
    "\n",
    "# 4. Link Dropdown to Update Function using interactive_output\n",
    "widgets.interactive_output(update_visualization, {'selected_country': country_dropdown, 'h3_resolution': grid_res})\n",
    "\n",
    "# 5. Display Widgets\n",
    "print(\"Select a country from the dropdown to visualize its H3 grid using Lonboard.\")\n",
    "display(country_dropdown, grid_res, map_output)\n",
    "# Trigger initial display for the default country if available\n",
    "if country_dropdown.value:\n",
    "    update_visualization(country_dropdown.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088385e8-4558-4b7a-9da5-0aae2855be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_iso = country_name_iso_map.get(country_dropdown.value)\n",
    "country_geom = get_country_geometry(country_iso, conn)\n",
    "country_geojson_interface = country_geom.__geo_interface__\n",
    "h3shape = h3.geo_to_h3shape(country_geojson_interface)\n",
    "all_cells_set = h3.h3shape_to_cells(h3shape, common_h3_res)\n",
    "all_cells_list = list(all_cells_set) # Convert set to list\n",
    "all_country_cells_df = pd.DataFrame({'hex_id': all_cells_list})\n",
    "\n",
    "import shapely\n",
    "h3_col_in_pv_table = f\"h3_res_{common_h3_res}\" # Use the global resolution variable\n",
    "conn.execute(f\"\"\"CREATE OR REPLACE TEMPORARY TABLE df_cells AS\n",
    "    SELECT h3_string_to_h3(hex_id) as hex_id, ST_GeomFromText(h3_cell_to_boundary_wkt(hex_id)) as geometry\n",
    "    FROM all_country_cells_df;\"\"\")\n",
    "\n",
    "# validate that any of the h3 cells intersect with the source country geometry\n",
    "val_df = conn.execute(f\"\"\"SELECT * FROM df_cells;\"\"\").df()\n",
    "# convert to gdf \n",
    "val_gdf = gpd.GeoDataFrame(\n",
    "    val_df,\n",
    "    geometry=val_df['geometry'],\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "# create country gdf from geo_interface\n",
    "country_gdf = gpd.GeoDataFrame(\n",
    "    geometry=[country_geom],\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "# check if any of the cells intersect with the country geometry\n",
    "intersects = val_gdf[val_gdf.intersects(country_gdf.unary_union)]\n",
    "\n",
    "h3_agg_query = f\"\"\"\n",
    "SELECT\n",
    "    {h3_col_in_pv_table} as hex,\n",
    "    COUNT(*) AS pv_count,\n",
    "    SUM(area_m2) AS total_area_m2\n",
    "FROM {pv_table_name}\n",
    "INNER JOIN df_cells ON ST_Intersects(ST_GeomFromText(h3_cell_to_boundary_wkt({h3_col_in_pv_table})), df_cells.geometry)\n",
    "GROUP BY 1;\n",
    "\"\"\"\n",
    "pv_agg_df = conn.execute(h3_agg_query).df()\n",
    "pv_agg_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e12afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_geom.__geo_interface__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66ec513-80b1-4245-8a35-00cad161e6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_cells = conn.execute(f\"SELECT h3_h3_to_string({h3_col_in_pv_table}) FROM {TABLE_NAME};\").fetchall()\n",
    "pv_cells = set([row[0] for row in pv_cells])\n",
    "print(pv_cells[:5])\n",
    "print(all_cells_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c9b7a8-bc5e-4e0d-a9cf-cbc34fa5b877",
   "metadata": {},
   "outputs": [],
   "source": [
    "any(cell in pv_cells for cell in all_cells_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cbbf69",
   "metadata": {},
   "source": [
    "## PyDeck Globe View and other layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7ff4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get country geoms as df\n",
    "bbox = region_bboxes[region_dropdown.value]\n",
    "countries_in_region = conn.execute(f\"\"\"\n",
    "    SELECT DISTINCT country_iso\n",
    "    FROM country_geoms\n",
    "    WHERE ST_Intersects(geometry, ST_MakeEnvelope({bbox['xmin']}, {bbox['ymin']}, {bbox['xmax']}, {bbox['ymax']}));\"\"\").fetchall()\n",
    "countries_in_region = [row[0] for row in countries_in_region]\n",
    "globe_countries = countries_in_region\n",
    "# globe_countries = [row[0] for row in globe_countries]\n",
    "# get the countries in the selected region\n",
    "countries_gdf = get_countries_gdf(globe_countries)\n",
    "display(countries_gdf.head(3))\n",
    "# ds_gdf_h3['geometry'] = ds_gdf_h3['geometry'].apply(wkt.loads)\n",
    "\n",
    "layers = [\n",
    "    pdk.Layer(\n",
    "        \"GeoJsonLayer\",\n",
    "        id=\"base-map\",\n",
    "        data=countries_gdf.__geo_interface__,\n",
    "        stroked=False,\n",
    "        filled=True,\n",
    "        get_fill_color=[200, 200, 200],\n",
    "    ),\n",
    "    pdk.Layer(\n",
    "        \"ColumnLayer\",\n",
    "        id=\"unified_id\",\n",
    "        data=ds_gdf_h3,\n",
    "        get_elevation=\"area_m2\",\n",
    "        get_position=[\"centroid_lon\", \"centroid_lat\"],\n",
    "        elevation_scale=2,\n",
    "        pickable=True,\n",
    "        auto_highlight=True,\n",
    "        radius=1000,\n",
    "        get_fill_color=\"dataset\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# get init view from centroid union of all countries\n",
    "init_lat, init_lon = countries_gdf.geometry.union_all().centroid.y, countries_gdf.geometry.union_all().centroid.x\n",
    "view_state = pdk.ViewState(latitude=init_lat, longitude=init_lon, zoom=2, min_zoom=2, pitch=5, bearing=0)\n",
    "# Set height and width variables\n",
    "view = pdk.View(type=\"_GlobeView\", controller=True, width=800, height=500)\n",
    "\n",
    "pdk_globe = pdk.Deck(\n",
    "    views=[view],\n",
    "    initial_view_state=view_state,\n",
    "    tooltip={\"Dataset\": \"{dataset}, {area_m2}m² plant in {division_country}\"},\n",
    "    layers=layers,\n",
    "    map_provider=None,\n",
    "    # Note that this must be set for the globe to be opaque\n",
    "    parameters={\"cull\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfb8e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_gdf_h3.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f540f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pdk_globe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4d47a9",
   "metadata": {},
   "source": [
    "### [Optional] Drop Overture Maps Divisions \n",
    "\n",
    "Now that we have the metadata for the Overture Maps Divisions that matches our PV labels, we can drop the \n",
    "large (~6.4GB without filtering by country or subtype) Divisions table from our DuckDB database and simply query the Overture Maps S3 bucket for the relevant division geometries when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd56a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "\n",
    "# display large widget text and large warning button in a layout \n",
    "# Create a warning message with HTML widget\n",
    "warning_text = widgets.HTML(\n",
    "    value=\"\"\"\n",
    "    <div style=\"color: #d9534f; font-size: 1.5em; font-weight: bold; text-align: center; \n",
    "               padding: 20px; border: 2px solid #d9534f; border-radius: 10px; background-color: #f9f2f4;\">\n",
    "    ⚠️ WARNING: This will permanently delete the Overture Maps Divisions table (~6.4GB) from the database and will need to be re-downloaded.<br>\n",
    "    Only do this if you're SURE you no longer need direct access to the division geometries.\n",
    "    </div>\n",
    "    \"\"\",\n",
    "    layout=widgets.Layout(margin=\"10px 0px 20px 0px\")\n",
    ")\n",
    "\n",
    "# Create a drop table button with warning styling\n",
    "drop_button = widgets.Button(\n",
    "    description=\"DROP Overture Divisions Table\",\n",
    "    button_style=\"danger\",\n",
    "    icon=\"trash\",\n",
    "    layout=widgets.Layout(width=\"80%\", height=\"60px\", margin=\"0 auto\", \n",
    "                          font_weight=\"bold\", font_size=\"1.2em\")\n",
    ")\n",
    "\n",
    "# Function to drop the table\n",
    "def drop_divisions_table(b):\n",
    "    \n",
    "    # Create output area to show results\n",
    "    output = widgets.Output()\n",
    "    display(output)\n",
    "    \n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Dropping table {ov_divisions_table}...\")\n",
    "        try:\n",
    "            # Use SQL magic to drop the table\n",
    "            %sql DROP TABLE IF EXISTS {{ov_divisions_table}};\n",
    "            print(f\"✅ Table {ov_divisions_table} successfully dropped!\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error dropping table: {e}\")\n",
    "\n",
    "# Connect the function to the button click event\n",
    "drop_button.on_click(drop_divisions_table)\n",
    "\n",
    "# Create the final layout\n",
    "widgets.VBox([warning_text, drop_button], \n",
    "            layout=widgets.Layout(width=\"70%\", margin=\"0 auto\", padding=\"20px\", \n",
    "                                 border=\"1px solid #d9534f\", border_radius=\"10px\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443f7f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stac_items = fetch_cubo_stac_rasters_samples(ds_gdf_h3, visualize_set=True, sample_size=5)\n",
    "stac_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3260f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize one of the samples\n",
    "np.random.seed(42)\n",
    "# print(pv_da)\n",
    "# pv_da = samples.isel(time=0)\n",
    "pv_uniq = stac_items.groupby('time').first()\n",
    "(pv_uniq.sel(band=[\"B04\",\"B03\",\"B02\"])/5000).clip(0,1).plot.imshow(col=\"time\",col_wrap = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263c5aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_temporal_animation(stacked_data, output_path=\"pv_animation.gif\"):\n",
    "#     \"\"\"Create an animated GIF showing temporal changes\"\"\"\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     import matplotlib.animation as animation\n",
    "    \n",
    "#     # Prepare RGB data\n",
    "#     rgb_data = []\n",
    "#     for t in stacked_data.time.values:\n",
    "#         # rgb = visualize_enhanced_rgb(stacked_data.sel(time=t))\n",
    "#         rgb_data.append(rgb.transpose(\"y\", \"x\", \"band\").values)\n",
    "    \n",
    "#     # Create animation\n",
    "#     fig, ax = plt.figure(figsize=(10, 10)), plt.gca()\n",
    "    \n",
    "#     # Create initial image\n",
    "#     im = ax.imshow(rgb_data[0])\n",
    "#     ax.set_axis_off()\n",
    "    \n",
    "#     # Define update function for animation\n",
    "#     def update(frame):\n",
    "#         im.set_array(rgb_data[frame])\n",
    "#         ax.set_title(f\"Date: {stacked_data.time.values[frame]}\")\n",
    "#         return [im]\n",
    "    \n",
    "#     # Create and save animation\n",
    "#     ani = animation.FuncAnimation(fig, update, frames=len(rgb_data), interval=500)\n",
    "#     ani.save(output_path, writer='pillow')\n",
    "    \n",
    "#     return ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1a5ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_temporal_animation(stac_items[0], output_path=\"pv_animation.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0271880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pystac_client\n",
    "# import pystac\n",
    "# import geopandas as gpd\n",
    "# from shapely.geometry import box\n",
    "# import matplotlib.pyplot as plt\n",
    "# import warnings\n",
    "\n",
    "# # Suppress specific UserWarning from pystac_client about ItemSearch link\n",
    "# warnings.filterwarnings(\"ignore\", message=\"Server does not conform to ITEM_SEARCH.*\", category=UserWarning)\n",
    "\n",
    "# # --- Configuration ---\n",
    "# MAXAR_OPEN_DATA_CATALOG_URL = \"https://maxar-opendata.s3.amazonaws.com/events/catalog.json\"\n",
    "\n",
    "# # --- STAC Traversal ---\n",
    "# print(f\"Connecting to catalog: {MAXAR_OPEN_DATA_CATALOG_URL}...\")\n",
    "# all_items = []\n",
    "# try:\n",
    "#     # Open the root STAC Catalog\n",
    "#     root_catalog = pystac_client.Client.open(MAXAR_OPEN_DATA_CATALOG_URL)\n",
    "#     print(\"Successfully connected to root catalog.\")\n",
    "\n",
    "#     # Get top-level children (likely Collections or Catalogs representing events)\n",
    "#     print(\"Fetching top-level children (Events)...\")\n",
    "#     top_children = list(root_catalog.get_children())\n",
    "#     print(f\"Found {len(top_children)} top-level children (Events).\")\n",
    "\n",
    "#     # Iterate through the top-level children (Events)\n",
    "#     for event_child in top_children:\n",
    "#         print(f\"\\nProcessing Event Child: {event_child.id} ({type(event_child)})...\")\n",
    "#         try:\n",
    "#             # Attempt to get items directly from this event child\n",
    "#             # get_items() should work for both Catalog and Collection objects if they contain items\n",
    "#             print(f\"  Fetching items from {event_child.id}...\")\n",
    "#             # Resolve the child first if it's just a link (get_children often returns resolved)\n",
    "#             # event_child.resolve_stac_object() # Might be needed if get_children returns unresolved links\n",
    "\n",
    "#             items_in_event = list(event_child.get_items(recursive=False)) # Use recursive=True just in case\n",
    "#             if items_in_event:\n",
    "#                  print(f\"    Found {len(items_in_event)} items directly in {event_child.id}.\")\n",
    "#                  all_items.extend(items_in_event)\n",
    "#           #   else:\n",
    "#           #        # If no items directly, check if it has children itself (deeper nesting)\n",
    "#           #        print(f\"    No items found directly in {event_child.id}. Checking for sub-children...\")\n",
    "#           #        try:\n",
    "#           #             sub_children = list(event_child.get_children())\n",
    "#           #             if sub_children:\n",
    "#           #                  print(f\"    Found {len(sub_children)} sub-children in {event_child.id}.\")\n",
    "#           #                  for sub_child in sub_children:\n",
    "#           #                       print(f\"      Fetching items from sub-child: {sub_child.id}...\")\n",
    "#           #                       try:\n",
    "#           #                            items_in_sub = list(sub_child.get_items(recursive=True))\n",
    "#           #                            print(f\"        Found {len(items_in_sub)} items in {sub_child.id}.\")\n",
    "#           #                            all_items.extend(items_in_sub)\n",
    "#           #                       except Exception as sub_item_err:\n",
    "#           #                            print(f\"        Error fetching items from {sub_child.id}: {sub_item_err}\")\n",
    "#           #             else:\n",
    "#           #                  print(f\"    No sub-children found in {event_child.id}.\")\n",
    "#           #        except Exception as sub_child_err:\n",
    "#           #             print(f\"    Error fetching sub-children for {event_child.id}: {sub_child_err}\")\n",
    "\n",
    "#         except AttributeError as ae:\n",
    "#              # Handle cases where an object might not have get_items or get_children\n",
    "#              print(f\"  Skipping {event_child.id}, does not appear to be a Catalog/Collection or error occurred: {ae}\")\n",
    "#         except Exception as event_err:\n",
    "#             print(f\"  Error processing {event_child.id}: {event_err}\")\n",
    "\n",
    "\n",
    "#     print(f\"\\nTotal items fetched from all events: {len(all_items)}\")\n",
    "\n",
    "#     # --- Data Extraction and Preparation --- (Same as before)\n",
    "#     if not all_items:\n",
    "#         print(\"No items found after traversing. Exiting.\")\n",
    "#     else:\n",
    "#         print(\"Extracting bounding boxes...\")\n",
    "#         bboxes = []\n",
    "#         item_ids = []\n",
    "#         for item in all_items:\n",
    "#             if item.bbox:\n",
    "#                 bboxes.append(item.bbox)\n",
    "#                 item_ids.append(item.id)\n",
    "#             else:\n",
    "#                 # It's also possible geometry exists but not bbox\n",
    "#                 if item.geometry:\n",
    "#                      try:\n",
    "#                           # Calculate bbox from geometry (more expensive)\n",
    "#                           min_lon, min_lat, max_lon, max_lat = gpd.GeoSeries([item.geometry], crs=\"EPSG:4326\").total_bounds\n",
    "#                           bboxes.append([min_lon, min_lat, max_lon, max_lat])\n",
    "#                           item_ids.append(item.id)\n",
    "#                           print(f\"Info: Item {item.id} using geometry bounds.\")\n",
    "#                      except Exception as geom_err:\n",
    "#                           print(f\"Warning: Item {item.id} has no bbox and failed to get bounds from geometry: {geom_err}\")\n",
    "#                 else:\n",
    "#                      print(f\"Warning: Item {item.id} has no bbox or geometry.\")\n",
    "\n",
    "\n",
    "#         if not bboxes:\n",
    "#              print(\"No items with bounding boxes found. Cannot plot.\")\n",
    "#         else:\n",
    "#             # Convert bounding boxes (min_lon, min_lat, max_lon, max_lat) to Shapely polygons\n",
    "#             geometries = [box(min_lon, min_lat, max_lon, max_lat) for min_lon, min_lat, max_lon, max_lat in bboxes]\n",
    "\n",
    "#             # Create a GeoDataFrame\n",
    "#             gdf_items = gpd.GeoDataFrame({'id': item_ids, 'geometry': geometries}, crs=\"EPSG:4326\")\n",
    "#             print(f\"Created GeoDataFrame with {len(gdf_items)} item extents.\")\n",
    "#             print(gdf_items.head()) # Display first few rows\n",
    "\n",
    "#             # --- Plotting --- (Same as before)\n",
    "#             print(\"Plotting item extents...\")\n",
    "#             world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "#             fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "#             world.plot(ax=ax, color='lightgray', edgecolor='black')\n",
    "#             gdf_items.plot(ax=ax, color='red', alpha=0.5, edgecolor='red', linewidth=0.5) # Adjusted linewidth\n",
    "#             ax.set_title(f'Maxar Open Data Catalog Item Extents ({len(gdf_items)} items)')\n",
    "#             ax.set_xlabel('Longitude')\n",
    "#             ax.set_ylabel('Latitude')\n",
    "#             # Consider setting limits based on data extent if it's geographically concentrated\n",
    "#             minx, miny, maxx, maxy = gdf_items.total_bounds\n",
    "#             ax.set_xlim(minx - 2, maxx + 2) # Add padding\n",
    "#             ax.set_ylim(miny - 2, maxy + 2)\n",
    "#             plt.tight_layout() # Adjust layout\n",
    "#             plt.show()\n",
    "#             print(\"Plotting complete.\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred during catalog processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6099559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from maxar_platform.session import session\n",
    "# session.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26e7244",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # --- Pydeck Visualization ---\n",
    "# country_gdf = pd.DataFrame(columns=['hex', 'region', 'pv_count', 'total_area_m2'])\n",
    "# selected_country = country_dropdown.value\n",
    "# country_iso = country_isos[selected_country]\n",
    "# country_gdf = get_country_hex_cells(country_gdf, country_iso, h3_col)\n",
    "# print(f\"Aggregated {len(country_gdf)} H3 cells for {selected_country}.\")\n",
    "# # check df column data types\n",
    "# print(country_gdf.dtypes)\n",
    "# country_center = conn.execute(f\"\"\"SELECT ST_X(country_centroid) AS country_lon, ST_Y(country_centroid) AS country_lat\n",
    "#     FROM (SELECT ST_Centroid(ST_MakeEnvelope(div_bbox.xmin, div_bbox.ymin, div_bbox.xmax, div_bbox.ymax)) AS country_centroid \n",
    "#     FROM {TABLE_NAME}\n",
    "#     WHERE division_country = '{country_iso}' and division_subtype = 'country'\n",
    "#     LIMIT 1) AS subquery\n",
    "#     \"\"\").fetchone()\n",
    "# # Calculate the center point from our data\n",
    "# view_lat = country_center[1]\n",
    "# view_lon = country_center[0]\n",
    "# print(f\"Map centered on {selected_country}: Lat={view_lat:.4f}, Lon={view_lon:.4f}\")\n",
    "\n",
    "# # print(country_gdf)\n",
    "\n",
    "# if not country_gdf.empty:\n",
    "    \n",
    "#     # Function to update map based on country selection\n",
    "#     # def update_map(change):\n",
    "#     #     if change['type'] == 'change' and change['name'] == 'value':\n",
    "#     #         selected_country = change['new']\n",
    "\n",
    "#     #         try:\n",
    "#     #             global country_gdf\n",
    "#     #             # Get ISO code for selected country\n",
    "#     #             country_iso = country_isos[selected_country]\n",
    "#     #             country_gdf = get_country_hex_cells(country_gdf, country_iso, h3_col)\n",
    "\n",
    "#     #             if not country_gdf.empty:\n",
    "#     #                 country_center = conn.execute(f\"\"\"SELECT ST_X(country_centroid) AS country_lon, ST_Y(country_centroid) AS country_lat\n",
    "#     #                 FROM (SELECT ST_Centroid(CAST(division_bbox AS Box_2D)) AS country_centroid FROM {TABLE_NAME})\n",
    "#     #                 WHERE division_country = '{country_isos[country_dropdown.value]}' and division_subtype = 'country'\n",
    "#     #                 \"\"\").fetchone()[0]\n",
    "#     #                 # Calculate the center point from our data\n",
    "#     #                 view_lat = country_center[1]\n",
    "#     #                 view_lon = country_center[0]\n",
    "#     #                 print(f\"Map centered on {selected_country}: Lat={view_lat:.4f}, Lon={view_lon:.4f}\")\n",
    "                \n",
    "#     #             print(f\"Map centered on {selected_country}: Lat={view_lat:.4f}, Lon={view_lon:.4f}\")\n",
    "#     #             # print(f\"Aggregated {len(country_gdf)} H3 cells for {selected_country}.\")\n",
    "#     #             # conn.close()\n",
    "                \n",
    "#     #             # The map will be updated when the cell is re-run\n",
    "#     #             # For automatic updates, we would need to integrate with a framework like ipywidgets interactions\n",
    "                \n",
    "#     #         except Exception as e:\n",
    "#     #             print(f\"Error updating map for {selected_country}: {e}\")\n",
    "    \n",
    "#     # Register callback\n",
    "#     # country_dropdown.observe(update_map)\n",
    "\n",
    "#     # 4. Create Pydeck Layer\n",
    "#     # Simple color scale: more PV installations -> more red\n",
    "#     # Adjust divisor (pv_count + 10) based on typical counts for better color spread\n",
    "#     color_expression = \"[255, (1 - pv_count / (pv_count + 50)) * 255, 200]\" # RGBA (Added alpha)\n",
    "#     # data format for H3HexagonLayer is [{h3_index_col: <cell_hex>, \"attr1\": <value1>, \"attr2\": <value2>, ...}, ...]\n",
    "#     # Convert Dataframe to list of dictionaries\n",
    "\n",
    "#     h3_cells = country_gdf.to_dict(orient='records')\n",
    "#     # print(h3_cells)\n",
    "#     # display(country_gdf.sample(5))\n",
    "\n",
    "\n",
    "#     h3_layer = pdk.Layer(\n",
    "#         \"H3HexagonLayer\",\n",
    "#         # data=country_gdf,\n",
    "#         data=h3_cells,\n",
    "#         pickable=True,\n",
    "#         stroked=True, # Show hexagon outlines\n",
    "#         filled=True,\n",
    "#         extruded=True, # Start flat, can be toggled\n",
    "#         get_hexagon=\"hex\", # H3 index column\n",
    "#         get_fill_color=color_expression,\n",
    "#         get_line_color=[255, 255, 255, 100], # White outline with some transparency\n",
    "#         line_width_min_pixels=2,\n",
    "#         # Elevation can be based on count or area if extruded=True\n",
    "#         get_elevation=\"total_area_m2\",\n",
    "#         elevation_scale=1.8, # Needs tuning if extruded\n",
    "#         get_weight=\"pv_count\", # Used by the layer for internal calculations\n",
    "#         auto_highlight=True,\n",
    "#     )\n",
    "#     # print(f\"Created H3HexagonLayer: {h3_layer}\")\n",
    "\n",
    "#     # 5. Define Initial View State\n",
    "#     view_state = pdk.ViewState(\n",
    "#         latitude=view_lat,\n",
    "#         longitude=view_lon,\n",
    "#         zoom=5, # Adjust zoom level for selected countries\n",
    "#         pitch=40.5,\n",
    "#         bearing=-27.36\n",
    "#     )\n",
    "\n",
    "#     # pv_points = gpd.GeoDataFrame(columns=['lng', 'lat'])\n",
    "#     # pv_points['lng'] = ds_gdf_h3['geometry'].apply(lambda x: x.centroid.x)\n",
    "#     # pv_points['lat'] = ds_gdf_h3['geometry'].apply(lambda x: x.centroid.y)\n",
    "#     # print(pv_points.sample(5))\n",
    "\n",
    "#     # layer = pdk.Layer(\n",
    "#     #     \"HexagonLayer\",\n",
    "#     #     ,\n",
    "#     #     get_position=[\"lng\", \"lat\"],\n",
    "#     #     auto_highlight=True,\n",
    "#     #     elevation_scale=50,\n",
    "#     #     pickable=True,\n",
    "#     #     elevation_range=[0, 3000],\n",
    "#     #     extruded=True,\n",
    "#     #     coverage=1,\n",
    "#     # )\n",
    "\n",
    "#     # 6. Define Tooltip\n",
    "#     tooltip = {\n",
    "#         \"text\": \"Region: {region}\\nPV Count: {pv_count}\\nTotal Area (m²): {total_area_m2}\"\n",
    "#     }\n",
    "\n",
    "#     # --- Interactivity (Optional: Toggle Extrusion) ---\n",
    "#     extruded_checkbox = widgets.Checkbox(value=False, description='Extrude by Total Area')\n",
    "\n",
    "#     # Function to update the layer's extruded property\n",
    "#     def update_extrusion(change):\n",
    "#         h3_layer.extruded = change['new']\n",
    "#         # Note: This requires re-rendering the deck or using a mechanism\n",
    "#         # that updates the layer state if pydeck supports it directly via traitlets.\n",
    "#         # For simplicity here, we'll just show how to set it initially or via a button.\n",
    "#         # A full interactive update might need deck.update() or similar if available,\n",
    "#         # or embedding in a framework like Voila/Streamlit.\n",
    "#         print(f\"Extrusion set to: {h3_layer.extruded}\") # Shows change, but won't auto-update displayed map\n",
    "\n",
    "#     # extruded_checkbox.observe(update_extrusion, names='value')\n",
    "#     # Display the checkbox (won't dynamically update the map below in standard Jupyter)\n",
    "#     # display(extruded_checkbox)\n",
    "\n",
    "\n",
    "#     # 7. Create and Display Pydeck Deck\n",
    "#     print(\"Rendering Pydeck map...\")\n",
    "#     deck = pdk.Deck(\n",
    "#         layers=[h3_layer],\n",
    "#         initial_view_state=view_state,\n",
    "#         map_style='dark', # Options: 'light', 'dark', 'satellite', 'road'\n",
    "#         tooltip=tooltip,\n",
    "#     )\n",
    "#     # deck = pdk.Deck([h3_layer])\n",
    "#     display(deck) # Display the map in Jupyter\n",
    "\n",
    "# else:\n",
    "#     print(\"No aggregated H3 data available to visualize for the selected countries.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eo-pv-cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "undefined": {
      "model_module": "@deck.gl/jupyter-widget",
      "model_module_version": "2.0.0",
      "model_name": "JupyterTransportModel",
      "state": {
       "_view_name": "ErrorWidgetView",
       "error": {},
       "msg": "Failed to load model class 'JupyterTransportModel' from module '@deck.gl/jupyter-widget'"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
