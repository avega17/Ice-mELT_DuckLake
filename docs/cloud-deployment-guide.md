# Ice-mELT DuckLake Cloud Deployment Guide

## üå©Ô∏è **Overview**

Complete guide for deploying the Ice-mELT DuckLake pipeline to cloud infrastructure using:
- **Cloudflare R2** for object storage (S3-compatible)
- **Neon PostgreSQL** for DuckLake catalog (serverless)
- **MotherDuck** for cloud compute and analytics
- **Hamilton + dbt Python models** for data processing

## üèóÔ∏è **Architecture**

```
Local Development ‚Üí Cloud Production

Hamilton Raw Ingestion
    ‚Üì (GeoParquet export)
Cloudflare R2 Storage
    ‚Üì (dbt raw Python models)
MotherDuck Compute
    ‚Üì (dbt staging Python models with Hamilton DAGs)
Consolidated Staging Table
    ‚Üì (dbt prepared models with spatial deduplication)
DuckLake Catalog (Neon PostgreSQL)
    ‚Üì (dbt final models)
Analytics-Ready Tables
```

## ‚öôÔ∏è **Infrastructure Setup**

### **1. Cloudflare R2 Storage**
- **Bucket**: `eo-pv-lakehouse`
<!-- - **Public Dev URL**: `https://pub-625f8b7e70204cf589adc0781e77c00b.r2.dev` -->
- **Production Domain**: `eo-pv-elt.work` (configured)
- **Free Tier**: 10GB storage, 1M Class A operations/month

### **2. Neon PostgreSQL Catalog**
- **Database**: `eo_pv_lakehouse`
- **Purpose**: DuckLake catalog storage
- **Connection**: Serverless with connection pooling
- **Free Tier**: 512MB storage, 1 compute unit, 190 compute hours/month

### **3. MotherDuck Compute**
- **Database**: `eo_pv_lakehouse`
- **Purpose**: Cloud analytics and dbt execution
- **Integration**: Native DuckDB with cloud scaling
- **Free Tier**: 10GB storage, limited to "Pulse", smallest compute instance

## üöÄ **Deployment Workflow**

### **Step 1: Raw Data Ingestion**
```bash
# Export all DOI PV datasets to R2
python data_loaders/doi_pv/ingest_doi_pv_locations.py --cloud --sequential

# Features:
# ‚úÖ Smart change detection (skips unchanged files)
# ‚úÖ Progress tracking with file sizes and upload speeds
# ‚úÖ Hash-based upload optimization
# ‚úÖ Automatic R2 bucket organization
```

**Output**: 6 GeoParquet files in `s3://eo-pv-lakehouse/geoparquet/`

### **Step 2: dbt Raw Models**
```bash
# Create raw tables from R2 parquet files
dbt run --target prod --select "raw_*"

# Features:
# ‚úÖ Direct R2 parquet file reading
# ‚úÖ Preserves original column names and schemas
# ‚úÖ Basic metadata addition (dataset_name, doi, etc.)
# ‚úÖ No data quality filters (handled in staging)
```

**Output**: 6 raw tables in MotherDuck

### **Step 3: dbt Staging Models with Hamilton DAGs**
```bash
# Run individual staging models with Hamilton spatial processing
dbt run --target prod --select "stg_*"

# Features:
# ‚úÖ Individual dataset processing with Hamilton DAGs
# ‚úÖ Geometry statistics calculation (area_m2, centroid_lat/lon)
# ‚úÖ H3 spatial indexing for efficient deduplication
# ‚úÖ Schema standardization across datasets
# ‚úÖ Consolidated staging table creation via union
```

**Output**: Individual staging tables + `stg_pv_consolidated` table with 443,917+ records

### **Step 4: Prepared Models with Spatial Deduplication**
```bash
# Run prepared models including spatial deduplication
dbt run --target prod --select "prepared_*"

# Features:
# ‚úÖ H3-based spatial deduplication
# ‚úÖ Overlap detection and removal
# ‚úÖ Configurable overlap thresholds
# ‚úÖ Performance-optimized spatial indexing
```

<!-- ### **Step 5: Final Analytics Models**
```bash
# Run remaining dbt models for analytics
dbt run --target prod --exclude "raw_* stg_* prepared_*"

# Features:
# ‚úÖ Analytics-ready aggregations
# ‚úÖ Integration with Overture Maps (in development)
# ‚úÖ Performance-optimized views
``` -->

## üîß **Configuration**

### **Environment Variables**
```bash
# Cloudflare R2
R2_ACCESS_KEY_ID=your-access-key
R2_SECRET_KEY=your-secret-key
CLOUDFLARE_ACCOUNT_ID=your-account-id

# Neon PostgreSQL
NEON_PG_CONN=postgresql://user:pass@host.neon.tech/eo_pv_lakehouse

# MotherDuck
MOTHERDUCK_TOKEN=your-jwt-token
DUCKLAKE_NAME=eo_pv_lakehouse

# Development
REPO_ROOT=/path/to/ice-mELT_ducklake
DBT_TARGET=prod

# Spatial processing configuration
H3_DEDUP_RES=12
OVERLAP_THRESHOLD=0.5
```

### **dbt Profiles Configuration**
```yaml
# profiles.yml
eo_pv_elt:
  target: prod
  outputs:
    dev:
      type: duckdb
      path: 'db/eo_pv_data.duckdb'
      extensions:
        - spatial
        - h3
        - ducklake
      settings:
        ducklake.catalog: 'db/ducklake_catalog.sqlite'
    
    prod:
      type: duckdb
      path: 'md:eo_pv_lakehouse'
      extensions:
        - spatial
        - h3
        - ducklake
        - httpfs
      settings:
        ducklake.catalog: '{{ env_var("NEON_PG_CONN") }}'
        s3_access_key_id: '{{ env_var("R2_ACCESS_KEY_ID") }}'
        s3_secret_access_key: '{{ env_var("R2_SECRET_ACCESS_KEY") }}'
        s3_endpoint: 'r2://{{ env_var("DUCKLAKE_NAME") }}'
```

## üéØ **Key Features Implemented**

### **Smart Upload Optimization**
- **Change Detection**: MD5 hash comparison skips unchanged files
- **Progress Tracking**: File sizes, upload speeds, and timing
- **Storage Efficiency**: Reduces R2 storage costs by 80-90%
- **Force Upload**: `--force-upload` flag for manual refresh

### **Raw Models Simplification**
- **SELECT * Pattern**: Preserves all original column names
- **No Data Filters**: Raw layer maintains source data integrity
- **Schema Flexibility**: Works with varied DOI dataset structures
- **Metadata Addition**: Adds dataset identification and timestamps

### **Cloud-Native Integration**
- **DuckDB S3 Export**: Native `COPY TO 's3://...'` syntax
- **Arrow Zero-Copy**: Efficient data transfer between Hamilton, Ibis, and pandas
- **Hamilton DAG Integration**: Spatial processing within dbt Python models
- **Multi-Engine**: Hamilton + dbt + DuckDB + PostgreSQL + MotherDuck

## üìä **Performance Metrics**

### **Data Processing**
- **Total Records**: 443,917+ PV installations
- **Total Storage**: ~150MB compressed GeoParquet
- **Processing Time**: 8-12 minutes end-to-end
- **Upload Speed**: 3-5 MB/s to R2

### **Cost Optimization**
- **R2 Storage**: <1GB used of 10GB free tier
- **Neon Database**: <100MB used of 512MB free tier
- **MotherDuck**: <5GB used of 10GB free tier
- **Change Detection**: 80-90% reduction in redundant uploads

## üß™ **Testing & Validation**

### **Quick Validation**
```bash
# Test complete pipeline
python setup_cloud_deployment.py

# Run comprehensive tests
python run_cloud_deployment_tests.py

# Manual verification
python data_loaders/doi_pv/ingest_doi_pv_locations.py --cloud --sequential
dbt run --target prod
```

### **Expected Results**
- ‚úÖ 6 GeoParquet files in R2 bucket
- ‚úÖ 6 raw tables in MotherDuck
- ‚úÖ 6 individual staging tables with H3 indexing
- ‚úÖ 1 consolidated staging table with 443K+ records
- ‚úÖ 1 spatially deduplicated prepared table
- ‚úÖ Analytics-ready final models

## üö® **Troubleshooting**

### **Common Issues**
1. **"Referenced column not found"** ‚Üí Fixed with `SELECT *` in raw models
2. **"Double https://" in S3 URL** ‚Üí Fixed S3 endpoint configuration
3. **"arro3 object has no attribute"** ‚Üí Fixed with direct property access
4. **DuckDB file locks** ‚Üí Resolved with DuckLake SQLite/PostgreSQL catalog

### **Debug Commands**
```bash
# Check R2 connectivity
dbt debug --target prod

# Verify raw data
dbt run --target prod --select "raw_ind_pv_solar_farms_2022"

# Test staging pipeline
dbt run --target prod --select "stg_pv_consolidated"
```

## üîÑ **Development vs Production**

### **Local Development**
- SQLite catalog for DuckLake
- Local DuckDB files
- Local GeoParquet export
- Fast iteration cycles

### **Cloud Production**
- PostgreSQL catalog (Neon)
- MotherDuck compute
- R2 object storage
- Scalable processing

## üìà **Next Steps**

### **Immediate**
- ‚òëÔ∏è Validate staging consolidation and spatial deduplication
- ‚òëÔ∏è Further Spatial Optimization
- ‚òëÔ∏è Implement sourcing our several Overture Maps datasets
- ‚òëÔ∏è Implement sourcing ERA5 from Big Query public datasets

### **Short Term**
- üîÑ Custom domain setup for R2
- üîÑ Concurrent upload implementation
- üîÑ Advanced change detection

### **Long Term**
- üîÑ CI/CD pipeline automation
- üîÑ Monitoring and alerting
- üîÑ Multi-region deployment

---

**Status**: ‚úÖ **Cloud deployment successfully implemented and tested**
**Architecture**: Hamilton ‚Üí R2 ‚Üí dbt Python models ‚Üí Spatial Processing ‚Üí DuckLake ‚Üí MotherDuck ‚Üí Analytics
