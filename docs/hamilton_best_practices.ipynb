{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hamilton Best Practices\n",
    "\n",
    "This notebook demonstrates the Hamilton best practices we've adopted for our data pipeline development. These practices ensure maintainable, scalable, and debuggable dataflows.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's load the Hamilton Jupyter magic and import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Hamilton Jupyter magic\n",
    "%load_ext hamilton.plugins.jupyter_magic\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pyarrow as pa\n",
    "from typing import Dict, List\n",
    "from hamilton.function_modifiers import config, cache, tag\n",
    "from hamilton.htypes import Parallelizable, Collect\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Configuration\n",
    "\n",
    "Use this dropdown to select execution mode for the configurable examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive execution mode selector\n",
    "execution_mode_dropdown = widgets.Dropdown(\n",
    "    options=['sequential', 'parallel'],\n",
    "    value='sequential',\n",
    "    description='Execution Mode:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "display(execution_mode_dropdown)\n",
    "\n",
    "# Function to get current selection\n",
    "def get_execution_mode():\n",
    "    return execution_mode_dropdown.value\n",
    "\n",
    "print(f\"Current execution mode: {get_execution_mode()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Principles\n",
    "\n",
    "### 1. Function Naming Conventions\n",
    "\n",
    "**✅ Use nouns, not verbs for function names**\n",
    "\n",
    "Hamilton functions represent data artifacts/nouns in your pipeline, not actions/verbs.\n",
    "\n",
    "**Reference**: [Function Naming Best Practices](https://hamilton.dagworks.io/en/latest/concepts/best-practices/function-naming/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cell_to_module good_naming --display\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pyarrow as pa\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# ✅ Good - noun-based naming\n",
    "def raw_data() -> pd.DataFrame:\n",
    "    \"\"\"Raw input data.\"\"\"\n",
    "    return pd.DataFrame({'x': [1, 2, 3], 'y': [4, 5, 6]})\n",
    "\n",
    "def processed_geodataframe(raw_data: pd.DataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Processed geodataframe with geometry.\"\"\"\n",
    "    gdf = gpd.GeoDataFrame(raw_data)\n",
    "    gdf['geometry'] = [Point(x, y) for x, y in zip(gdf.x, gdf.y)]\n",
    "    return gdf\n",
    "\n",
    "def geoarrow_table(processed_geodataframe: gpd.GeoDataFrame) -> pa.Table:\n",
    "    \"\"\"Arrow table with geospatial data.\"\"\"\n",
    "    return pa.Table.from_pandas(processed_geodataframe, preserve_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**❌ Bad - verb-based naming (don't do this)**\n",
    "\n",
    "Compare the DAG visualization below - verb-based naming makes it harder to understand what data artifacts the pipeline produces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cell_to_module bad_naming --display\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pyarrow as pa\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# ❌ Bad - verb-based naming  \n",
    "def load_raw_data() -> pd.DataFrame:\n",
    "    \"\"\"Function name is a verb - confusing!\"\"\"\n",
    "    return pd.DataFrame({'x': [1, 2, 3], 'y': [4, 5, 6]})\n",
    "\n",
    "def process_geodataframe(load_raw_data: pd.DataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Another verb - what does this represent?\"\"\"\n",
    "    gdf = gpd.GeoDataFrame(load_raw_data)\n",
    "    gdf['geometry'] = [Point(x, y) for x, y in zip(gdf.x, gdf.y)]\n",
    "    return gdf\n",
    "\n",
    "def convert_to_geoarrow(process_geodataframe: gpd.GeoDataFrame) -> pa.Table:\n",
    "    \"\"\"Verb-based naming makes DAG harder to read.\"\"\"\n",
    "    return pa.Table.from_pandas(process_geodataframe, preserve_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Helper Functions\n",
    "\n",
    "**✅ Prefix helper functions with underscore**\n",
    "\n",
    "Helper functions with `_` prefix are excluded from DAG visualization and execution tracking, keeping the pipeline graph clean.\n",
    "\n",
    "**Reference**: [Code Organization Best Practices](https://hamilton.dagworks.io/en/latest/concepts/best-practices/code-organization/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cell_to_module helper_example --display\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pyarrow as pa\n",
    "from shapely.geometry import Point\n",
    "from typing import Dict\n",
    "\n",
    "# ✅ Good - helper functions excluded from DAG\n",
    "def _geoarrow_conversion_helper(gdf: gpd.GeoDataFrame, dataset_name: str) -> pa.Table:\n",
    "    \"\"\"Helper function for GeoArrow conversion - won't appear in DAG.\"\"\"\n",
    "    # Complex conversion logic here\n",
    "    table = pa.Table.from_pandas(gdf, preserve_index=False)\n",
    "    metadata = {\"dataset_name\": dataset_name}\n",
    "    return table.replace_schema_metadata(metadata)\n",
    "\n",
    "def _duckdb_storage_helper(table: pa.Table, conn) -> int:\n",
    "    \"\"\"Helper function for DuckDB storage - won't appear in DAG.\"\"\"\n",
    "    # Complex storage logic here\n",
    "    return len(table)\n",
    "\n",
    "# Hamilton DAG nodes that use helpers\n",
    "def sample_data() -> gpd.GeoDataFrame:\n",
    "    \"\"\"Sample geodataframe.\"\"\"\n",
    "    df = pd.DataFrame({'x': [1, 2], 'y': [3, 4], 'name': ['A', 'B']})\n",
    "    gdf = gpd.GeoDataFrame(df)\n",
    "    gdf['geometry'] = [Point(x, y) for x, y in zip(gdf.x, gdf.y)]\n",
    "    return gdf\n",
    "\n",
    "def processed_arrow_table(sample_data: gpd.GeoDataFrame) -> pa.Table:\n",
    "    \"\"\"Processed arrow table using helper function.\"\"\"\n",
    "    return _geoarrow_conversion_helper(sample_data, \"sample_dataset\")\n",
    "\n",
    "def storage_result(processed_arrow_table: pa.Table) -> Dict:\n",
    "    \"\"\"Storage result using helper function.\"\"\"\n",
    "    # In real implementation, would pass actual connection\n",
    "    row_count = len(processed_arrow_table)\n",
    "    return {\"rows_stored\": row_count, \"status\": \"success\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dependency Injection Over Direct Calls\n",
    "\n",
    "**✅ Use Hamilton's dependency injection**\n",
    "\n",
    "Dependency injection enables Hamilton's caching, parallelization, and visualization features.\n",
    "\n",
    "**Reference**: [Hamilton Node Concepts](https://hamilton.dagworks.io/en/latest/concepts/node/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cell_to_module dependency_injection_good --display\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pyarrow as pa\n",
    "from shapely.geometry import Point\n",
    "from typing import Dict\n",
    "\n",
    "# ✅ Good - dependency injection\n",
    "def config_params() -> Dict:\n",
    "    \"\"\"Configuration parameters.\"\"\"\n",
    "    return {\"buffer_size\": 100, \"crs\": \"EPSG:4326\"}\n",
    "\n",
    "def raw_data() -> pd.DataFrame:\n",
    "    \"\"\"Raw input data.\"\"\"\n",
    "    return pd.DataFrame({'x': [1, 2, 3], 'y': [4, 5, 6], 'value': [10, 20, 30]})\n",
    "\n",
    "def processed_geodataframe(raw_data: pd.DataFrame, config_params: Dict) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Processed geodataframe using injected dependencies.\"\"\"\n",
    "    gdf = gpd.GeoDataFrame(raw_data)\n",
    "    gdf['geometry'] = [Point(x, y) for x, y in zip(gdf.x, gdf.y)]\n",
    "    gdf = gdf.set_crs(config_params[\"crs\"])\n",
    "    return gdf\n",
    "\n",
    "def arrow_table(processed_geodataframe: gpd.GeoDataFrame) -> pa.Table:\n",
    "    \"\"\"Arrow table from processed geodataframe.\"\"\"\n",
    "    return pa.Table.from_pandas(processed_geodataframe, preserve_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**❌ Bad - direct function calls (don't do this)**\n",
    "\n",
    "This breaks Hamilton's dependency tracking and prevents caching/parallelization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cell_to_module dependency_injection_bad --display\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pyarrow as pa\n",
    "from shapely.geometry import Point\n",
    "from typing import Dict\n",
    "\n",
    "# ❌ Bad - direct function calls\n",
    "def config_params() -> Dict:\n",
    "    \"\"\"Configuration parameters.\"\"\"\n",
    "    return {\"buffer_size\": 100, \"crs\": \"EPSG:4326\"}\n",
    "\n",
    "def raw_data() -> pd.DataFrame:\n",
    "    \"\"\"Raw input data.\"\"\"\n",
    "    return pd.DataFrame({'x': [1, 2, 3], 'y': [4, 5, 6], 'value': [10, 20, 30]})\n",
    "\n",
    "def arrow_table_bad() -> pa.Table:\n",
    "    \"\"\"Arrow table with direct calls - breaks Hamilton's dependency tracking!\"\"\"\n",
    "    # Direct function calls - Hamilton can't track these dependencies\n",
    "    data = raw_data()  # Direct call\n",
    "    config = config_params()  # Direct call\n",
    "    \n",
    "    gdf = gpd.GeoDataFrame(data)\n",
    "    gdf['geometry'] = [Point(x, y) for x, y in zip(gdf.x, gdf.y)]\n",
    "    gdf = gdf.set_crs(config[\"crs\"])\n",
    "    \n",
    "    return pa.Table.from_pandas(gdf, preserve_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Configuration and Execution Modes\n",
    "\n",
    "**✅ Use config.when for conditional execution**\n",
    "\n",
    "The key fix: Use `Collect[str]` for parallel mode to properly handle the collected results from `Parallelizable[str]`.\n",
    "\n",
    "**Reference**: [Parallel Task Concepts](https://hamilton.dagworks.io/en/latest/concepts/parallel-task/)\n",
    "\n",
    "Use the dropdown above to switch between execution modes and see how the DAG changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cell_to_module execution_modes --display --config execution_mode=sequential\n",
    "\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "from hamilton.function_modifiers import config\n",
    "from hamilton.htypes import Parallelizable, Collect\n",
    "\n",
    "def metadata() -> Dict:\n",
    "    \"\"\"Sample metadata for datasets.\"\"\"\n",
    "    return {\n",
    "        \"dataset_a\": {\"url\": \"http://example.com/a\", \"size\": 100},\n",
    "        \"dataset_b\": {\"url\": \"http://example.com/b\", \"size\": 200},\n",
    "        \"dataset_c\": {\"url\": \"http://example.com/c\", \"size\": 150}\n",
    "    }\n",
    "\n",
    "@config.when(execution_mode=\"parallel\")\n",
    "def dataset_names__parallel(metadata: Dict) -> Parallelizable[str]:\n",
    "    \"\"\"Dataset names for parallel processing.\"\"\"\n",
    "    for name in metadata.keys():\n",
    "        yield name\n",
    "\n",
    "@config.when(execution_mode=\"sequential\") \n",
    "def dataset_names__sequential(metadata: Dict) -> List[str]:\n",
    "    \"\"\"Dataset names for sequential processing.\"\"\"\n",
    "    return list(metadata.keys())\n",
    "\n",
    "# Fix: Use Collect for parallel mode, direct List for sequential\n",
    "@config.when(execution_mode=\"parallel\")\n",
    "def processing_summary__parallel(dataset_names: Collect[str], metadata: Dict) -> Dict:\n",
    "    \"\"\"Summary of datasets to process (parallel mode).\"\"\"\n",
    "    dataset_list = list(dataset_names)  # Convert Collect to list\n",
    "    total_size = sum(metadata[name][\"size\"] for name in dataset_list)\n",
    "    return {\n",
    "        \"total_datasets\": len(dataset_list),\n",
    "        \"total_size\": total_size,\n",
    "        \"datasets\": dataset_list\n",
    "    }\n",
    "\n",
    "@config.when(execution_mode=\"sequential\")\n",
    "def processing_summary__sequential(dataset_names: List[str], metadata: Dict) -> Dict:\n",
    "    \"\"\"Summary of datasets to process (sequential mode).\"\"\"\n",
    "    total_size = sum(metadata[name][\"size\"] for name in dataset_names)\n",
    "    return {\n",
    "        \"total_datasets\": len(dataset_names),\n",
    "        \"total_size\": total_size,\n",
    "        \"datasets\": dataset_names\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Modular Organization\n",
    "\n",
    "**✅ Keep dataflow files under 500 lines**\n",
    "\n",
    "Break large dataflows into focused modules for maintainability.\n",
    "\n",
    "**Reference**: [Code Organization](https://hamilton.dagworks.io/en/latest/concepts/best-practices/code-organization/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cell_to_module modular_organization --display\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from typing import Dict\n",
    "\n",
    "# ✅ Good - focused module for data loading\n",
    "def raw_dataset_metadata() -> Dict:\n",
    "    \"\"\"Metadata for raw datasets - focused on data loading.\"\"\"\n",
    "    return {\n",
    "        \"pv_locations\": {\"source\": \"doi:10.1234/pv-data\", \"format\": \"geojson\"},\n",
    "        \"admin_boundaries\": {\"source\": \"overture\", \"format\": \"parquet\"},\n",
    "        \"irradiance_data\": {\"source\": \"nrel\", \"format\": \"zarr\"}\n",
    "    }\n",
    "\n",
    "def dataset_priorities(raw_dataset_metadata: Dict) -> Dict:\n",
    "    \"\"\"Processing priorities for datasets.\"\"\"\n",
    "    return {\n",
    "        \"pv_locations\": 1,  # High priority\n",
    "        \"admin_boundaries\": 2,  # Medium priority  \n",
    "        \"irradiance_data\": 3   # Low priority\n",
    "    }\n",
    "\n",
    "def processing_plan(raw_dataset_metadata: Dict, dataset_priorities: Dict) -> Dict:\n",
    "    \"\"\"Combined processing plan.\"\"\"\n",
    "    return {\n",
    "        \"datasets\": list(raw_dataset_metadata.keys()),\n",
    "        \"total_datasets\": len(raw_dataset_metadata),\n",
    "        \"high_priority\": [k for k, v in dataset_priorities.items() if v == 1]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example module structure for our project:**\n",
    "\n",
    "```\n",
    "dataflows/\n",
    "├── data_loading.py      # DOI downloads, STAC queries\n",
    "├── geospatial.py        # GeoDataFrame processing, CRS handling\n",
    "├── geoarrow_conversion.py  # Arrow table creation, DuckDB export\n",
    "├── validation.py        # Data quality checks\n",
    "└── visualization.py     # DAG visualization helpers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Strategic Caching\n",
    "\n",
    "**✅ Cache expensive operations, avoid problematic data types**\n",
    "\n",
    "**Reference**: [Hamilton Caching](https://hamilton.dagworks.io/en/latest/concepts/caching/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cell_to_module caching_example --display\n",
    "\n",
    "# Cache expensive downloads\n",
    "@cache(behavior=\"default\", format=\"json\")\n",
    "def dataset_download_path() -> str:\n",
    "    \"\"\"Expensive download operation - cached.\"\"\"\n",
    "    # Simulate expensive download\n",
    "    import time\n",
    "    time.sleep(0.1)  # Simulate network delay\n",
    "    return \"/tmp/downloaded_dataset.json\"\n",
    "\n",
    "# Disable caching for problematic data types\n",
    "@cache(behavior=\"disable\")  # GeoArrow extension types have serialization issues\n",
    "def geoarrow_table() -> pa.Table:\n",
    "    \"\"\"GeoArrow table - caching disabled due to serialization issues.\"\"\"\n",
    "    # Simulate GeoArrow table creation\n",
    "    return pa.table({\"geometry\": [b\"\\x01\\x01\\x00\\x00\\x00\"], \"id\": [1]})\n",
    "\n",
    "def final_result(dataset_download_path: str, geoarrow_table: pa.Table) -> Dict:\n",
    "    \"\"\"Final processing result.\"\"\"\n",
    "    return {\n",
    "        \"source_path\": dataset_download_path,\n",
    "        \"table_rows\": len(geoarrow_table),\n",
    "        \"status\": \"processed\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Fixes Applied:\n",
    "\n",
    "1. **Correct Magic Syntax**: Used `%%cell_to_module` instead of `%%hamilton_module`\n",
    "2. **Fixed Type Compatibility**: Used `Collect[str]` for parallel mode to properly handle collected results from `Parallelizable[str]`\n",
    "3. **Proper Function Naming**: All functions use noun-based naming with helper functions prefixed with `_`\n",
    "4. **Strategic Caching**: Demonstrated proper caching patterns\n",
    "\n",
    "### Key Benefits:\n",
    "\n",
    "1. **Clean DAG Visualization** - Helper functions don't clutter the pipeline graph\n",
    "2. **Maintainable Code** - Modular organization with clear separation of concerns\n",
    "3. **Flexible Execution** - Easy switching between parallel and sequential modes\n",
    "4. **Efficient Caching** - Strategic caching without serialization issues\n",
    "\n",
    "### References\n",
    "\n",
    "**Core Documentation:**\n",
    "- [Hamilton Documentation](https://hamilton.dagworks.io/)\n",
    "- [Hamilton Node Concepts](https://hamilton.dagworks.io/en/latest/concepts/node/)\n",
    "- [Function Modifiers](https://hamilton.dagworks.io/en/latest/concepts/function-modifiers/)\n",
    "\n",
    "**Best Practices:**\n",
    "- [Function Naming](https://hamilton.dagworks.io/en/latest/concepts/best-practices/function-naming/)\n",
    "- [Code Organization](https://hamilton.dagworks.io/en/latest/concepts/best-practices/code-organization/)\n",
    "\n",
    "**Advanced Features:**\n",
    "- [Visualization](https://hamilton.dagworks.io/en/latest/concepts/visualization/)\n",
    "- [Parallel Tasks](https://hamilton.dagworks.io/en/latest/concepts/parallel-task/)\n",
    "- [Caching](https://hamilton.dagworks.io/en/latest/concepts/caching/)\n",
    "- [Lineage](https://hamilton.dagworks.io/en/latest/how-tos/use-hamilton-for-lineage/)\n",
    "\n",
    "**Tools:**\n",
    "- [CLI Reference](https://hamilton.dagworks.io/en/latest/how-tos/cli-reference/)\n",
    "- [Hamilton Jupyter Magic](https://hamilton.dagworks.io/en/latest/how-tos/use-in-jupyter-notebook/)\n",
    "- [Hamilton Jupyter Examples](https://github.com/apache/hamilton/blob/main/examples/jupyter_notebook_magic/example.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook demonstrates live examples of Hamilton best practices with interactive DAG visualizations and proper imports for all cell modules.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
