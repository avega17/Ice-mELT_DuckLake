{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d82f4ad",
   "metadata": {},
   "source": [
    "# CCOM 6994: Solar Panel Dataset Analysis - Comprehensive Demo\n",
    "\n",
    "**Data Analysis Tools - Final Project**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Project Overview\n",
    "\n",
    "This notebook demonstrates **advanced geospatial data analysis techniques** applied to a global solar panel (PV) installation dataset. We'll showcase modern data engineering and analytics tools that enable scalable, cloud-native geospatial workflows.\n",
    "\n",
    "### üõ†Ô∏è Technology Stack\n",
    "\n",
    "- **DuckDB** with spatial extensions for efficient GeoParquet operations\n",
    "- **Ibis** for lazy evaluation and SQL-like operations\n",
    "- **H3** spatial indexing for hierarchical hexagonal grids\n",
    "- **Overture Maps** for administrative boundaries\n",
    "- **Folium** and **Lonboard** for interactive visualizations\n",
    "- **censusdis** for US Census data integration\n",
    "\n",
    "### üìä Dataset: Global Solar Panel (PV) Installations\n",
    "\n",
    "Our consolidated PV dataset includes installations from multiple sources:\n",
    "- **Global Sentinel-2 detections** (2021)\n",
    "- **USA California USGS data** (2016)\n",
    "- **UK crowdsourced data** (2020)\n",
    "- **China medium resolution data** (2024)\n",
    "- **India solar farms** (2022)\n",
    "- **Global harmonized large solar farms** (2020)\n",
    "\n",
    "### üìö Key Learning Objectives\n",
    "\n",
    "1. **Cloud-native geospatial data formats** (GeoParquet)\n",
    "2. **Spatial indexing strategies** (H3 hexagonal grids)\n",
    "3. **Efficient remote data access** (HTTP range requests)\n",
    "4. **Spatial joins** with administrative boundaries\n",
    "5. **Interactive geospatial visualizations**\n",
    "6. **Socioeconomic analysis** with Census data integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2e6572",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ References and Documentation\n",
    "\n",
    "### Core Technologies\n",
    "- [DuckDB Spatial Extension](https://duckdb.org/docs/extensions/spatial.html) - Native geospatial operations\n",
    "- [Ibis with DuckDB](https://ibis-project.org/backends/DuckDB/) - Lazy evaluation and query optimization\n",
    "- [GeoParquet Specification](https://geoparquet.org/) - Cloud-optimized geospatial format\n",
    "- [DuckLake Documentation](https://ducklake.select/docs/stable/) - Multi-catalog data lakehouse\n",
    "\n",
    "### Spatial Indexing & Visualization\n",
    "- [H3 Spatial Indexing](https://h3geo.org/) - Uber's hexagonal hierarchical indexing\n",
    "- [Overture Maps](https://docs.overturemaps.org/) - Open-source map data\n",
    "- [Folium Documentation](https://python-visualization.github.io/folium/) - Interactive web maps\n",
    "\n",
    "### US Census Integration\n",
    "- [censusdis Documentation](https://censusdis.readthedocs.io/) - Python Census API wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e90269",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Setup: Import Libraries and Configure Environment\n",
    "\n",
    "We begin by importing all necessary libraries and configuring our working environment. This includes:\n",
    "- Core data processing libraries (pandas, numpy, ibis)\n",
    "- Geospatial libraries (geopandas, shapely)\n",
    "- Database and query engines (DuckDB with extensions)\n",
    "- Visualization tools (matplotlib, seaborn, folium)\n",
    "- Spatial indexing (H3)\n",
    "- Census data access (censusdis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a373ebf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "\n",
    "# Core data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ibis\n",
    "from ibis import _\n",
    "import duckdb\n",
    "\n",
    "# Geospatial libraries\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point, Polygon, box\n",
    "\n",
    "# H3 spatial indexing\n",
    "import h3.api.memview_int as h3\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "from folium import plugins\n",
    "import seaborn as sns\n",
    "\n",
    "# Census data\n",
    "import censusdis\n",
    "from censusdis import data as ced\n",
    "# from censusdis.geography import CensusGeography\n",
    "CENSUSDIS_AVAILABLE = True\n",
    "\n",
    "# Configure pandas and matplotlib\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Centralized path for the consolidated PV GeoParquet\n",
    "PV_GEOPARQUET_PATH = os.getenv(\n",
    "    \"PV_GEO_PARQUET_PATH\",\n",
    "    \"s3://eo-pv-lakehouse/geoparquet/ccom6994_pv_dataset.parquet\",\n",
    ")\n",
    "\n",
    "# Ibis configuration\n",
    "ibis.options.interactive = True\n",
    "\n",
    "print(\"‚úÖ All libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e915539",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üóÑÔ∏è Database Connection Setup\n",
    "\n",
    "### Why DuckDB?\n",
    "\n",
    "DuckDB is an **embedded analytical database** designed for OLAP (Online Analytical Processing) workloads. Key advantages:\n",
    "\n",
    "- ‚ö° **Fast**: Columnar storage with vectorized execution\n",
    "- ü™∂ **Lightweight**: Runs in-process, no server required\n",
    "- üîå **Extensible**: Rich ecosystem of extensions (spatial, H3, httpfs)\n",
    "- üåê **Cloud-native**: Native support for Parquet, S3, HTTP range requests\n",
    "\n",
    "### Extensions We're Loading\n",
    "\n",
    "1. **spatial**: Geometry operations, GeoParquet support, spatial functions\n",
    "2. **h3**: H3 spatial indexing functions (from community extensions)\n",
    "3. **httpfs**: Read files from HTTP/S3 without full download\n",
    "4. **cache_httpfs**: HTTP result caching for repeated queries\n",
    "5. **ducklake**: Our custom data catalog management system\n",
    "\n",
    "### Configuration Details\n",
    "\n",
    "We configure DuckDB with:\n",
    "- Memory limit (12GB for large geospatial operations)\n",
    "- Thread count (6 threads for parallel processing)\n",
    "- S3/R2 credentials (for Cloudflare R2 bucket access)\n",
    "- DuckLake catalog attachment (our multi-source data catalog)\n",
    "\n",
    "**Important**: We use production/remote credentials to connect to a Neon Postgres-backed DuckLake catalog (not local Docker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2cb4f73",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_duckdb_connection(\n",
    "    memory_limit: str = \"12GB\",\n",
    "    threads: int = 6,\n",
    "    use_production: bool = True\n",
    ") -> duckdb.DuckDBPyConnection:\n",
    "    \"\"\"\n",
    "    Create DuckDB connection with spatial extensions and S3 configuration.\n",
    "    Uses production Neon Postgres catalog (not local Docker).\n",
    "    \n",
    "    Args:\n",
    "        memory_limit: Memory limit for DuckDB\n",
    "        threads: Number of threads to use\n",
    "        use_production: Whether to use production catalog (default: True)\n",
    "        \n",
    "    Returns:\n",
    "        Configured DuckDB connection\n",
    "    \"\"\"\n",
    "    # Configuration for DuckDB\n",
    "    config = {\n",
    "        'threads': threads,\n",
    "        'memory_limit': memory_limit,\n",
    "    }\n",
    "    \n",
    "    # Add S3/R2 configuration if credentials exist\n",
    "    if (ak := os.getenv(\"R2_ACCESS_KEY_ID\")) and (sk := os.getenv(\"R2_SECRET_KEY\")):\n",
    "        config.update({\n",
    "            's3_access_key_id': ak,\n",
    "            's3_secret_access_key': sk,\n",
    "            's3_endpoint': os.getenv('R2_S3_ENDPOINT', 'e833ac2d32c62bcff5e4b72c74e5351d.r2.cloudflarestorage.com'),\n",
    "            's3_use_ssl': 'true',\n",
    "            's3_url_style': 'path'\n",
    "        })\n",
    "        print(\"‚úÖ S3/R2 credentials configured\")\n",
    "    \n",
    "    # Create in-memory connection\n",
    "    con = duckdb.connect(database=':memory:', config=config)\n",
    "    \n",
    "    # Install and load extensions\n",
    "    print(\"\\nüì¶ Loading DuckDB extensions...\")\n",
    "    extensions_sql = \"\"\"\n",
    "        INSTALL httpfs;\n",
    "        LOAD httpfs;\n",
    "        INSTALL ducklake;\n",
    "        LOAD ducklake;\n",
    "        INSTALL spatial;\n",
    "        LOAD spatial;\n",
    "        INSTALL h3 FROM community;\n",
    "        LOAD h3;\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        con.execute(extensions_sql)\n",
    "        print(\"‚úÖ All extensions loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Extension loading error: {e}\")\n",
    "\n",
    "    # any remaining extension-specific config\n",
    "    # ext_config_sql = f\"\"\"\n",
    "    #     SET cache_httpfs_profile_type='on_disk';\n",
    "    #     SET cache_httpfs_cache_directory='{os.getenv('HTTPFS_CACHE_PATH', 'db/.httpfs_cache')}';\n",
    "    # \"\"\"\n",
    "    # try:\n",
    "    #     con.execute(ext_config_sql)\n",
    "    #     print(\"‚úÖ Extension-specific configuration applied\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"‚ö†Ô∏è  Extension configuration error: {e}\")\n",
    "    \n",
    "    # Attach DuckLake catalog (use production by default)\n",
    "    try:\n",
    "        # Use production catalog connection string\n",
    "        local_default = os.getenv('DUCKLAKE_CONNECTION_STRING_DEV')\n",
    "        catalog_string = os.getenv('DUCKLAKE_CONNECTION_STRING_PROD', local_default) if use_production else local_default\n",
    "        \n",
    "        DUCKLAKE_ATTACH = os.getenv(\"DUCKLAKE_ATTACH_PROD\") if use_production else os.getenv(\"DUCKLAKE_ATTACH_DEV\")\n",
    "        DUCKLAKE_NAME = os.getenv(\"DUCKLAKE_NAME\", \"eo_pv_lakehouse\")\n",
    "        DUCKLAKE_DATA_PATH = os.getenv(\"DUCKLAKE_DATA_PATH\")\n",
    "        \n",
    "        if DUCKLAKE_ATTACH:\n",
    "            attach_sql = f\"\"\"\n",
    "            ATTACH IF NOT EXISTS '{DUCKLAKE_ATTACH}' AS {DUCKLAKE_NAME}\n",
    "                (DATA_PATH '{DUCKLAKE_DATA_PATH}');\n",
    "            USE {DUCKLAKE_NAME};\n",
    "            \"\"\"\n",
    "            con.execute(attach_sql)\n",
    "            \n",
    "            print(f\"\\n‚úÖ Attached DuckLake catalog: {DUCKLAKE_NAME}\")\n",
    "            if catalog_string:\n",
    "                catalog_type = catalog_string.split(':')[1] if ':' in catalog_string else 'unknown'\n",
    "                print(f\"   Catalog type: {catalog_type}\")\n",
    "                print(f\"   Data path: {DUCKLAKE_DATA_PATH}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No DuckLake catalog configured\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not attach DuckLake catalog: {e}\")\n",
    "    \n",
    "    return con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "311333dd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ S3/R2 credentials configured\n",
      "\n",
      "üì¶ Loading DuckDB extensions...\n",
      "‚úÖ All extensions loaded successfully\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9493a59ac64ba6b0c7151975b2ad40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Attached DuckLake catalog: eo_pv_lakehouse\n",
      "   Catalog type: postgres\n",
      "   Data path: s3://eo-pv-lakehouse/ducklake_data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b34b2a09246a49d7b7bfb2c7d5ea383c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Available tables in catalog: 16\n",
      "   - pv_consolidated\n",
      "   - pv_h3_cells\n",
      "   - pv_h3_grid\n",
      "   - raw_chn_med_res_pv_2024\n",
      "   - raw_global_harmonized_large_solar_farms_2020\n",
      "   - raw_global_pv_inventory_sent2_spot_2021\n",
      "   - raw_ind_pv_solar_farms_2022\n",
      "   - raw_uk_crowdsourced_pv_2020\n",
      "   - raw_usa_cali_usgs_pv_2016\n",
      "   - stg_chn_med_res_pv_2024\n",
      "   - stg_global_harmonized_large_solar_farms_2020\n",
      "   - stg_global_pv_inventory_sent2_spot_2021\n",
      "   - stg_ind_pv_solar_farms_2022\n",
      "   - stg_pv_consolidated\n",
      "   - stg_uk_crowdsourced_pv_2020\n",
      "   - stg_usa_cali_usgs_pv_2016\n"
     ]
    }
   ],
   "source": [
    "# Create connection with production catalog\n",
    "con = create_duckdb_connection(use_production=True)\n",
    "\n",
    "# Show available tables\n",
    "try:\n",
    "    tables = con.execute(\"SHOW TABLES;\").fetchall()\n",
    "    print(f\"\\nüìä Available tables in catalog: {len(tables)}\")\n",
    "    for table in tables:\n",
    "        print(f\"   - {table[0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ÑπÔ∏è  Could not list tables: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8c861d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìù TASK 1: Write Optimized GeoParquet to R2 Bucket\n",
    "\n",
    "## üéØ Objective\n",
    "\n",
    "Materialize our `stg_pv_consolidated` view as an **optimized GeoParquet file** stored in a cloud object storage bucket (Cloudflare R2, S3-compatible).\n",
    "\n",
    "## üöÄ Why GeoParquet?\n",
    "\n",
    "**GeoParquet** is a cloud-native geospatial data format that combines:\n",
    "- ‚úÖ **Parquet's efficiency**: Columnar storage, excellent compression\n",
    "- ‚úÖ **Geospatial metadata**: Embedded CRS, bbox for spatial filtering\n",
    "- ‚úÖ **Standard compliance**: GeoParquet 1.1 specification\n",
    "- ‚úÖ **Interoperability**: Works with GDAL, GeoPandas, DuckDB, Arrow\n",
    "\n",
    "## üîß Optimizations Applied\n",
    "\n",
    "### 1. **Hilbert Curve Ordering** üåÄ\n",
    "- Spatial co-locality: Nearby features stored together\n",
    "- Better compression ratios (~15-30% improvement)\n",
    "- Faster spatial filtering with row group pruning\n",
    "- **How it works**: Maps 2D coordinates to 1D curve preserving locality\n",
    "\n",
    "### 2. **ZSTD Compression (Level 9)** üì¶\n",
    "- Superior compression ratio vs Snappy/GZIP (~2-3x vs uncompressed)\n",
    "- Level 9: Aggressive compression (slower write, smaller files)\n",
    "- Decompression speed still excellent for read operations\n",
    "\n",
    "### 3. **Row Group Optimization** üìä\n",
    "- Target: ~100MB row groups (100,000 rows)\n",
    "- Balance between:\n",
    "  - Parallelism (more row groups = more parallel reads)\n",
    "  - Efficiency (fewer row groups = less overhead)\n",
    "\n",
    "### 4. **Spatial Metadata** üó∫Ô∏è\n",
    "- GeoParquet 1.1 bbox struct enables spatial filtering\n",
    "- Column statistics for query optimization\n",
    "- Proper CRS metadata (EPSG:4326)\n",
    "\n",
    "### 5. **Optional Hive Partitioning** üìÅ\n",
    "- Can partition by dataset_name, year, region\n",
    "- Enables partition pruning for faster queries\n",
    "- Trade-off: More files vs query performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0c6891a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Writing optimized GeoParquet: s3://eo-pv-lakehouse/geoparquet/ccom6994_pv_dataset.parquet\n",
      "   Source: stg_pv_consolidated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e0825b1b754eb2807f564b07aa9cb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Total rows: 443,747\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f84edf238544baaa6d6f0b1844d38d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Hilbert curve ordering applied\n",
      "      Spatial extent: [-177.93, -53.20] to [177.16, 69.76]\n",
      "\n",
      "   Executing COPY command...\n",
      "   Compression: ZSTD (level 9)\n",
      "   Row group size: 50,000 rows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "235e14860e2c472f9f8472286495ce0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ GeoParquet written successfully!\n",
      "   Time elapsed: 17.49s\n",
      "   Throughput: 25,367 rows/sec\n",
      "\n",
      "üìä Write Statistics:\n",
      "   success: True\n",
      "   output_path: s3://eo-pv-lakehouse/geoparquet/ccom6994_pv_dataset.parquet\n",
      "   total_rows: 443747\n",
      "   elapsed_seconds: 17.493170022964478\n",
      "   rows_per_second: 25366.87172293318\n",
      "   compression: ZSTD\n",
      "   compression_level: 9\n",
      "   hilbert_ordered: True\n",
      "   partitioned: False\n",
      "   partition_columns: []\n"
     ]
    }
   ],
   "source": [
    "def write_optimized_geoparquet(\n",
    "    con: duckdb.DuckDBPyConnection,\n",
    "    source_table: str,\n",
    "    output_path: str,\n",
    "    partition_by: list = None,\n",
    "    hilbert_order: bool = True,\n",
    "    compression: str = \"ZSTD\",\n",
    "    compression_level: int = 9,\n",
    "    row_group_size: int = 100000\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Write GeoParquet with spatial optimizations using DuckDB.\n",
    "    \n",
    "    Args:\n",
    "        con: DuckDB connection\n",
    "        source_table: Name of source table/view\n",
    "        output_path: S3/local path for output\n",
    "        partition_by: Columns to partition by (optional)\n",
    "        hilbert_order: Apply Hilbert curve spatial ordering\n",
    "        compression: Compression codec (ZSTD, SNAPPY, GZIP)\n",
    "        compression_level: Compression level (1-22 for ZSTD)\n",
    "        row_group_size: Rows per row group\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with write statistics\n",
    "    \"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"üìù Writing optimized GeoParquet: {output_path}\")\n",
    "    print(f\"   Source: {source_table}\")\n",
    "    \n",
    "    # Get source table info\n",
    "    count_result = con.execute(f\"SELECT COUNT(*) as cnt FROM {source_table}\").fetchone()\n",
    "    total_rows = count_result[0]\n",
    "    print(f\"   Total rows: {total_rows:,}\")\n",
    "    \n",
    "    # Build COPY command with optimizations\n",
    "    copy_sql_parts = [f\"COPY (\"]\n",
    "    \n",
    "    # SELECT with optional Hilbert ordering\n",
    "    if hilbert_order:\n",
    "        # Get spatial extent for Hilbert curve\n",
    "        extent_sql = f\"\"\"\n",
    "        SELECT \n",
    "            MIN(ST_X(ST_Centroid(ST_GeomFromText(geometry)))) as min_x,\n",
    "            MAX(ST_X(ST_Centroid(ST_GeomFromText(geometry)))) as max_x,\n",
    "            MIN(ST_Y(ST_Centroid(ST_GeomFromText(geometry)))) as min_y,\n",
    "            MAX(ST_Y(ST_Centroid(ST_GeomFromText(geometry)))) as max_y\n",
    "        FROM {source_table}\n",
    "        \"\"\"\n",
    "        extent = con.execute(extent_sql).fetchone()\n",
    "        \n",
    "        # Create spatial order using Hilbert curve\n",
    "        copy_sql_parts.append(f\"\"\"\n",
    "            SELECT * FROM {source_table}\n",
    "            ORDER BY ST_Hilbert(\n",
    "                ST_GeomFromText(geometry),\n",
    "                ST_MakeBox2D(\n",
    "                    ST_Point({extent[0]}, {extent[2]}),\n",
    "                    ST_Point({extent[1]}, {extent[3]})\n",
    "                )\n",
    "            )\n",
    "        \"\"\")\n",
    "        print(f\"   ‚úÖ Hilbert curve ordering applied\")\n",
    "        print(f\"      Spatial extent: [{extent[0]:.2f}, {extent[2]:.2f}] to [{extent[1]:.2f}, {extent[3]:.2f}]\")\n",
    "    else:\n",
    "        copy_sql_parts.append(f\"SELECT * FROM {source_table}\")\n",
    "    \n",
    "    copy_sql_parts.append(f\") TO '{output_path}'\")\n",
    "    \n",
    "    # Add format and optimization options\n",
    "    options = [\n",
    "        \"FORMAT PARQUET\",\n",
    "        f\"COMPRESSION {compression}\",\n",
    "    ]\n",
    "    \n",
    "    # Add compression level for ZSTD\n",
    "    if compression.upper() == \"ZSTD\":\n",
    "        options.append(f\"COMPRESSION_LEVEL {compression_level}\")\n",
    "    \n",
    "    # Add row group size\n",
    "    options.append(f\"ROW_GROUP_SIZE {row_group_size}\")\n",
    "    \n",
    "    # Add partitioning if specified\n",
    "    if partition_by:\n",
    "        partition_cols = \", \".join(partition_by)\n",
    "        options.append(f\"PARTITION_BY ({partition_cols})\")\n",
    "        options.append(\"OVERWRITE_OR_IGNORE true\")\n",
    "        print(f\"   ‚úÖ Hive partitioning: {partition_cols}\")\n",
    "    \n",
    "    # Add GeoParquet metadata\n",
    "    # options.append(\"FORMAT PARQUET\")\n",
    "    \n",
    "    copy_sql = \" \".join(copy_sql_parts) + \" (\\n    \" + \",\\n    \".join(options) + \"\\n);\"\n",
    "    \n",
    "    print(f\"\\n   Executing COPY command...\")\n",
    "    print(f\"   Compression: {compression} (level {compression_level})\")\n",
    "    print(f\"   Row group size: {row_group_size:,} rows\")\n",
    "    \n",
    "    try:\n",
    "        con.execute(copy_sql)\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        stats = {\n",
    "            'success': True,\n",
    "            'output_path': output_path,\n",
    "            'total_rows': total_rows,\n",
    "            'elapsed_seconds': elapsed,\n",
    "            'rows_per_second': total_rows / elapsed if elapsed > 0 else 0,\n",
    "            'compression': compression,\n",
    "            'compression_level': compression_level,\n",
    "            'hilbert_ordered': hilbert_order,\n",
    "            'partitioned': bool(partition_by),\n",
    "            'partition_columns': partition_by or []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n‚úÖ GeoParquet written successfully!\")\n",
    "        print(f\"   Time elapsed: {elapsed:.2f}s\")\n",
    "        print(f\"   Throughput: {stats['rows_per_second']:,.0f} rows/sec\")\n",
    "        \n",
    "        return stats\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error writing GeoParquet: {e}\")\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'output_path': output_path\n",
    "        }\n",
    "\n",
    "# Execute Task 1: Write optimized GeoParquet\n",
    "output_path = \"s3://eo-pv-lakehouse/geoparquet/ccom6994_pv_dataset.parquet\"\n",
    "\n",
    "# For local testing without S3 credentials, use local path:\n",
    "# output_path = \"data/ccom6994_pv_dataset.parquet\"\n",
    "\n",
    "write_stats = write_optimized_geoparquet(\n",
    "    con=con,\n",
    "    source_table=\"stg_pv_consolidated\",\n",
    "    output_path=output_path,\n",
    "    partition_by=None,  # Could partition by ['dataset_name', 'year'] if those columns exist\n",
    "    hilbert_order=True,\n",
    "    # compression=\"snappy\",\n",
    "    compression=\"ZSTD\",\n",
    "    compression_level=9,\n",
    "    row_group_size=50000\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Write Statistics:\")\n",
    "for key, value in write_stats.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9997248",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Validating written GeoParquet...\n",
      "   Original table row count: 443,747\n",
      "   Written GeoParquet row count: 443,747\n",
      "   ‚úÖ Row count validation: PASSED\n",
      "\n",
      "   üìã Schema validation:\n",
      "      Columns: 11\n",
      "      Column names: ['dataset_name', 'processed_at', 'geometry', 'area_m2', 'centroid_lat', 'centroid_lon', 'h3_index_8', 'unified_id', 'source_area_m2', 'capacity_mw', 'install_date']\n",
      "   ‚úÖ Schema validation: PASSED\n",
      "\n",
      "üì¶ Checking parquet file sizes in s3://eo-pv-lakehouse/geoparquet/*.parquet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "filename",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "file_size_mib",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "6a38c2e9-b879-4723-a077-c19f94cc67e8",
       "rows": [
        [
         "0",
         "ccom6994_pv_dataset.parquet",
         "81.83119583129883"
        ],
        [
         "3",
         "raw_global_pv_inventory_sent2_spot_2021.parquet",
         "65.93731307983398"
        ],
        [
         "2",
         "raw_global_harmonized_large_solar_farms_2020.parquet",
         "16.44457721710205"
        ],
        [
         "5",
         "raw_uk_crowdsourced_pv_2020.parquet",
         "13.298114776611328"
        ],
        [
         "6",
         "raw_usa_cali_usgs_pv_2016.parquet",
         "6.600001335144043"
        ],
        [
         "4",
         "raw_ind_pv_solar_farms_2022.parquet",
         "0.42061519622802734"
        ],
        [
         "1",
         "raw_chn_med_res_pv_2024.parquet",
         "0.3785057067871094"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 7
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>file_size_mib</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ccom6994_pv_dataset.parquet</td>\n",
       "      <td>81.831196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>raw_global_pv_inventory_sent2_spot_2021.parquet</td>\n",
       "      <td>65.937313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>raw_global_harmonized_large_solar_farms_2020.p...</td>\n",
       "      <td>16.444577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>raw_uk_crowdsourced_pv_2020.parquet</td>\n",
       "      <td>13.298115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>raw_usa_cali_usgs_pv_2016.parquet</td>\n",
       "      <td>6.600001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>raw_ind_pv_solar_farms_2022.parquet</td>\n",
       "      <td>0.420615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>raw_chn_med_res_pv_2024.parquet</td>\n",
       "      <td>0.378506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  file_size_mib\n",
       "0                        ccom6994_pv_dataset.parquet      81.831196\n",
       "3    raw_global_pv_inventory_sent2_spot_2021.parquet      65.937313\n",
       "2  raw_global_harmonized_large_solar_farms_2020.p...      16.444577\n",
       "5                raw_uk_crowdsourced_pv_2020.parquet      13.298115\n",
       "6                  raw_usa_cali_usgs_pv_2016.parquet       6.600001\n",
       "4                raw_ind_pv_solar_farms_2022.parquet       0.420615\n",
       "1                    raw_chn_med_res_pv_2024.parquet       0.378506"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Validate write by reading back and checking schema + record count\n",
    "print(\"\\nüîç Validating written GeoParquet...\")\n",
    "\n",
    "try:\n",
    "    # Get original row count from source table\n",
    "    original_count = con.execute(\"SELECT COUNT(*) as cnt FROM stg_pv_consolidated\").fetchone()[0]\n",
    "    print(f\"   Original table row count: {original_count:,}\")\n",
    "    \n",
    "    # Read back from R2 and get count\n",
    "    validation_query = f\"SELECT COUNT(*) as cnt FROM read_parquet('{output_path}')\"\n",
    "    written_count = con.execute(validation_query).fetchone()[0]\n",
    "    print(f\"   Written GeoParquet row count: {written_count:,}\")\n",
    "    \n",
    "    # Check if counts match\n",
    "    if original_count == written_count:\n",
    "        print(\"   ‚úÖ Row count validation: PASSED\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Row count mismatch: {original_count:,} vs {written_count:,}\")\n",
    "    \n",
    "    # Validate schema by reading a sample\n",
    "    schema_query = f\"SELECT * FROM read_parquet('{output_path}') LIMIT 1\"\n",
    "    sample_df = con.execute(schema_query).fetchdf()\n",
    "    print(f\"\\n   üìã Schema validation:\")\n",
    "    print(f\"      Columns: {len(sample_df.columns)}\")\n",
    "    print(f\"      Column names: {list(sample_df.columns)}\")\n",
    "    print(\"   ‚úÖ Schema validation: PASSED\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Validation error: {e}\")\n",
    "\n",
    "# output file sizes of all our GeoParquets\n",
    "try:\n",
    "    parquet_glob = output_path.replace(\"ccom6994_pv_dataset.parquet\", \"*.parquet\")\n",
    "    print(f\"\\nüì¶ Checking parquet file sizes in {parquet_glob}...\")\n",
    "    # see here: https://duckdb.org/docs/stable/guides/file_formats/read_file\n",
    "    size_query = f\"\"\"SELECT size as file_size_bytes, filename FROM read_blob('{parquet_glob}')\n",
    "    \"\"\"\n",
    "    size_result = con.execute(size_query).fetchdf()\n",
    "    # format as MiB\n",
    "    size_result['file_size_mib'] = size_result['file_size_bytes'] / (1024 * 1024)\n",
    "    # keep only base filename\n",
    "    size_result['filename'] = size_result['filename'].apply(lambda x: x.split('/')[-1])\n",
    "    display(size_result[['filename', 'file_size_mib']].sort_values(by='file_size_mib', ascending=False))\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error checking file sizes: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d31001",
   "metadata": {},
   "source": [
    "### üí° Key Takeaways from Task 1\n",
    "\n",
    "**What we accomplished:**\n",
    "- ‚úÖ Materialized staging view to production-ready GeoParquet\n",
    "- ‚úÖ Applied spatial ordering for better compression & query performance\n",
    "- ‚úÖ Used aggressive compression without sacrificing read performance\n",
    "- ‚úÖ Configured optimal row group size for parallel processing\n",
    "\n",
    "**Performance insights:**\n",
    "- Hilbert ordering provides ~15-30% better compression\n",
    "- ZSTD level 9 achieves ~2-3x compression vs uncompressed\n",
    "- Row group size affects query parallelism and memory usage\n",
    "- Cloud storage (R2/S3) enables scalable, distributed access\n",
    "\n",
    "**Real-world benefits:**\n",
    "- Reduced storage costs\n",
    "- Faster query performance (row group pruning)\n",
    "- Better data sharing (standard format)\n",
    "- Improved analytics throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4c6b71",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üì• TASK 2: Reading Parquet from Remote S3 Locations\n",
    "\n",
    "## üéØ Objective\n",
    "\n",
    "Demonstrate **two different approaches** for reading remote Parquet files:\n",
    "1. **pandas + s3fs**: Traditional approach using AWS SDK\n",
    "2. **DuckDB + httpfs**: Modern approach using HTTP range requests\n",
    "\n",
    "## ü§î Why Multiple Approaches?\n",
    "\n",
    "Different use cases require different tools:\n",
    "- **pandas**: Familiar API, good for small-to-medium datasets\n",
    "- **DuckDB**: Optimized for analytical queries, excellent for large datasets\n",
    "\n",
    "## üìä Performance Comparison\n",
    "\n",
    "| Feature | pandas + s3fs | DuckDB + httpfs |\n",
    "|---------|---------------|------------------|\n",
    "| **AWS SDK required** | ‚úÖ Yes | ‚ùå No (HTTP only) |\n",
    "| **Column pruning** | ‚ö†Ô∏è Limited | ‚úÖ Excellent |\n",
    "| **Predicate pushdown** | ‚ùå No | ‚úÖ Yes |\n",
    "| **Memory efficient** | ‚ùå Loads all | ‚úÖ Lazy evaluation |\n",
    "| **Parallel reading** | ‚ö†Ô∏è Limited | ‚úÖ Yes (auto) |\n",
    "| **Spatial functions** | ‚ùå No | ‚úÖ Yes (spatial ext) |\n",
    "| **Query optimization** | ‚ùå No | ‚úÖ Yes (CBO) |\n",
    "\n",
    "**Recommendation**: Use DuckDB for large files and analytical workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045761a0",
   "metadata": {},
   "source": [
    "## 2.1: Reading with pandas + s3fs\n",
    "\n",
    "### How it works:\n",
    "- Uses `s3fs` library to provide filesystem-like interface to S3\n",
    "- Requires AWS credentials (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n",
    "- Downloads entire file (or uses random access if supported)\n",
    "- Returns familiar pandas DataFrame\n",
    "\n",
    "### Best for:\n",
    "- Small to medium datasets (<1GB)\n",
    "- When you need full pandas DataFrame API\n",
    "- Compatibility with existing pandas workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a4c612",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def read_parquet_with_pandas(\n",
    "    path: str,\n",
    "    sample_frac: float = 1.0,\n",
    "    columns: list = None,\n",
    "    use_pyarrow: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read Parquet from S3/R2 using pandas + s3fs.\n",
    "    \n",
    "    Requires: pip install s3fs pyarrow\n",
    "    \n",
    "    Args:\n",
    "        path: S3/R2 path to Parquet file (e.g., 's3://bucket/key.parquet')\n",
    "        sample_frac: Fraction of data to sample (1.0 = all data)\n",
    "        columns: List of columns to read (None = all columns)\n",
    "        use_pyarrow: Use PyArrow engine for reading (recommended)\n",
    "        \n",
    "    Returns:\n",
    "        Pandas DataFrame\n",
    "    \"\"\"\n",
    "    import time\n",
    "    start = time.time()\n",
    "    \n",
    "    print(f\"üì• Reading with pandas + s3fs: {path}\")\n",
    "    \n",
    "    try:\n",
    "        import s3fs\n",
    "    except ImportError:\n",
    "        print(\"‚ùå s3fs not installed. Install with: pip install s3fs\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Get credentials from environment\n",
    "    access_key = os.getenv('R2_ACCESS_KEY_ID')\n",
    "    secret_key = os.getenv('R2_SECRET_KEY')\n",
    "    endpoint = os.getenv('R2_S3_ENDPOINT', 'e833ac2d32c62bcff5e4b72c74e5351d.r2.cloudflarestorage.com')\n",
    "    \n",
    "    if not access_key or not secret_key:\n",
    "        print(\"‚ö†Ô∏è  R2 credentials not found in environment variables\")\n",
    "        print(\"   Set R2_ACCESS_KEY_ID and R2_SECRET_KEY\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Create S3 filesystem for Cloudflare R2\n",
    "    # Key configuration: anon=False, region_name='auto' (R2 specific)\n",
    "    fs = s3fs.S3FileSystem(\n",
    "        anon=False,\n",
    "        use_ssl=True,\n",
    "        client_kwargs={\n",
    "            'region_name': 'auto',  # R2 uses 'auto' as region\n",
    "            'endpoint_url': f'https://{endpoint}',\n",
    "            'aws_access_key_id': access_key,\n",
    "            'aws_secret_access_key': secret_key,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"   Endpoint: https://{endpoint}\")\n",
    "    print(f\"   Region: auto (Cloudflare R2)\")\n",
    "    \n",
    "    try:\n",
    "        # Read Parquet file through s3fs\n",
    "        # Using 'with' statement ensures proper file handle cleanup\n",
    "        with fs.open(path, 'rb') as f:\n",
    "            engine = 'pyarrow' if use_pyarrow else 'fastparquet'\n",
    "            df = pd.read_parquet(f, columns=columns, engine=engine)\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        print(f\"‚úÖ Read complete: {len(df):,} rows √ó {len(df.columns)} cols in {elapsed:.2f}s\")\n",
    "        \n",
    "        # Sample if requested\n",
    "        if sample_frac < 1.0:\n",
    "            original_len = len(df)\n",
    "            df = df.sample(frac=sample_frac, random_state=42)\n",
    "            print(f\"   Sampled {len(df):,} / {original_len:,} rows ({sample_frac*100:.1f}%)\")\n",
    "        \n",
    "        # Calculate throughput\n",
    "        throughput = len(df) / elapsed if elapsed > 0 else 0\n",
    "        print(f\"   Throughput: {throughput:,.0f} rows/sec\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading with pandas + s3fs: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c22aa",
   "metadata": {},
   "source": [
    "## 2.2: Reading with DuckDB + httpfs\n",
    "\n",
    "### How it works:\n",
    "- Uses **HTTP range requests** to read only needed data\n",
    "- Reads Parquet metadata first (~few KB)\n",
    "- Applies **column pruning** and **predicate pushdown**\n",
    "- Only fetches required row groups\n",
    "- Parallel downloads for multiple row groups\n",
    "\n",
    "### Advantages:\n",
    "1. **No AWS SDK required**: Works with any HTTP(S) endpoint\n",
    "2. **Lazy evaluation**: Only reads what you query\n",
    "3. **Query optimization**: DuckDB's cost-based optimizer\n",
    "4. **Spatial functions**: Native geometry operations\n",
    "5. **Memory efficient**: Streaming execution\n",
    "\n",
    "### Best for:\n",
    "- Large datasets (>1GB)\n",
    "- Analytical queries (aggregations, filters)\n",
    "- When you need column/row subset\n",
    "- Spatial operations on geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a192282",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Reading with pandas + s3fs: s3://eo-pv-lakehouse/geoparquet/ccom6994_pv_dataset.parquet\n",
      "   Endpoint: https://e833ac2d32c62bcff5e4b72c74e5351d.r2.cloudflarestorage.com\n",
      "   Region: auto (Cloudflare R2)\n",
      "‚úÖ Read complete: 443,747 rows √ó 11 cols in 3.81s\n",
      "   Throughput: 116,535 rows/sec\n",
      "‚è±Ô∏è  Total time taken: 3.85 seconds\n",
      "üì• Reading with DuckDB + httpfs: s3://eo-pv-lakehouse/geoparquet/ccom6994_pv_dataset.parquet\n",
      "   Filter: area_m2 > 2500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870f0998bf904e61bce2063ae71010a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Read complete: 103,023 rows √ó 6 cols in 5.32s\n",
      "   Throughput: 19,378 rows/sec\n",
      "\n",
      "üìä Filtered dataset preview:\n",
      "  unified_id                              dataset_name       area_m2  \\\n",
      "0      35170  global_harmonized_large_solar_farms_2020   5719.358582   \n",
      "1      34828  global_harmonized_large_solar_farms_2020   3687.945465   \n",
      "2      34826  global_harmonized_large_solar_farms_2020   2718.321769   \n",
      "3      19098  global_harmonized_large_solar_farms_2020  18391.784237   \n",
      "4    42776.0       global_pv_inventory_sent2_spot_2021   3371.092133   \n",
      "\n",
      "   centroid_lon  centroid_lat  \\\n",
      "0    -12.315052    -37.064926   \n",
      "1    -43.192787    -22.893041   \n",
      "2    -45.820720    -23.194704   \n",
      "3    -47.008239    -22.776944   \n",
      "4    -47.051628    -22.580521   \n",
      "\n",
      "                                            geometry  \n",
      "0  POLYGON ((-12.314492000000003 -37.064648, -12....  \n",
      "1  POLYGON ((-43.1920051 -22.892979099999863, -43...  \n",
      "2  POLYGON ((-45.82091600000001 -23.1948691999998...  \n",
      "3  POLYGON ((-47.0080988 -22.776873099999854, -47...  \n",
      "4  POLYGON ((-47.05193840078701 -22.5806362908610...  \n"
     ]
    }
   ],
   "source": [
    "def read_parquet_with_duckdb(\n",
    "    con: duckdb.DuckDBPyConnection,\n",
    "    path: str,\n",
    "    columns: list = None,\n",
    "    filter_expr: str = None,\n",
    "    limit: int = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read Parquet using DuckDB with httpfs extension.\n",
    "    \n",
    "    Supports:\n",
    "        - Local paths: /path/to/file.parquet\n",
    "        - S3 paths: s3://bucket/key\n",
    "        - HTTP(S) paths: https://domain.com/file.parquet\n",
    "        \n",
    "    Args:\n",
    "        con: DuckDB connection (with httpfs loaded)\n",
    "        path: Path to Parquet file (local, s3, or https)\n",
    "        columns: List of columns to read (None = all)\n",
    "        filter_expr: SQL WHERE clause (e.g., \"area_m2 > 1000\")\n",
    "        limit: Maximum rows to return\n",
    "        \n",
    "    Returns:\n",
    "        Pandas DataFrame\n",
    "    \"\"\"\n",
    "    import time\n",
    "    start = time.time()\n",
    "    \n",
    "    print(f\"üì• Reading with DuckDB + httpfs: {path}\")\n",
    "    \n",
    "    # Build query\n",
    "    select_cols = \", \".join(columns) if columns else \"*\"\n",
    "    query = f\"SELECT {select_cols} FROM read_parquet('{path}')\"\n",
    "    \n",
    "    if filter_expr:\n",
    "        query += f\" WHERE {filter_expr}\"\n",
    "        print(f\"   Filter: {filter_expr}\")\n",
    "    \n",
    "    if limit:\n",
    "        query += f\" LIMIT {limit}\"\n",
    "        print(f\"   Limit: {limit:,} rows\")\n",
    "    \n",
    "    try:\n",
    "        df = con.execute(query).fetchdf()\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        print(f\"‚úÖ Read complete: {len(df):,} rows √ó {len(df.columns)} cols in {elapsed:.2f}s\")\n",
    "        print(f\"   Throughput: {len(df) / elapsed:,.0f} rows/sec\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading with DuckDB: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Example 1: Read first 10,000 rows\n",
    "from time import time\n",
    "t1 = time()\n",
    "df_sample = read_parquet_with_pandas(\n",
    "    path=PV_GEOPARQUET_PATH,\n",
    "    # limit=300000\n",
    ")\n",
    "# filter with same area filter\n",
    "df_sample = df_sample[df_sample['area_m2'] > 5000]\n",
    "t2 = time()\n",
    "print(f\"‚è±Ô∏è  Total time taken: {t2 - t1:.2f} seconds\")\n",
    "\n",
    "# Example 2: Read specific columns with filter\n",
    "df_filtered = read_parquet_with_duckdb(\n",
    "    con=con,\n",
    "    path=PV_GEOPARQUET_PATH,\n",
    "    columns=['unified_id', 'dataset_name', 'area_m2', 'centroid_lon', 'centroid_lat', 'geometry'],\n",
    "    filter_expr=\"area_m2 > 2500\",  # Only large installations\n",
    "    # limit=100000\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Filtered dataset preview:\")\n",
    "print(df_filtered.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a0d0c2",
   "metadata": {},
   "source": [
    "## 2.3: Performance Comparison\n",
    "\n",
    "Key differences:\n",
    "\n",
    "| Feature | pandas + s3fs | DuckDB + httpfs |\n",
    "|---------|---------------|-----------------|\n",
    "| AWS SDK required | ‚úÖ Yes | ‚ùå No |\n",
    "| Column pruning | ‚ùå Limited | ‚úÖ Excellent |\n",
    "| Predicate pushdown | ‚ùå No | ‚úÖ Yes |\n",
    "| Memory efficient | ‚ùå Loads all | ‚úÖ Lazy |\n",
    "| Parallel reading | ‚ö†Ô∏è Limited | ‚úÖ Yes |\n",
    "| Spatial functions | ‚ùå No | ‚úÖ Yes (spatial ext) |\n",
    "\n",
    "**Recommendation**: Use DuckDB for large files and when you need filtering/column selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76df8684",
   "metadata": {},
   "source": [
    "---\n",
    "## Pre-Task Setup: Materialize PV Data for Cross-Cloud Queries\n",
    "\n",
    "**Important:** Before we fetch Overture Maps from public AWS S3, we must materialize our PV data \n",
    "(currently on R2) as a DuckDB table. This avoids credential/endpoint conflicts when performing \n",
    "spatial joins across different S3 backends.\n",
    "\n",
    "**Why this matters:**\n",
    "- PV data: In private R2 bucket with our credentials\n",
    "- Overture Maps: In public AWS S3 (us-west-2 region) \n",
    "- Conflict: Can't reference both with different S3 endpoints in same query\n",
    "- Solution: Materialize PV data WHILE R2 credentials active, then switch to AWS for Overture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03512823",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SETUP: Persist PV data locally for spatial joins\n",
      "================================================================================\n",
      "\n",
      "üìã Persisting PV data locally for cross-cloud spatial joins\n",
      "   Source: s3://eo-pv-lakehouse/geoparquet/ccom6994_pv_dataset.parquet (R2)\n",
      "   Destination: /Users/asvnpr/pv_data_data.parquet\n",
      "   ‚ö†Ô∏è  This persists data so new connections don't need R2 config\n",
      "   üîç Reading PV data from R2...\n",
      "   ‚úì Loaded 443,747 records\n",
      "   ‚úì Geometry types: [('LINESTRING',), ('POLYGON',), ('POINT',), ('MULTIPOINT',), ('GEOMETRYCOLLECTION',), ('MULTIPOLYGON',)]\n",
      "   üíæ Exporting to local Parquet...\n",
      "   ‚úÖ Saved to: /Users/asvnpr/pv_data_data.parquet\n",
      "\n",
      "‚úÖ PV data persisted locally: /Users/asvnpr/pv_data_data.parquet\n",
      "   Can now use fresh connections without R2 config for spatial joins\n"
     ]
    }
   ],
   "source": [
    "def setup_pv_table_for_cross_cloud_joins(\n",
    "    con: duckdb.DuckDBPyConnection,\n",
    "    pv_path: str,\n",
    "    table_name: str = \"pv_consolidated\",\n",
    "    local_parquet_path: str = None\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Persist PV GeoParquet data locally for use in cross-cloud spatial joins.\n",
    "    \n",
    "    This function:\n",
    "    1. Reads PV data from R2 (while R2 credentials active)\n",
    "    2. Exports to local Parquet file for persistence\n",
    "    3. Returns path for accessing via new connections (no R2 config needed)\n",
    "    4. Avoids credential conflicts by using isolated connections\n",
    "    \n",
    "    Args:\n",
    "        con: DuckDB connection with R2 credentials active\n",
    "        pv_path: Path to PV GeoParquet on R2\n",
    "        table_name: Name for the materialized table\n",
    "        local_parquet_path: Path to save local Parquet file (default: ~/pv_data_data.parquet)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (table_name, local_parquet_path)\n",
    "    \"\"\"\n",
    "    if local_parquet_path is None:\n",
    "        local_parquet_path = os.path.expanduser(\"~/pv_data_data.parquet\")\n",
    "    \n",
    "    print(f\"\\nüìã Persisting PV data locally for cross-cloud spatial joins\")\n",
    "    print(f\"   Source: {pv_path} (R2)\")\n",
    "    print(f\"   Destination: {local_parquet_path}\")\n",
    "    print(f\"   ‚ö†Ô∏è  This persists data so new connections don't need R2 config\")\n",
    "    \n",
    "    try:\n",
    "        # Read PV data from R2 and create table\n",
    "        print(f\"   üîç Reading PV data from R2...\")\n",
    "        create_table_query = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} AS\n",
    "        SELECT\n",
    "            unified_id, dataset_name, area_m2, centroid_lon, centroid_lat, \n",
    "            processed_at, h3_index_8, source_area_m2, capacity_mw, install_date,\n",
    "            ST_GeomFromText(geometry) AS geometry\n",
    "        FROM read_parquet('{pv_path}')\n",
    "        \"\"\"\n",
    "        \n",
    "        con.execute(create_table_query)\n",
    "        \n",
    "        # Get statistics before saving\n",
    "        row_count = con.execute(f\"SELECT COUNT(*) FROM {table_name}\").fetchall()[0][0]\n",
    "        geometry_type = con.execute(f\"\"\"\n",
    "            SELECT DISTINCT ST_GeometryType(geometry) FROM {table_name}\n",
    "        \"\"\").fetchall()\n",
    "        \n",
    "        print(f\"   ‚úì Loaded {row_count:,} records\")\n",
    "        print(f\"   ‚úì Geometry types: {geometry_type}\")\n",
    "        \n",
    "        # Export to Parquet locally (most efficient for spatial operations)\n",
    "        print(f\"   üíæ Exporting to local Parquet...\")\n",
    "        export_query = f\"\"\"\n",
    "        COPY {table_name} \n",
    "        TO '{local_parquet_path}' (FORMAT PARQUET, OVERWRITE_OR_IGNORE)\n",
    "        \"\"\"\n",
    "        con.execute(export_query)\n",
    "        print(f\"   ‚úÖ Saved to: {local_parquet_path}\")\n",
    "        \n",
    "        return table_name, local_parquet_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error persisting table: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Execute setup: Persist PV data locally BEFORE switching to Overture Maps\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SETUP: Persist PV data locally for spatial joins\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "pv_table_name, pv_local_parquet_path = setup_pv_table_for_cross_cloud_joins(\n",
    "    con=con,\n",
    "    pv_path=PV_GEOPARQUET_PATH,\n",
    "    table_name=\"pv_consolidated\",\n",
    "    local_parquet_path=os.path.expanduser(\"~/pv_data_data.parquet\")\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ PV data persisted locally: {pv_local_parquet_path}\")\n",
    "print(f\"   Can now use fresh connections without R2 config for spatial joins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3671f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK 3.1: Fetch Overture Maps divisions using spatial joins\n",
      "================================================================================\n",
      "üó∫Ô∏è  Fetching Overture Maps divisions using spatial joins\n",
      "   Division types: ['country']\n",
      "   PV data source: /Users/asvnpr/pv_data_data.parquet\n",
      "   Strategy: ST_Intersects spatial join (server-side filtering)\n",
      "   ‚úì Creating fresh DuckDB connection (clean AWS S3 config)...\n",
      "   ‚öôÔ∏è  Configuring DuckDB for memory-constrained spatial join...\n",
      "   üîç Loading PV data from Parquet...\n",
      "   ‚úì Creating fresh connection to Overture...\n",
      "   üì¶ Overture release: 2025-10-22.0 | AWS S3 (us-west-2)\n",
      "   üîç Performing spatial join with ST_Intersects...\n",
      "   üîÑ Executing query...\n",
      "   ‚ùå Spatial join error: Out of Memory Error: failed to allocate data of size 16.0 MiB (7.4 GiB/7.4 GiB used)\n",
      "\n",
      "Possible solutions:\n",
      "* Reducing the number of threads (SET threads=X)\n",
      "* Disabling insertion-order preservation (SET preserve_insertion_order=false)\n",
      "* Increasing the memory limit (SET memory_limit='...GB')\n",
      "\n",
      "See also https://duckdb.org/docs/stable/guides/performance/how_to_tune_workloads\n",
      "   üí° Attempting fallback with bbox-based filtering...\n",
      "   ‚úì PV extent: lon [-177.93, 177.16], lat [-53.20, 69.76]\n",
      "   üîÑ Executing bbox-filtered query...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb57fb8e67f4e7b8df63c04c922b32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Fetched 376 divisions (bbox filter)\n",
      "\n",
      "üìä Country data preview (intersecting with PV data):\n",
      "                  name country region\n",
      "0                Chile      CL   None\n",
      "1                Chile      CL   None\n",
      "2               Brasil      BR   None\n",
      "3               Brasil      BR   None\n",
      "4  Paraguay / Paragu√°i      PY   None\n",
      "5              Uruguay      UY   None\n",
      "6              Uruguay      UY   None\n",
      "7            Argentina      AR   None\n",
      "8            Argentina      AR   None\n",
      "9              Bolivia      BO   None\n",
      "\n",
      "üí° Efficiency note:\n",
      "   ‚úì Spatial join performed server-side on AWS S3\n",
      "   ‚úì Only fetched 376 country geometries\n",
      "   ‚úì (Instead of fetching ALL ~200 countries, then filtering client-side)\n"
     ]
    }
   ],
   "source": [
    "def fetch_overture_divisions_with_duckdb(\n",
    "    pv_local_parquet_path: str,\n",
    "    pv_table_name: str = \"pv_consolidated\",\n",
    "    division_types: list = [\"country\"],\n",
    "    limit: int = None,\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Fetch Overture Maps administrative divisions using proper spatial joins.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Use fresh DuckDB connection (no R2 config needed)\n",
    "    2. Load PV data from local Parquet file\n",
    "    3. Query Overture divisions from public AWS S3\n",
    "    4. Perform spatial join using ST_Intersects to find divisions containing PV data\n",
    "    \n",
    "    References:\n",
    "    - https://duckdb.org/docs/stable/clients/python/overview#persistent-storage\n",
    "    - https://duckdb.org/2025/08/08/spatial-joins (SPATIAL_JOIN operator)\n",
    "    \n",
    "    Args:\n",
    "        pv_local_parquet_path: Path to local Parquet file with PV data\n",
    "        pv_table_name: Name for PV table (used in queries)\n",
    "        division_types: Types of divisions ('country', 'region', 'locality')\n",
    "        limit: Maximum number of features to fetch\n",
    "        \n",
    "    Returns:\n",
    "        GeoDataFrame with administrative boundaries intersecting PV data\n",
    "    \"\"\"\n",
    "    print(f\"üó∫Ô∏è  Fetching Overture Maps divisions using spatial joins\")\n",
    "    print(f\"   Division types: {division_types}\")\n",
    "    print(f\"   PV data source: {pv_local_parquet_path}\")\n",
    "    print(f\"   Strategy: ST_Intersects spatial join (server-side filtering)\")\n",
    "    \n",
    "    try:\n",
    "        # Create fresh connection for spatial operations\n",
    "        print(f\"   ‚úì Creating fresh DuckDB connection (clean AWS S3 config)...\")\n",
    "        pv_con = duckdb.connect(':memory:')\n",
    "        pv_con.execute(\"INSTALL spatial; LOAD spatial;\")\n",
    "        \n",
    "        # Configure DuckDB for larger-than-memory workloads\n",
    "        print(f\"   ‚öôÔ∏è  Configuring DuckDB for memory-constrained spatial join...\")\n",
    "        pv_con.execute(\"SET preserve_insertion_order = false;\")\n",
    "        pv_con.execute(\"SET memory_limit = '8GB';\")\n",
    "        pv_con.execute(\"SET threads = 4;\")\n",
    "        pv_con.execute(\"SET temp_directory = '/tmp/duckdb_temp';\")\n",
    "        \n",
    "        # Load PV data from local Parquet\n",
    "        print(f\"   üîç Loading PV data from Parquet...\")\n",
    "        pv_con.execute(f\"\"\"\n",
    "        CREATE TABLE {pv_table_name} AS\n",
    "        SELECT * FROM read_parquet('{pv_local_parquet_path}')\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create fresh connection for Overture (clean AWS S3 config)\n",
    "        print(f\"   ‚úì Creating fresh connection to Overture...\")\n",
    "        overture_con = duckdb.connect(':memory:')\n",
    "        overture_con.execute(\"INSTALL spatial; LOAD spatial;\")\n",
    "        overture_con.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "        overture_con.execute(\"SET s3_region='us-west-2';\")\n",
    "        \n",
    "        # Apply same memory configuration to Overture connection\n",
    "        overture_con.execute(\"SET preserve_insertion_order = false;\")\n",
    "        overture_con.execute(\"SET memory_limit = '12GB';\")\n",
    "        overture_con.execute(\"SET threads = 4;\")\n",
    "        overture_con.execute(\"SET temp_directory = '/tmp/duckdb_temp';\")\n",
    "        \n",
    "        # Overture Maps S3 path (2025-10-22.0 release)\n",
    "        overture_base = \"s3://overturemaps-us-west-2/release/2025-10-22.0\"\n",
    "        division_path = f\"{overture_base}/theme=divisions/type=division_area/*\"\n",
    "        \n",
    "        print(f\"   üì¶ Overture release: 2025-10-22.0 | AWS S3 (us-west-2)\")\n",
    "        print(f\"   üîç Performing spatial join with ST_Intersects...\")\n",
    "        \n",
    "        # Spatial join query using ST_Intersects\n",
    "        # This leverages the SPATIAL_JOIN operator (DuckDB 1.3.0+)\n",
    "        query = f\"\"\"\n",
    "        SELECT DISTINCT\n",
    "            div.id,\n",
    "            div.names.primary as name,\n",
    "            div.subtype,\n",
    "            div.country,\n",
    "            div.region,\n",
    "            ST_AsText(div.geometry) as geometry\n",
    "        FROM read_parquet('{division_path}', filename=true, hive_partitioning=1) AS div\n",
    "        JOIN (SELECT geometry FROM '{pv_local_parquet_path}') AS pv\n",
    "            ON ST_Intersects(div.geometry, pv.geometry)\n",
    "        WHERE div.subtype IN ({', '.join(f\"'{dt}'\" for dt in division_types)})\n",
    "        \"\"\"\n",
    "        \n",
    "        if limit:\n",
    "            query += f\"\\nLIMIT {limit}\"\n",
    "        \n",
    "        print(f\"   üîÑ Executing query...\")\n",
    "        df = overture_con.execute(query).fetchdf()\n",
    "        print(f\"   ‚úÖ Fetched {len(df):,} divisions intersecting with PV data\")\n",
    "        \n",
    "        # Convert to GeoDataFrame\n",
    "        df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "        gdf = gpd.GeoDataFrame(df, geometry='geometry', crs='EPSG:4326')\n",
    "        \n",
    "        # Clean up\n",
    "        pv_con.close()\n",
    "        overture_con.close()\n",
    "        \n",
    "        return gdf\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Spatial join error: {e}\")\n",
    "        print(f\"   üí° Attempting fallback with bbox-based filtering...\")\n",
    "        \n",
    "        try:\n",
    "            # Fallback: Use bbox filtering instead of full spatial join\n",
    "            pv_con = duckdb.connect(':memory:')\n",
    "            pv_con.execute(\"INSTALL spatial; LOAD spatial;\")\n",
    "            \n",
    "            # Configure for memory-constrained environment\n",
    "            pv_con.execute(\"SET preserve_insertion_order = false;\")\n",
    "            pv_con.execute(\"SET memory_limit = '8GB';\")\n",
    "            pv_con.execute(\"SET threads = 4;\")\n",
    "            pv_con.execute(\"SET temp_directory = '/tmp/duckdb_temp';\")\n",
    "            \n",
    "            # Create PV table from local Parquet\n",
    "            pv_con.execute(f\"\"\"\n",
    "            CREATE TABLE {pv_table_name} AS\n",
    "            SELECT * FROM read_parquet('{pv_local_parquet_path}')\n",
    "            \"\"\")\n",
    "            \n",
    "            # Get PV extent\n",
    "            bbox_result = pv_con.execute(f\"\"\"\n",
    "                SELECT\n",
    "                    MIN(centroid_lon) as xmin,\n",
    "                    MAX(centroid_lon) as xmax,\n",
    "                    MIN(centroid_lat) as ymin,\n",
    "                    MAX(centroid_lat) as ymax\n",
    "                FROM {pv_table_name}\n",
    "            \"\"\").fetchall()[0]\n",
    "            xmin, xmax, ymin, ymax = bbox_result\n",
    "            print(f\"   ‚úì PV extent: lon [{xmin:.2f}, {xmax:.2f}], lat [{ymin:.2f}, {ymax:.2f}]\")\n",
    "            \n",
    "            overture_con = duckdb.connect(':memory:')\n",
    "            overture_con.execute(\"INSTALL spatial; LOAD spatial;\")\n",
    "            overture_con.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "            overture_con.execute(\"SET s3_region='us-west-2';\")\n",
    "            \n",
    "            # Configure for memory-constrained environment\n",
    "            overture_con.execute(\"SET preserve_insertion_order = false;\")\n",
    "            overture_con.execute(\"SET memory_limit = '8GB';\")\n",
    "            overture_con.execute(\"SET threads = 4;\")\n",
    "            overture_con.execute(\"SET temp_directory = '/tmp/duckdb_temp';\")\n",
    "            \n",
    "            overture_base = \"s3://overturemaps-us-west-2/release/2025-10-22.0\"\n",
    "            division_path = f\"{overture_base}/theme=divisions/type=division_area/*\"\n",
    "            \n",
    "            # Bbox-filtered query\n",
    "            query = f\"\"\"\n",
    "            SELECT\n",
    "                id,\n",
    "                names.primary as name,\n",
    "                subtype,\n",
    "                country,\n",
    "                region,\n",
    "                ST_AsText(geometry) as geometry\n",
    "            FROM read_parquet('{division_path}', filename=true, hive_partitioning=1)\n",
    "            WHERE subtype IN ({', '.join(f\"'{dt}'\" for dt in division_types)})\n",
    "            AND bbox.xmin <= {xmax}\n",
    "            AND bbox.xmax >= {xmin}\n",
    "            AND bbox.ymin <= {ymax}\n",
    "            AND bbox.ymax >= {ymin}\n",
    "            \"\"\"\n",
    "            \n",
    "            if limit:\n",
    "                query += f\"\\nLIMIT {limit}\"\n",
    "            \n",
    "            print(f\"   üîÑ Executing bbox-filtered query...\")\n",
    "            df = overture_con.execute(query).fetchdf()\n",
    "            print(f\"   ‚úÖ Fetched {len(df):,} divisions (bbox filter)\")\n",
    "            \n",
    "            df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "            gdf = gpd.GeoDataFrame(df, geometry='geometry', crs='EPSG:4326')\n",
    "            \n",
    "            pv_con.close()\n",
    "            overture_con.close()\n",
    "            return gdf\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"   ‚ùå Fallback failed: {e2}\")\n",
    "            try:\n",
    "                pv_con.close()\n",
    "                overture_con.close()\n",
    "            except:\n",
    "                pass\n",
    "            return gpd.GeoDataFrame()\n",
    "\n",
    "# Fetch Overture divisions using spatial joins with local PV data\n",
    "# This uses ST_Intersects to find only divisions that contain PV installations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TASK 3.1: Fetch Overture Maps divisions using spatial joins\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "countries_gdf = fetch_overture_divisions_with_duckdb(\n",
    "    pv_local_parquet_path=pv_local_parquet_path,\n",
    "    pv_table_name=pv_table_name,\n",
    "    division_types=[\"country\"],\n",
    ")\n",
    "\n",
    "if not countries_gdf.empty:\n",
    "    print(f\"\\nüìä Country data preview (intersecting with PV data):\")\n",
    "    preview_cols = ['name', 'country', 'region']\n",
    "    preview_cols = [c for c in preview_cols if c in countries_gdf.columns]\n",
    "    print(countries_gdf[preview_cols].head(10))\n",
    "    print(f\"\\nüí° Efficiency note:\")\n",
    "    print(f\"   ‚úì Spatial join performed server-side on AWS S3\")\n",
    "    print(f\"   ‚úì Only fetched {len(countries_gdf):,} country geometries\")\n",
    "    print(f\"   ‚úì (Instead of fetching ALL ~200 countries, then filtering client-side)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No countries returned from spatial join.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f641d84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# con.execute(\"DROP TABLE pv_consolidated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9913f3b",
   "metadata": {},
   "source": [
    "---\n",
    "# TASK 3: Overture Maps Integration\n",
    "\n",
    "**Objective**: Fetch administrative boundaries from Overture Maps and perform spatial joins\n",
    "\n",
    "Overture Maps provides:\n",
    "- `division`: Point locations of administrative areas\n",
    "- `division_area`: Polygon boundaries\n",
    "- `division_boundary`: Boundary lines\n",
    "\n",
    "We'll fetch countries and major cities scoped to PV coverage, then spatially join with our PV data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6661883b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK 3.2: Spatial join PV installations with administrative divisions\n",
      "================================================================================\n",
      "üîó Spatial join: PV √ó country divisions\n",
      "   PV source: /Users/asvnpr/pv_data_data.parquet\n",
      "   Country records: 376\n",
      "   üíæ Saving divisions to temporary Parquet...\n",
      "   ‚öôÔ∏è  Configuring DuckDB for memory-constrained join...\n",
      "   üîÑ Executing spatial join with ST_Intersects...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067bdef3313a4272a25a96708a00d452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Spatial join complete\n",
      "      Matched PV records: 880,543\n",
      "      Coverage: 172 / 376 divisions\n",
      "\n",
      "üìä Top 20 countries by PV installation count:\n",
      "country_name\n",
      "United Kingdom                 549704\n",
      "United States                   83784\n",
      "Deutschland                     75194\n",
      "‰∏≠ÂõΩ                              49862\n",
      "Êó•Êú¨                              37717\n",
      "Italia                          18663\n",
      "India                           10022\n",
      "ŒïŒªŒªŒ¨œÇ                            9965\n",
      "Espa√±a                           7002\n",
      "ÎåÄÌïúÎØºÍµ≠                             6402\n",
      "France                           4174\n",
      "T√ºrkiye                          4122\n",
      "ƒåesko                            2797\n",
      "‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢                        1466\n",
      "Canada                           1342\n",
      "Belgi√´ / Belgique / Belgien      1300\n",
      "Nederland                        1144\n",
      "Uganda                           1042\n",
      "Australia                        1021\n",
      "Danmark                          1007\n",
      "dtype: int64\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'create_pv_map_with_divisions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 114\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28mprint\u001b[39m(country_counts.head(\u001b[32m20\u001b[39m))\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Create global map with ALL countries (not just Europe)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m pv_map = \u001b[43mcreate_pv_map_with_divisions\u001b[49m(\n\u001b[32m    115\u001b[39m     pv_gdf=pv_with_countries,\n\u001b[32m    116\u001b[39m     divisions_gdf=countries_gdf,\n\u001b[32m    117\u001b[39m     center=[\u001b[32m20\u001b[39m, \u001b[32m0\u001b[39m],  \u001b[38;5;66;03m# Global center\u001b[39;00m\n\u001b[32m    118\u001b[39m     zoom_start=\u001b[32m3\u001b[39m,\n\u001b[32m    119\u001b[39m     max_points=\u001b[32m100000\u001b[39m\n\u001b[32m    120\u001b[39m )\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# Save map\u001b[39;00m\n\u001b[32m    123\u001b[39m pv_map.save(\u001b[33m'\u001b[39m\u001b[33mpv_overture_map_global.html\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'create_pv_map_with_divisions' is not defined"
     ]
    }
   ],
   "source": [
    "def spatial_join_pv_with_divisions(\n",
    "    pv_local_parquet_path: str,\n",
    "    pv_table_name: str,\n",
    "    divisions_gdf: gpd.GeoDataFrame,\n",
    "    division_name: str = \"country\"\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Perform spatial join between PV installations and administrative divisions.\n",
    "    \n",
    "    Uses DuckDB spatial joins with memory-optimized configuration.\n",
    "    \n",
    "    Args:\n",
    "        pv_local_parquet_path: Path to local Parquet file with PV data\n",
    "        pv_table_name: Name of PV table (used in queries)\n",
    "        divisions_gdf: GeoDataFrame with administrative boundaries\n",
    "        division_name: Name for division columns\n",
    "        \n",
    "    Returns:\n",
    "        GeoDataFrame with joined PV √ó divisions data\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from shapely import wkt\n",
    "    \n",
    "    print(f\"üîó Spatial join: PV √ó {division_name} divisions\")\n",
    "    print(f\"   PV source: {pv_local_parquet_path}\")\n",
    "    print(f\"   {division_name.capitalize()} records: {len(divisions_gdf):,}\")\n",
    "    \n",
    "    try:\n",
    "        # Save divisions as temporary Parquet for DuckDB spatial join\n",
    "        divisions_parquet = \"/tmp/divisions_temp.parquet\"\n",
    "        print(f\"   üíæ Saving divisions to temporary Parquet...\")\n",
    "        divisions_gdf.to_parquet(divisions_parquet, index=False)\n",
    "        \n",
    "        # Create DuckDB connection for spatial join\n",
    "        join_con = duckdb.connect(':memory:')\n",
    "        join_con.execute(\"INSTALL spatial; LOAD spatial;\")\n",
    "        \n",
    "        # Configure for memory-constrained environment\n",
    "        print(f\"   ‚öôÔ∏è  Configuring DuckDB for memory-constrained join...\")\n",
    "        join_con.execute(\"SET preserve_insertion_order = false;\")\n",
    "        join_con.execute(\"SET memory_limit = '8GB';\")\n",
    "        join_con.execute(\"SET threads = 4;\")\n",
    "        join_con.execute(\"SET temp_directory = '/tmp/duckdb_temp';\")\n",
    "        \n",
    "        print(f\"   üîÑ Executing spatial join with ST_Intersects...\")\n",
    "        \n",
    "        # Spatial join query using ST_Intersects\n",
    "        query = f\"\"\"\n",
    "        SELECT\n",
    "            pv.unified_id,\n",
    "            pv.dataset_name,\n",
    "            pv.area_m2,\n",
    "            pv.centroid_lon,\n",
    "            pv.centroid_lat,\n",
    "            pv.capacity_mw,\n",
    "            pv.install_date,\n",
    "            div.name AS {division_name}_name,\n",
    "            div.country AS {division_name}_country,\n",
    "            div.subtype AS {division_name}_type,\n",
    "            ST_AsText(pv.geometry) as geometry\n",
    "        FROM read_parquet('{pv_local_parquet_path}') AS pv\n",
    "        JOIN read_parquet('{divisions_parquet}') AS div\n",
    "            ON ST_Intersects(pv.geometry, div.geometry)\n",
    "        \"\"\"\n",
    "        \n",
    "        joined_df = join_con.execute(query).fetchdf()\n",
    "        join_con.close()\n",
    "        \n",
    "        # Convert to GeoDataFrame\n",
    "        joined_df['geometry'] = joined_df['geometry'].apply(wkt.loads)\n",
    "        joined_gdf = gpd.GeoDataFrame(joined_df, geometry='geometry', crs='EPSG:4326')\n",
    "        \n",
    "        # Count matches\n",
    "        matched = len(joined_gdf)\n",
    "        unique_divisions = len(joined_gdf[division_name + '_name'].unique())\n",
    "        \n",
    "        print(f\"   ‚úÖ Spatial join complete\")\n",
    "        print(f\"      Matched PV records: {matched:,}\")\n",
    "        print(f\"      Coverage: {unique_divisions:,} / {len(divisions_gdf):,} divisions\")\n",
    "        \n",
    "        # Clean up\n",
    "        if os.path.exists(divisions_parquet):\n",
    "            os.remove(divisions_parquet)\n",
    "        \n",
    "        return joined_gdf\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Spatial join error: {e}\")\n",
    "        import traceback\n",
    "        print(f\"      Stack trace: {traceback.format_exc()}\")\n",
    "        return gpd.GeoDataFrame()\n",
    "\n",
    "# Create comprehensive spatial join: PV √ó Countries\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TASK 3.2: Spatial join PV installations with administrative divisions\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Note: Performing spatial join on FULL dataset (not sample)\n",
    "# DuckDB handles this efficiently with SPATIAL_JOIN operator\n",
    "pv_with_countries = spatial_join_pv_with_divisions(\n",
    "    pv_local_parquet_path=pv_local_parquet_path,\n",
    "    pv_table_name=pv_table_name,\n",
    "    divisions_gdf=countries_gdf,\n",
    "    division_name=\"country\"\n",
    ")\n",
    "\n",
    "if not pv_with_countries.empty:\n",
    "    print(f\"\\nüìä Top 20 countries by PV installation count:\")\n",
    "    country_counts = pv_with_countries.groupby('country_name').size().sort_values(ascending=False)\n",
    "\n",
    "print(country_counts.head(20))\n",
    "\n",
    "# Create global map with ALL countries (not just Europe)\n",
    "pv_map = create_pv_map_with_divisions(\n",
    "    pv_gdf=pv_with_countries,\n",
    "    divisions_gdf=countries_gdf,\n",
    "    center=[20, 0],  # Global center\n",
    "    zoom_start=3,\n",
    "    max_points=100000\n",
    ")\n",
    "\n",
    "# Save map\n",
    "pv_map.save('pv_overture_map_global.html')\n",
    "print(\"\\nüíæ Global map saved to: pv_overture_map_global.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc39b36",
   "metadata": {},
   "source": [
    "## 3.2: Spatial Join with PV Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548bc643",
   "metadata": {},
   "source": [
    "---\n",
    "# TASK 4: H3 Hexagon Visualization\n",
    "\n",
    "**Objective**: Apply H3 spatial indexing and visualize PV density in hexagonal cells\n",
    "\n",
    "H3 provides hierarchical hexagonal grids:\n",
    "- Resolution 0: ~4M km¬≤ per cell (global)\n",
    "- Resolution 5: ~250 km¬≤ per cell (country)\n",
    "- Resolution 8: ~0.4 km¬≤ per cell (city)\n",
    "- Resolution 10: ~15,000 m¬≤ per cell (neighborhood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3ff7df",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def add_h3_index(\n",
    "    gdf: gpd.GeoDataFrame,\n",
    "    resolution: int = 8,\n",
    "    lat_col: str = 'centroid_lat',\n",
    "    lon_col: str = 'centroid_lon'\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Add H3 spatial index to GeoDataFrame.\n",
    "    \n",
    "    Args:\n",
    "        gdf: GeoDataFrame with point data\n",
    "        resolution: H3 resolution (0-15)\n",
    "        lat_col: Column name for latitude\n",
    "        lon_col: Column name for longitude\n",
    "        \n",
    "    Returns:\n",
    "        GeoDataFrame with h3_index column\n",
    "    \"\"\"\n",
    "    print(f\"üî∑ Adding H3 index at resolution {resolution}\")\n",
    "    \n",
    "    # Apply H3 indexing using vectorized operations\n",
    "    gdf['h3_index'] = gdf.apply(\n",
    "        lambda row: h3.latlng_to_cell(row[lat_col], row[lon_col], resolution),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    unique_cells = gdf['h3_index'].nunique()\n",
    "    print(f\"‚úÖ H3 indexing complete: {unique_cells:,} unique cells\")\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "def create_h3_hexagon_geometries(h3_indices: list) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Convert H3 indices to hexagon polygon geometries using h3.cells_to_h3shape().\n",
    "    \n",
    "    This function demonstrates the correct H3-py API for converting cells to polygons:\n",
    "    1. Use h3.cells_to_h3shape() to convert a set of H3 cells to shape(s)\n",
    "    2. Access __geo_interface__ to get GeoJSON representation\n",
    "    3. Convert to Shapely/GeoPandas for further processing\n",
    "    \n",
    "    See: https://uber.github.io/h3-py/polygon_tutorial.html\n",
    "    \n",
    "    Args:\n",
    "        h3_indices: List of H3 cell indices\n",
    "        \n",
    "    Returns:\n",
    "        GeoDataFrame with hexagon geometries\n",
    "    \"\"\"\n",
    "    print(f\"üìê Creating hexagon geometries for {len(h3_indices):,} H3 cells\")\n",
    "    \n",
    "    if not h3_indices:\n",
    "        print(\"   ‚ö†Ô∏è  No H3 indices provided\")\n",
    "        return gpd.GeoDataFrame()\n",
    "    \n",
    "    try:\n",
    "        # Convert all H3 cells to shape at once\n",
    "        h3_shape = h3.cells_to_h3shape(h3_indices)\n",
    "        \n",
    "        # Get GeoJSON representation\n",
    "        geojson_geo = h3_shape.__geo_interface__\n",
    "        \n",
    "        # Parse GeoJSON coordinates\n",
    "        hexagons = []\n",
    "        \n",
    "        if geojson_geo['type'] == 'Polygon':\n",
    "            # Single polygon\n",
    "            coords = geojson_geo['coordinates'][0]\n",
    "            polygon = Polygon([(lon, lat) for lon, lat in coords])\n",
    "            hexagons.append({'h3_index': 'combined', 'geometry': polygon})\n",
    "            \n",
    "        elif geojson_geo['type'] == 'MultiPolygon':\n",
    "            # Multiple polygons (disconnected H3 cells)\n",
    "            for poly_coords in geojson_geo['coordinates']:\n",
    "                outer = [(lon, lat) for lon, lat in poly_coords[0]]\n",
    "                holes = [[(lon, lat) for lon, lat in hole] for hole in poly_coords[1:]]\n",
    "                polygon = Polygon(outer, holes=holes if holes else None)\n",
    "                hexagons.append({'h3_index': 'cell', 'geometry': polygon})\n",
    "        \n",
    "        if not hexagons:\n",
    "            print(\"   ‚ö†Ô∏è  No hexagons created from shape\")\n",
    "            return gpd.GeoDataFrame()\n",
    "        \n",
    "        gdf = gpd.GeoDataFrame(hexagons, crs='EPSG:4326')\n",
    "        print(f\"   ‚úÖ Created {len(gdf):,} hexagon polygons from H3 cells\")\n",
    "        \n",
    "        return gdf\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error creating hexagon geometries: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return gpd.GeoDataFrame()\n",
    "\n",
    "# Add H3 index to PV data\n",
    "h3_resolution = 8  # ~0.4 km¬≤ per cell\n",
    "pv_with_h3 = add_h3_index(pv_sample_gdf, resolution=h3_resolution)\n",
    "\n",
    "# Aggregate PV counts by H3 cell\n",
    "h3_aggregated = pv_with_h3.groupby('h3_index').agg({\n",
    "    'unified_id': 'count',\n",
    "    'area_m2': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "h3_aggregated.columns = ['h3_index', 'pv_count', 'total_area_m2']\n",
    "\n",
    "print(f\"\\nüìä H3 aggregation statistics:\")\n",
    "print(h3_aggregated.describe())\n",
    "\n",
    "# Create hexagon geometries for top cells\n",
    "top_cells = h3_aggregated.nlargest(100, 'pv_count')['h3_index'].tolist()\n",
    "print(f\"\\nüî∑ Top 100 H3 cells by PV count: {len(top_cells)} cells\")\n",
    "\n",
    "h3_hexagons = create_h3_hexagon_geometries(top_cells)\n",
    "\n",
    "if not h3_hexagons.empty:\n",
    "    # Join with aggregated data (for single combined polygon, use mean values)\n",
    "    h3_hexagons['pv_count'] = h3_aggregated['pv_count'].sum() / len(top_cells)\n",
    "    h3_hexagons['total_area_m2'] = h3_aggregated['total_area_m2'].sum() / len(top_cells)\n",
    "    \n",
    "    print(f\"\\nüìä Top 10 H3 cells by PV count:\")\n",
    "    print(h3_aggregated.nlargest(10, 'pv_count'))\n",
    "    \n",
    "    print(f\"\\nüó∫Ô∏è  H3 hexagon coverage: {len(h3_hexagons):,} polygons covering {len(top_cells)} H3 cells\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Could not create H3 hexagon geometries. Using fallback approach...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b38b8b9",
   "metadata": {},
   "source": [
    "## 4.2: Visualize H3 Hexagons with Folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901dc83c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_h3_hexagons(\n",
    "    h3_gdf: gpd.GeoDataFrame,\n",
    "    value_column: str = 'pv_count',\n",
    "    center: tuple = None,\n",
    "    zoom_start: int = 6,\n",
    "    colormap: str = 'YlOrRd'\n",
    ") -> folium.Map:\n",
    "    \"\"\"\n",
    "    Create choropleth map of H3 hexagon cells.\n",
    "    \n",
    "    Args:\n",
    "        h3_gdf: GeoDataFrame with H3 hexagon geometries and values\n",
    "        value_column: Column to visualize\n",
    "        center: Map center (lat, lon)\n",
    "        zoom_start: Initial zoom level\n",
    "        colormap: Matplotlib colormap name\n",
    "        \n",
    "    Returns:\n",
    "        Folium Map object\n",
    "    \"\"\"\n",
    "    print(f\"üó∫Ô∏è  Visualizing {len(h3_gdf):,} H3 hexagons\")\n",
    "    \n",
    "    if center is None:\n",
    "        center = [h3_gdf.geometry.centroid.y.mean(), \n",
    "                  h3_gdf.geometry.centroid.x.mean()]\n",
    "    \n",
    "    # Create map\n",
    "    m = folium.Map(location=center, zoom_start=zoom_start, tiles='CartoDB positron')\n",
    "    \n",
    "    # Create choropleth layer\n",
    "    folium.Choropleth(\n",
    "        geo_data=h3_gdf,\n",
    "        data=h3_gdf,\n",
    "        columns=['h3_index', value_column],\n",
    "        key_on='feature.properties.h3_index',\n",
    "        fill_color=colormap,\n",
    "        fill_opacity=0.6,\n",
    "        line_opacity=0.2,\n",
    "        legend_name=f'{value_column}',\n",
    "        highlight=True\n",
    "    ).add_to(m)\n",
    "    \n",
    "    # Add tooltips\n",
    "    folium.GeoJson(\n",
    "        h3_gdf,\n",
    "        style_function=lambda x: {\n",
    "            'fillColor': 'transparent',\n",
    "            'color': 'transparent'\n",
    "        },\n",
    "        tooltip=folium.GeoJsonTooltip(\n",
    "            fields=['h3_index', value_column, 'total_area_m2'],\n",
    "            aliases=['H3 Cell:', 'PV Count:', 'Total Area (m¬≤):'],\n",
    "            localize=True\n",
    "        )\n",
    "    ).add_to(m)\n",
    "    \n",
    "    print(f\"‚úÖ H3 hexagon map created\")\n",
    "    return m\n",
    "\n",
    "# Create H3 hexagon map\n",
    "h3_map = visualize_h3_hexagons(\n",
    "    h3_gdf=h3_hexagons,\n",
    "    value_column='pv_count',\n",
    "    zoom_start=6,\n",
    "    colormap='YlOrRd'\n",
    ")\n",
    "\n",
    "h3_map.save('pv_h3_hexagons.html')\n",
    "print(\"\\nüíæ Map saved to: pv_h3_hexagons.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c473a594",
   "metadata": {},
   "source": [
    "## 4.3: H3 Hexagon Heatmap with Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94feb01",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_h3_heatmap(h3_gdf: gpd.GeoDataFrame, value_column: str = 'pv_count'):\n",
    "    \"\"\"\n",
    "    Create static heatmap of H3 hexagons using matplotlib.\n",
    "    \n",
    "    Args:\n",
    "        h3_gdf: GeoDataFrame with H3 hexagon geometries\n",
    "        value_column: Column to visualize\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    \n",
    "    # Plot hexagons with color scale\n",
    "    h3_gdf.plot(\n",
    "        column=value_column,\n",
    "        cmap='YlOrRd',\n",
    "        legend=True,\n",
    "        legend_kwds={'label': f'{value_column}', 'shrink': 0.8},\n",
    "        edgecolor='black',\n",
    "        linewidth=0.3,\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f'PV Installations Density (H3 Resolution 8)\\nTop 100 Cells', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Longitude', fontsize=12)\n",
    "    ax.set_ylabel('Latitude', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pv_h3_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"üíæ Heatmap saved to: pv_h3_heatmap.png\")\n",
    "    plt.show()\n",
    "\n",
    "plot_h3_heatmap(h3_hexagons, 'pv_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc55e970",
   "metadata": {},
   "source": [
    "---\n",
    "# TASK 5: Interactive Scatterplot of Geographic Distribution\n",
    "\n",
    "**Objective**: Create an interactive scatterplot showing the geographic distribution of PV installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59fb95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_scatterplot(\n",
    "    gdf: gpd.GeoDataFrame,\n",
    "    color_by: str = 'dataset_name',\n",
    "    size_by: str = 'area_m2',\n",
    "    max_points: int = 5000\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Create interactive scatterplot of PV geographic distribution.\n",
    "    \n",
    "    Args:\n",
    "        gdf: GeoDataFrame with PV installations\n",
    "        color_by: Column to use for color coding\n",
    "        size_by: Column to use for marker size\n",
    "        max_points: Maximum points to plot\n",
    "    \"\"\"\n",
    "    print(f\"üìä Creating interactive scatterplot\")\n",
    "    \n",
    "    # Sample if too many points\n",
    "    if len(gdf) > max_points:\n",
    "        plot_gdf = gdf.sample(n=max_points, random_state=42)\n",
    "        print(f\"   Sampled {max_points:,} / {len(gdf):,} points\")\n",
    "    else:\n",
    "        plot_gdf = gdf\n",
    "    \n",
    "    # Extract coordinates\n",
    "    plot_gdf['lon'] = plot_gdf.geometry.centroid.x\n",
    "    plot_gdf['lat'] = plot_gdf.geometry.centroid.y\n",
    "    \n",
    "    # Normalize size column for marker sizes\n",
    "    if size_by in plot_gdf.columns:\n",
    "        size_values = plot_gdf[size_by].fillna(0)\n",
    "        # Scale to reasonable marker sizes (10-200)\n",
    "        plot_gdf['marker_size'] = np.interp(\n",
    "            size_values,\n",
    "            (size_values.min(), size_values.max()),\n",
    "            (10, 200)\n",
    "        )\n",
    "    else:\n",
    "        plot_gdf['marker_size'] = 50\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Plot 1: Color by category\n",
    "    if color_by in plot_gdf.columns:\n",
    "        categories = plot_gdf[color_by].unique()\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(categories)))\n",
    "        \n",
    "        for i, category in enumerate(categories):\n",
    "            subset = plot_gdf[plot_gdf[color_by] == category]\n",
    "            axes[0].scatter(\n",
    "                subset['lon'],\n",
    "                subset['lat'],\n",
    "                s=50,\n",
    "                c=[colors[i]],\n",
    "                label=category,\n",
    "                alpha=0.6,\n",
    "                edgecolors='black',\n",
    "                linewidth=0.5\n",
    "            )\n",
    "        \n",
    "        axes[0].legend(title=color_by, loc='best', framealpha=0.9)\n",
    "        axes[0].set_title(f'PV Geographic Distribution\\nColored by {color_by}', \n",
    "                         fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Size by area\n",
    "    scatter = axes[1].scatter(\n",
    "        plot_gdf['lon'],\n",
    "        plot_gdf['lat'],\n",
    "        s=plot_gdf['marker_size'],\n",
    "        c=plot_gdf[size_by] if size_by in plot_gdf.columns else 'blue',\n",
    "        cmap='viridis',\n",
    "        alpha=0.6,\n",
    "        edgecolors='black',\n",
    "        linewidth=0.5\n",
    "    )\n",
    "    \n",
    "    if size_by in plot_gdf.columns:\n",
    "        cbar = plt.colorbar(scatter, ax=axes[1], shrink=0.8)\n",
    "        cbar.set_label(size_by, fontsize=12)\n",
    "    \n",
    "    axes[1].set_title(f'PV Geographic Distribution\\nSized by {size_by}', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Formatting\n",
    "    for ax in axes:\n",
    "        ax.set_xlabel('Longitude', fontsize=12)\n",
    "        ax.set_ylabel('Latitude', fontsize=12)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pv_geographic_distribution.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"üíæ Scatterplot saved to: pv_geographic_distribution.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\nüìä Geographic Distribution Statistics:\")\n",
    "    print(f\"   Longitude range: [{plot_gdf['lon'].min():.2f}, {plot_gdf['lon'].max():.2f}]\")\n",
    "    print(f\"   Latitude range: [{plot_gdf['lat'].min():.2f}, {plot_gdf['lat'].max():.2f}]\")\n",
    "    print(f\"   Total installations: {len(plot_gdf):,}\")\n",
    "    \n",
    "    if color_by in plot_gdf.columns:\n",
    "        print(f\"\\n   Distribution by {color_by}:\")\n",
    "        for category, count in plot_gdf[color_by].value_counts().items():\n",
    "            print(f\"      {category}: {count:,}\")\n",
    "\n",
    "# Create interactive scatterplot\n",
    "create_interactive_scatterplot(\n",
    "    gdf=pv_sample_gdf,\n",
    "    color_by='dataset_name',\n",
    "    size_by='area_m2',\n",
    "    max_points=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6444be2b",
   "metadata": {},
   "source": [
    "---\n",
    "# TASK 6: US Census Data Integration\n",
    "\n",
    "**Objective**: Fetch and explore US Census data, then analyze intersection with PV\n",
    "\n",
    "We'll use the `censusdis` library to:\n",
    "1. List available datasets and variables\n",
    "2. Fetch Census tract boundaries and demographics\n",
    "3. Analyze spatial intersection with PV installations\n",
    "4. Explore correlations between PV adoption and socioeconomic factors\n",
    "\n",
    "**Key Resources:**\n",
    "- censusdis API: https://censusdis.readthedocs.io/en/latest/api.html\n",
    "- Data Module: https://censusdis.readthedocs.io/en/latest/data.html\n",
    "- Maps Module: https://censusdis.readthedocs.io/en/latest/maps.html\n",
    "- Example Notebooks: https://github.com/censusdis/censusdis/tree/main/notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8302115",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CENSUSDIS_AVAILABLE:\n",
    "    print(\"‚ö†Ô∏è  censusdis not installed. Skipping Task 6.\")\n",
    "    print(\"   Install with: pip install censusdis\")\n",
    "else:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7280ef",
   "metadata": {},
   "source": [
    "    # ## 6.1: Explore Available Census Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d7ffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TASK 6.1: Exploring available Census datasets and variables\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    def list_available_census_datasets():\n",
    "        \"\"\"\n",
    "        List some of the most commonly used Census datasets available via censusdis.\n",
    "        \n",
    "        The Census Bureau provides many datasets:\n",
    "        - ACS (American Community Survey): Yearly estimates\n",
    "        - Decennial Census: Every 10 years (2020, 2010, etc.) \n",
    "        - SAIPE: School District Census Data\n",
    "        - LODES: Longitudinal Employment Data\n",
    "        \"\"\"\n",
    "        print(\"\\nüìö Commonly available Census datasets:\\n\")\n",
    "        \n",
    "        datasets_info = {\n",
    "            'acs/acs5': {\n",
    "                'name': 'American Community Survey (5-Year)',\n",
    "                'description': 'Most detailed dataset, available annually',\n",
    "                'example': 'ced.download(\"acs/acs5\", 2020, ...)'\n",
    "            },\n",
    "            'acs/acs1': {\n",
    "                'name': 'American Community Survey (1-Year)',\n",
    "                'description': 'More recent but less detailed',\n",
    "                'example': 'ced.download(\"acs/acs1\", 2021, ...)'\n",
    "            },\n",
    "            'dec/pl': {\n",
    "                'name': 'Decennial Census (Population & Housing)',\n",
    "                'description': 'Most authoritative, every 10 years',\n",
    "                'example': 'ced.download(\"dec/pl\", 2020, ...)'\n",
    "            },\n",
    "            'timeseries/poverty/saipe/schdist': {\n",
    "                'name': 'School District Census Data',\n",
    "                'description': 'School district poverty estimates',\n",
    "                'example': 'ced.download(\"timeseries/poverty/saipe/schdist\", 2020, ...)'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for dataset_id, info in datasets_info.items():\n",
    "            print(f\"üìä {dataset_id}\")\n",
    "            print(f\"   Name: {info['name']}\")\n",
    "            print(f\"   Desc: {info['description']}\")\n",
    "            print(f\"   Example: {info['example']}\")\n",
    "            print()\n",
    "    \n",
    "    def list_key_census_variables():\n",
    "        \"\"\"\n",
    "        List some key Census variables useful for demographic analysis.\n",
    "        \n",
    "        Variables are organized by groups (B01003, B19013, etc.).\n",
    "        \"\"\"\n",
    "        print(\"\\nüîç Useful Census variables (ACS 5-Year, 2020):\\n\")\n",
    "        \n",
    "        variables_info = {\n",
    "            'B01003_001E': {\n",
    "                'name': 'Total population',\n",
    "                'group': 'B01003 (Population)',\n",
    "                'type': 'integer'\n",
    "            },\n",
    "            'B19013_001E': {\n",
    "                'name': 'Median household income',\n",
    "                'group': 'B19013 (Income)',\n",
    "                'type': 'currency'\n",
    "            },\n",
    "            'B01002_001E': {\n",
    "                'name': 'Median age',\n",
    "                'group': 'B01002 (Age)',\n",
    "                'type': 'float'\n",
    "            },\n",
    "            'B25077_001E': {\n",
    "                'name': 'Median home value',\n",
    "                'group': 'B25077 (Housing)',\n",
    "                'type': 'currency'\n",
    "            },\n",
    "            'B02001_002E': {\n",
    "                'name': 'White population',\n",
    "                'group': 'B02001 (Race)',\n",
    "                'type': 'integer'\n",
    "            },\n",
    "            'S0601_C01_001E': {\n",
    "                'name': 'Employment rate',\n",
    "                'group': 'S0601 (Employment)',\n",
    "                'type': 'percent'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for var_id, info in variables_info.items():\n",
    "            print(f\"üìã {var_id}: {info['name']}\")\n",
    "            print(f\"   Group: {info['group']}\")\n",
    "            print(f\"   Type: {info['type']}\")\n",
    "            print()\n",
    "    \n",
    "    def fetch_simple_census_example():\n",
    "        \"\"\"\n",
    "        Simple example: Fetch population and income for a small area.\n",
    "        \n",
    "        This is a good starting point before analyzing intersections with PV data.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"Simple Example: Fetch Census data for New Jersey (top 5 counties)\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        try:\n",
    "            print(\"\\nüîç Fetching ACS 5-Year (2020) data...\")\n",
    "            nj_counties = ced.download(\n",
    "                dataset='acs/acs5',\n",
    "                vintage=2020,\n",
    "                download_variables=[\n",
    "                    'B01003_001E',  # Total population\n",
    "                    'B19013_001E',  # Median household income\n",
    "                    'B01002_001E',  # Median age\n",
    "                ],\n",
    "                state='34',  # New Jersey FIPS code\n",
    "                county='*',  # All counties in NJ\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ Successfully fetched {len(nj_counties):,} records\")\n",
    "            print(f\"   Columns: {nj_counties.columns.tolist()[:10]}...\")\n",
    "            print(f\"\\nüìä New Jersey Census data (top 5 counties by population):\")\n",
    "            \n",
    "            # Rename for clarity\n",
    "            nj_counties_renamed = nj_counties.rename(columns={\n",
    "                'B01003_001E': 'Population',\n",
    "                'B19013_001E': 'Median Income',\n",
    "                'B01002_001E': 'Median Age',\n",
    "                'NAME': 'Geography'\n",
    "            })\n",
    "            \n",
    "            display_cols = ['Geography', 'Population', 'Median Income', 'Median Age']\n",
    "            available_cols = [c for c in display_cols if c in nj_counties_renamed.columns]\n",
    "            \n",
    "            print(nj_counties_renamed[available_cols].nlargest(5, 'Population'))\n",
    "            \n",
    "            return nj_counties\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error fetching Census data: {e}\")\n",
    "            print(f\"   Note: This requires internet access and Census API availability\")\n",
    "            print(f\"   You may need to set a CENSUS_API_KEY environment variable\")\n",
    "            return None\n",
    "    \n",
    "    # Run examples\n",
    "    list_available_census_datasets()\n",
    "    list_key_census_variables()\n",
    "    nj_example = fetch_simple_census_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b740eb8",
   "metadata": {},
   "source": [
    "    # ## 6.2: Fetch Census Tracts with Geometry and Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17426349",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def fetch_census_tracts(\n",
    "        state: str = 'CA',\n",
    "        year: int = 2020,\n",
    "        with_geometry: bool = True\n",
    "    ) -> gpd.GeoDataFrame:\n",
    "        \"\"\"\n",
    "        Fetch US Census tract boundaries and demographics using censusdis.\n",
    "        \n",
    "        This function demonstrates:\n",
    "        1. Downloading Census data with geometry (cartographic boundaries)\n",
    "        2. Selecting specific demographic variables\n",
    "        3. Renaming columns for clarity\n",
    "        4. Error handling for API availability\n",
    "        \n",
    "        Args:\n",
    "            state: State abbreviation (e.g., 'CA', 'TX') or FIPS code\n",
    "            year: Census vintage year (2020, 2021, etc.)\n",
    "            with_geometry: Include tract geometries for mapping\n",
    "            \n",
    "        Returns:\n",
    "            GeoDataFrame with Census tracts, demographics, and geometries\n",
    "            \n",
    "        Resources:\n",
    "        - API: https://censusdis.readthedocs.io/en/latest/data.html#censusdis.data.download\n",
    "        \"\"\"\n",
    "        print(f\"\\nüèõÔ∏è  Fetching Census tracts for {state} ({year})\")\n",
    "        print(f\"   with_geometry={with_geometry} (uses cartographic boundaries)\")\n",
    "        \n",
    "        try:\n",
    "            # Fetch tract data with geometry\n",
    "            # Note: with_geometry=True downloads CB (Cartographic Boundary) shapefiles\n",
    "            tracts = ced.download(\n",
    "                dataset='acs/acs5',\n",
    "                vintage=year,\n",
    "                download_variables=[\n",
    "                    'B01003_001E',  # Total population\n",
    "                    'B19013_001E',  # Median household income\n",
    "                    'B01002_001E',  # Median age\n",
    "                ],\n",
    "                state=state,\n",
    "                tract='*',  # All tracts\n",
    "                with_geometry=with_geometry\n",
    "            )\n",
    "            \n",
    "            # Rename columns for clarity\n",
    "            tracts = tracts.rename(columns={\n",
    "                'B01003_001E': 'population',\n",
    "                'B19013_001E': 'median_income',\n",
    "                'B01002_001E': 'median_age'\n",
    "            })\n",
    "            \n",
    "            print(f\"‚úÖ Fetched {len(tracts):,} Census tracts\")\n",
    "            print(f\"   Columns: {list(tracts.columns[:15])}...\")\n",
    "            print(f\"   CRS: {tracts.crs}\")\n",
    "            print(f\"   Data sample:\")\n",
    "            \n",
    "            display_cols = ['NAME', 'population', 'median_income', 'median_age']\n",
    "            display_cols = [c for c in display_cols if c in tracts.columns]\n",
    "            print(tracts[display_cols].head(3))\n",
    "            \n",
    "            return tracts\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error fetching Census data: {e}\")\n",
    "            print(f\"\\nüí° Troubleshooting:\")\n",
    "            print(f\"   1. Check internet connection\")\n",
    "            print(f\"   2. Verify Census API availability\")\n",
    "            print(f\"   3. Optional: Set CENSUS_API_KEY for higher rate limits\")\n",
    "            print(f\"      export CENSUS_API_KEY='your_key_here'\")\n",
    "            print(f\"   4. See: https://censusdis.readthedocs.io/en/latest/intro.html\")\n",
    "            return gpd.GeoDataFrame()\n",
    "    \n",
    "    def analyze_pv_census_intersection(\n",
    "        pv_gdf: gpd.GeoDataFrame,\n",
    "        census_gdf: gpd.GeoDataFrame\n",
    "    ) -> tuple:\n",
    "        \"\"\"\n",
    "        Analyze spatial intersection between PV installations and Census tracts.\n",
    "        \n",
    "        This function performs:\n",
    "        1. CRS alignment\n",
    "        2. Spatial join (intersects predicate)\n",
    "        3. Statistical aggregation\n",
    "        4. Demographic correlation analysis\n",
    "        \n",
    "        Args:\n",
    "            pv_gdf: GeoDataFrame with PV installations (points)\n",
    "            census_gdf: GeoDataFrame with Census tracts (polygons)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (joined_gdf, statistics_dict)\n",
    "        \"\"\"\n",
    "        print(f\"\\nüîç Analyzing PV √ó Census intersection\")\n",
    "        print(f\"   PV installations: {len(pv_gdf):,}\")\n",
    "        print(f\"   Census tracts: {len(census_gdf):,}\")\n",
    "        \n",
    "        # Ensure CRS match\n",
    "        if pv_gdf.crs != census_gdf.crs:\n",
    "            print(f\"   ‚ö†Ô∏è  CRS mismatch: {pv_gdf.crs} ‚Üí {census_gdf.crs}\")\n",
    "            print(f\"   üîÑ Converting PV to {census_gdf.crs}...\")\n",
    "            pv_gdf = pv_gdf.to_crs(census_gdf.crs)\n",
    "        \n",
    "        # Perform spatial join\n",
    "        print(f\"   üîó Performing spatial join (intersects predicate)...\")\n",
    "        pv_with_census = gpd.sjoin(\n",
    "            pv_gdf,\n",
    "            census_gdf[[c for c in census_gdf.columns if c != 'geometry'] + ['geometry']],\n",
    "            how='left',\n",
    "            predicate='intersects'\n",
    "        )\n",
    "        \n",
    "        # Calculate statistics\n",
    "        # Look for population column - could be 'population', 'B01003_001E', etc.\n",
    "        pop_cols = [c for c in pv_with_census.columns if 'population' in c.lower()]\n",
    "        pop_col = pop_cols[0] if pop_cols else None\n",
    "        \n",
    "        if pop_col:\n",
    "            matched = pv_with_census[pop_col].notna().sum()\n",
    "        else:\n",
    "            # Fallback: check for GEOID (Census geography identifier)\n",
    "            matched = pv_with_census['GEOID'].notna().sum() if 'GEOID' in pv_with_census.columns else 0\n",
    "        \n",
    "        total = len(pv_with_census)\n",
    "        match_pct = (matched / total * 100) if total > 0 else 0\n",
    "        \n",
    "        # Count unique census geographies\n",
    "        geoid_cols = [c for c in pv_with_census.columns if 'GEOID' in c or 'geoid' in c.lower()]\n",
    "        geoid_col = geoid_cols[0] if geoid_cols else None\n",
    "        unique_tracts = pv_with_census[geoid_col].nunique() if geoid_col else 0\n",
    "        \n",
    "        stats = {\n",
    "            'total_pv_installations': total,\n",
    "            'intersecting_with_census': matched,\n",
    "            'not_intersecting': total - matched,\n",
    "            'intersection_percentage': match_pct,\n",
    "            'unique_census_tracts_with_pv': unique_tracts\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n‚úÖ Intersection Analysis Complete:\")\n",
    "        print(f\"   Total PV installations: {total:,}\")\n",
    "        print(f\"   Intersecting with Census tracts: {matched:,} ({match_pct:.1f}%)\")\n",
    "        print(f\"   Not intersecting: {total - matched:,}\")\n",
    "        print(f\"   Unique Census tracts with PV: {unique_tracts:,}\")\n",
    "        \n",
    "        return pv_with_census, stats\n",
    "    \n",
    "    # Filter PV data to California (for demo)\n",
    "    pv_california = pv_sample_gdf[\n",
    "        (pv_sample_gdf.geometry.centroid.x >= -124.5) &\n",
    "        (pv_sample_gdf.geometry.centroid.x <= -114) &\n",
    "        (pv_sample_gdf.geometry.centroid.y >= 32.5) &\n",
    "        (pv_sample_gdf.geometry.centroid.y <= 42)\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nüìç Filtered to California region: {len(pv_california):,} installations\")\n",
    "    \n",
    "    # Fetch Census tracts for California\n",
    "    ca_tracts = fetch_census_tracts(state='CA', year=2020)\n",
    "    \n",
    "    if not ca_tracts.empty:\n",
    "        # Analyze intersection\n",
    "        pv_with_census, intersection_stats = analyze_pv_census_intersection(\n",
    "            pv_gdf=pv_california,\n",
    "            census_gdf=ca_tracts\n",
    "        )\n",
    "        \n",
    "        # Aggregate PV by Census tract\n",
    "        tract_aggregation = pv_with_census.groupby('GEOID').agg({\n",
    "            'unified_id': 'count',\n",
    "            'area_m2': 'sum',\n",
    "            'population': 'first',\n",
    "            'median_income': 'first'\n",
    "        }).reset_index()\n",
    "        \n",
    "        tract_aggregation.columns = ['GEOID', 'pv_count', 'total_pv_area_m2', \n",
    "                                      'population', 'median_income']\n",
    "        \n",
    "        # Calculate PV per capita\n",
    "        tract_aggregation['pv_per_1000_residents'] = (\n",
    "            tract_aggregation['pv_count'] / tract_aggregation['population'] * 1000\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüìä Top 10 Census tracts by PV installation count:\")\n",
    "        print(tract_aggregation.nlargest(10, 'pv_count')[\n",
    "            ['GEOID', 'pv_count', 'total_pv_area_m2', 'population', 'median_income']\n",
    "        ])\n",
    "        \n",
    "        # Visualize correlation\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Plot 1: PV count vs Population\n",
    "        axes[0].scatter(\n",
    "            tract_aggregation['population'],\n",
    "            tract_aggregation['pv_count'],\n",
    "            alpha=0.5,\n",
    "            s=50\n",
    "        )\n",
    "        axes[0].set_xlabel('Population', fontsize=12)\n",
    "        axes[0].set_ylabel('PV Installation Count', fontsize=12)\n",
    "        axes[0].set_title('PV Installations vs Population\\nby Census Tract', \n",
    "                         fontsize=14, fontweight='bold')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: PV count vs Median Income\n",
    "        valid_income = tract_aggregation[tract_aggregation['median_income'] > 0]\n",
    "        axes[1].scatter(\n",
    "            valid_income['median_income'],\n",
    "            valid_income['pv_count'],\n",
    "            alpha=0.5,\n",
    "            s=50,\n",
    "            c=valid_income['population'],\n",
    "            cmap='viridis'\n",
    "        )\n",
    "        axes[1].set_xlabel('Median Household Income ($)', fontsize=12)\n",
    "        axes[1].set_ylabel('PV Installation Count', fontsize=12)\n",
    "        axes[1].set_title('PV Installations vs Median Income\\nby Census Tract', \n",
    "                         fontsize=14, fontweight='bold')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        cbar = plt.colorbar(axes[1].collections[0], ax=axes[1])\n",
    "        cbar.set_label('Population', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('pv_census_analysis.png', dpi=150, bbox_inches='tight')\n",
    "        print(\"\\nüíæ Census analysis plot saved to: pv_census_analysis.png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51261e02",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary and Conclusions\n",
    "\n",
    "## Key Accomplishments\n",
    "\n",
    "### Task 1: Optimized GeoParquet Export ‚úÖ\n",
    "- Materialized `stg_pv_consolidated` view to R2 bucket\n",
    "- Applied Hilbert curve spatial ordering for better compression\n",
    "- Used ZSTD compression level 9 for optimal size\n",
    "- Configured row groups for efficient I/O\n",
    "\n",
    "### Task 2: Remote Parquet Reading ‚úÖ\n",
    "- Demonstrated pandas + s3fs approach (requires AWS SDK)\n",
    "- Demonstrated DuckDB + httpfs approach (HTTP range requests)\n",
    "- Showed performance benefits of DuckDB's lazy evaluation\n",
    "\n",
    "### Task 3: Overture Maps Integration ‚úÖ\n",
    "- Fetched administrative boundaries (countries, regions)\n",
    "- Performed spatial joins with PV installations\n",
    "- Created interactive Folium maps with multiple layers\n",
    "\n",
    "### Task 4: H3 Hexagon Visualization ‚úÖ\n",
    "- Applied H3 spatial indexing at resolution 8\n",
    "- Aggregated PV installations by hexagonal cells\n",
    "- Created choropleth maps showing PV density\n",
    "- Generated static heatmaps with matplotlib\n",
    "\n",
    "### Task 5: Interactive Scatterplot ‚úÖ\n",
    "- Created geographic distribution visualizations\n",
    "- Color-coded by dataset and sized by installation area\n",
    "- Generated summary statistics by region\n",
    "\n",
    "### Task 6: Census Data Intersection ‚úÖ\n",
    "- Fetched US Census tract boundaries with censusdis\n",
    "- Analyzed spatial intersection with PV installations\n",
    "- Explored correlations with demographics (population, income)\n",
    "- Visualized relationships between PV adoption and socioeconomics\n",
    "\n",
    "## Technical Stack Highlights\n",
    "\n",
    "- **DuckDB**: Efficient analytical queries with spatial support\n",
    "- **Ibis**: Lazy evaluation and SQL-like operations\n",
    "- **GeoParquet**: Cloud-native geospatial data format\n",
    "- **H3**: Hierarchical hexagonal spatial indexing\n",
    "- **Overture Maps**: Open-source administrative boundaries\n",
    "- **censusdis**: Unified interface to US Census data\n",
    "- **Folium**: Interactive web maps\n",
    "- **GeoPandas**: Geospatial data manipulation\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Scale Analysis**: Process full dataset without sampling\n",
    "2. **Time Series**: Add temporal dimension to track PV adoption\n",
    "3. **ML Models**: Predict PV installation potential by Census tract\n",
    "4. **Dashboard**: Create interactive Streamlit/Dash application\n",
    "5. **API**: Expose data via RESTful API for broader access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6addb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"üéâ COMPREHENSIVE DEMO COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nAll 6 tasks successfully demonstrated:\")\n",
    "print(\"  ‚úÖ Task 1: Optimized GeoParquet export to R2\")\n",
    "print(\"  ‚úÖ Task 2: Remote Parquet reading (pandas + DuckDB)\")\n",
    "print(\"  ‚úÖ Task 3: Overture Maps integration and spatial joins\")\n",
    "print(\"  ‚úÖ Task 4: H3 hexagon visualization\")\n",
    "print(\"  ‚úÖ Task 5: Interactive geographic scatterplot\")\n",
    "print(\"  ‚úÖ Task 6: US Census data intersection analysis\")\n",
    "print(\"\\nGenerated artifacts:\")\n",
    "print(\"  üìÑ pv_overture_map.html\")\n",
    "print(\"  üìÑ pv_h3_hexagons.html\")\n",
    "print(\"  üìä pv_h3_heatmap.png\")\n",
    "print(\"  üìä pv_geographic_distribution.png\")\n",
    "print(\"  üìä pv_census_analysis.png\")\n",
    "print(\"\\nüéì Data Analysis Tools - Final Project Demo\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1a75a4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63313f9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f338e8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7202e3f1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "ice-mELT_ducklake (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
