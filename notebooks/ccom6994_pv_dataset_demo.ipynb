{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fac214d",
   "metadata": {},
   "source": [
    "# CCOM 6994: Solar Panel Dataset Analysis - Comprehensive Demo\n",
    "\n",
    "**Data Analysis Tools - Final Project**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Project Overview\n",
    "\n",
    "This notebook demonstrates **advanced geospatial data analysis techniques** applied to a global solar panel (PV) installation dataset. We'll showcase modern data engineering and analytics tools that enable scalable, cloud-native geospatial workflows.\n",
    "\n",
    "### üõ†Ô∏è Technology Stack\n",
    "\n",
    "- **DuckDB** with spatial extensions for efficient GeoParquet operations\n",
    "- **Ibis** for lazy evaluation and SQL-like operations\n",
    "- **H3** spatial indexing for hierarchical hexagonal grids\n",
    "- **Overture Maps** for administrative boundaries\n",
    "- **Folium** and **Lonboard** for interactive visualizations\n",
    "- **censusdis** for US Census data integration\n",
    "\n",
    "### üìä Dataset: Global Solar Panel (PV) Installations\n",
    "\n",
    "Our consolidated PV dataset includes installations from multiple sources:\n",
    "- **Global Sentinel-2 detections** (2021)\n",
    "- **USA California USGS data** (2016)\n",
    "- **UK crowdsourced data** (2020)\n",
    "- **China medium resolution data** (2024)\n",
    "- **India solar farms** (2022)\n",
    "- **Global harmonized large solar farms** (2020)\n",
    "\n",
    "### üìö Key Learning Objectives\n",
    "\n",
    "1. **Cloud-native geospatial data formats** (GeoParquet)\n",
    "2. **Spatial indexing strategies** (H3 hexagonal grids)\n",
    "3. **Efficient remote data access** (HTTP range requests)\n",
    "4. **Spatial joins** with administrative boundaries\n",
    "5. **Interactive geospatial visualizations**\n",
    "6. **Socioeconomic analysis** with Census data integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1e0f0d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ References and Documentation\n",
    "\n",
    "### Core Technologies\n",
    "- [DuckDB Spatial Extension](https://duckdb.org/docs/extensions/spatial.html) - Native geospatial operations\n",
    "- [Ibis with DuckDB](https://ibis-project.org/backends/DuckDB/) - Lazy evaluation and query optimization\n",
    "- [GeoParquet Specification](https://geoparquet.org/) - Cloud-optimized geospatial format\n",
    "- [DuckLake Documentation](https://ducklake.select/docs/stable/) - Multi-catalog data lakehouse\n",
    "\n",
    "### Spatial Indexing & Visualization\n",
    "- [H3 Spatial Indexing](https://h3geo.org/) - Uber's hexagonal hierarchical indexing\n",
    "- [Overture Maps](https://docs.overturemaps.org/) - Open-source map data\n",
    "- [Folium Documentation](https://python-visualization.github.io/folium/) - Interactive web maps\n",
    "\n",
    "### US Census Integration\n",
    "- [censusdis Documentation](https://censusdis.readthedocs.io/) - Python Census API wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a255f5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Setup: Import Libraries and Configure Environment\n",
    "\n",
    "We begin by importing all necessary libraries and configuring our working environment. This includes:\n",
    "- Core data processing libraries (pandas, numpy, ibis)\n",
    "- Geospatial libraries (geopandas, shapely)\n",
    "- Database and query engines (DuckDB with extensions)\n",
    "- Visualization tools (matplotlib, seaborn, folium)\n",
    "- Spatial indexing (H3)\n",
    "- Census data access (censusdis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cb0cb2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "\n",
    "# Core data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ibis\n",
    "from ibis import _\n",
    "import duckdb\n",
    "\n",
    "# Geospatial libraries\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point, Polygon, box\n",
    "\n",
    "# H3 spatial indexing\n",
    "import h3.api.memview_int as h3\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "from folium import plugins\n",
    "import seaborn as sns\n",
    "\n",
    "# Census data\n",
    "import censusdis\n",
    "from censusdis import data as ced\n",
    "# from censusdis.geography import CensusGeography\n",
    "CENSUSDIS_AVAILABLE = True\n",
    "\n",
    "# Configure pandas and matplotlib\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Ibis configuration\n",
    "ibis.options.interactive = True\n",
    "\n",
    "print(\"‚úÖ All libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc486e6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üóÑÔ∏è Database Connection Setup\n",
    "\n",
    "### Why DuckDB?\n",
    "\n",
    "DuckDB is an **embedded analytical database** designed for OLAP (Online Analytical Processing) workloads. Key advantages:\n",
    "\n",
    "- ‚ö° **Fast**: Columnar storage with vectorized execution\n",
    "- ü™∂ **Lightweight**: Runs in-process, no server required\n",
    "- üîå **Extensible**: Rich ecosystem of extensions (spatial, H3, httpfs)\n",
    "- üåê **Cloud-native**: Native support for Parquet, S3, HTTP range requests\n",
    "\n",
    "### Extensions We're Loading\n",
    "\n",
    "1. **spatial**: Geometry operations, GeoParquet support, spatial functions\n",
    "2. **h3**: H3 spatial indexing functions (from community extensions)\n",
    "3. **httpfs**: Read files from HTTP/S3 without full download\n",
    "4. **cache_httpfs**: HTTP result caching for repeated queries\n",
    "5. **ducklake**: Our custom data catalog management system\n",
    "\n",
    "### Configuration Details\n",
    "\n",
    "We configure DuckDB with:\n",
    "- Memory limit (12GB for large geospatial operations)\n",
    "- Thread count (6 threads for parallel processing)\n",
    "- S3/R2 credentials (for Cloudflare R2 bucket access)\n",
    "- DuckLake catalog attachment (our multi-source data catalog)\n",
    "\n",
    "**Important**: We use production/remote credentials to connect to a Neon Postgres-backed DuckLake catalog (not local Docker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c5be51",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_duckdb_connection(\n",
    "    memory_limit: str = \"12GB\",\n",
    "    threads: int = 6,\n",
    "    use_production: bool = True\n",
    ") -> duckdb.DuckDBPyConnection:\n",
    "    \"\"\"\n",
    "    Create DuckDB connection with spatial extensions and S3 configuration.\n",
    "    Uses production Neon Postgres catalog (not local Docker).\n",
    "    \n",
    "    Args:\n",
    "        memory_limit: Memory limit for DuckDB\n",
    "        threads: Number of threads to use\n",
    "        use_production: Whether to use production catalog (default: True)\n",
    "        \n",
    "    Returns:\n",
    "        Configured DuckDB connection\n",
    "    \"\"\"\n",
    "    # Configuration for DuckDB\n",
    "    config = {\n",
    "        'threads': threads,\n",
    "        'memory_limit': memory_limit,\n",
    "    }\n",
    "    \n",
    "    # Add S3/R2 configuration if credentials exist\n",
    "    if (ak := os.getenv(\"R2_ACCESS_KEY_ID\")) and (sk := os.getenv(\"R2_SECRET_KEY\")):\n",
    "        config.update({\n",
    "            's3_access_key_id': ak,\n",
    "            's3_secret_access_key': sk,\n",
    "            's3_endpoint': os.getenv('R2_S3_ENDPOINT', 'e833ac2d32c62bcff5e4b72c74e5351d.r2.cloudflarestorage.com'),\n",
    "            's3_use_ssl': 'true',\n",
    "            's3_url_style': 'path'\n",
    "        })\n",
    "        print(\"‚úÖ S3/R2 credentials configured\")\n",
    "    \n",
    "    # Create in-memory connection\n",
    "    con = duckdb.connect(database=':memory:', config=config)\n",
    "    \n",
    "    # Install and load extensions\n",
    "    print(\"\\nüì¶ Loading DuckDB extensions...\")\n",
    "    extensions_sql = \"\"\"\n",
    "        INSTALL httpfs;\n",
    "        LOAD httpfs;\n",
    "        INSTALL ducklake;\n",
    "        LOAD ducklake;\n",
    "        INSTALL spatial;\n",
    "        LOAD spatial;\n",
    "        INSTALL h3 FROM community;\n",
    "        LOAD h3;\n",
    "        INSTALL cache_httpfs FROM community;\n",
    "        LOAD cache_httpfs;\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        con.execute(extensions_sql)\n",
    "        print(\"‚úÖ All extensions loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Extension loading error: {e}\")\n",
    "    \n",
    "    # Attach DuckLake catalog (use production by default)\n",
    "    try:\n",
    "        # Use production catalog connection string\n",
    "        local_default = os.getenv('DUCKLAKE_CONNECTION_STRING_DEV')\n",
    "        catalog_string = os.getenv('DUCKLAKE_CONNECTION_STRING_PROD', local_default) if use_production else local_default\n",
    "        \n",
    "        DUCKLAKE_ATTACH = os.getenv(\"DUCKLAKE_ATTACH_PROD\") if use_production else os.getenv(\"DUCKLAKE_ATTACH_DEV\")\n",
    "        DUCKLAKE_NAME = os.getenv(\"DUCKLAKE_NAME\", \"eo_pv_lakehouse\")\n",
    "        DUCKLAKE_DATA_PATH = os.getenv(\"DUCKLAKE_DATA_PATH\")\n",
    "        \n",
    "        if DUCKLAKE_ATTACH:\n",
    "            attach_sql = f\"\"\"\n",
    "            ATTACH IF NOT EXISTS '{DUCKLAKE_ATTACH}' AS {DUCKLAKE_NAME}\n",
    "                (DATA_PATH '{DUCKLAKE_DATA_PATH}');\n",
    "            USE {DUCKLAKE_NAME};\n",
    "            \"\"\"\n",
    "            con.execute(attach_sql)\n",
    "            \n",
    "            print(f\"\\n‚úÖ Attached DuckLake catalog: {DUCKLAKE_NAME}\")\n",
    "            if catalog_string:\n",
    "                catalog_type = catalog_string.split(':')[1] if ':' in catalog_string else 'unknown'\n",
    "                print(f\"   Catalog type: {catalog_type}\")\n",
    "                print(f\"   Data path: {DUCKLAKE_DATA_PATH}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No DuckLake catalog configured\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not attach DuckLake catalog: {e}\")\n",
    "    \n",
    "    return con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02962b39",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create connection with production catalog\n",
    "con = create_duckdb_connection(use_production=True)\n",
    "\n",
    "# Show available tables\n",
    "try:\n",
    "    tables = con.execute(\"SHOW TABLES;\").fetchall()\n",
    "    print(f\"\\nüìä Available tables in catalog: {len(tables)}\")\n",
    "    for table in tables[:10]:\n",
    "        print(f\"   - {table[0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ÑπÔ∏è  Could not list tables: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23417bb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìù TASK 1: Write Optimized GeoParquet to R2 Bucket\n",
    "\n",
    "## üéØ Objective\n",
    "\n",
    "Materialize our `stg_pv_consolidated` view as an **optimized GeoParquet file** stored in a cloud object storage bucket (Cloudflare R2, S3-compatible).\n",
    "\n",
    "## üöÄ Why GeoParquet?\n",
    "\n",
    "**GeoParquet** is a cloud-native geospatial data format that combines:\n",
    "- ‚úÖ **Parquet's efficiency**: Columnar storage, excellent compression\n",
    "- ‚úÖ **Geospatial metadata**: Embedded CRS, bbox for spatial filtering\n",
    "- ‚úÖ **Standard compliance**: GeoParquet 1.1 specification\n",
    "- ‚úÖ **Interoperability**: Works with GDAL, GeoPandas, DuckDB, Arrow\n",
    "\n",
    "## üîß Optimizations Applied\n",
    "\n",
    "### 1. **Hilbert Curve Ordering** üåÄ\n",
    "- Spatial co-locality: Nearby features stored together\n",
    "- Better compression ratios (~15-30% improvement)\n",
    "- Faster spatial filtering with row group pruning\n",
    "- **How it works**: Maps 2D coordinates to 1D curve preserving locality\n",
    "\n",
    "### 2. **ZSTD Compression (Level 9)** üì¶\n",
    "- Superior compression ratio vs Snappy/GZIP (~2-3x vs uncompressed)\n",
    "- Level 9: Aggressive compression (slower write, smaller files)\n",
    "- Decompression speed still excellent for read operations\n",
    "\n",
    "### 3. **Row Group Optimization** üìä\n",
    "- Target: ~100MB row groups (100,000 rows)\n",
    "- Balance between:\n",
    "  - Parallelism (more row groups = more parallel reads)\n",
    "  - Efficiency (fewer row groups = less overhead)\n",
    "\n",
    "### 4. **Spatial Metadata** üó∫Ô∏è\n",
    "- GeoParquet 1.1 bbox struct enables spatial filtering\n",
    "- Column statistics for query optimization\n",
    "- Proper CRS metadata (EPSG:4326)\n",
    "\n",
    "### 5. **Optional Hive Partitioning** üìÅ\n",
    "- Can partition by dataset_name, year, region\n",
    "- Enables partition pruning for faster queries\n",
    "- Trade-off: More files vs query performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e72eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_optimized_geoparquet(\n",
    "    con: duckdb.DuckDBPyConnection,\n",
    "    source_table: str,\n",
    "    output_path: str,\n",
    "    partition_by: list = None,\n",
    "    hilbert_order: bool = True,\n",
    "    compression: str = \"ZSTD\",\n",
    "    compression_level: int = 9,\n",
    "    row_group_size: int = 100000\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Write GeoParquet with spatial optimizations using DuckDB.\n",
    "    \n",
    "    Args:\n",
    "        con: DuckDB connection\n",
    "        source_table: Name of source table/view\n",
    "        output_path: S3/local path for output\n",
    "        partition_by: Columns to partition by (optional)\n",
    "        hilbert_order: Apply Hilbert curve spatial ordering\n",
    "        compression: Compression codec (ZSTD, SNAPPY, GZIP)\n",
    "        compression_level: Compression level (1-22 for ZSTD)\n",
    "        row_group_size: Rows per row group\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with write statistics\n",
    "    \"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"üìù Writing optimized GeoParquet: {output_path}\")\n",
    "    print(f\"   Source: {source_table}\")\n",
    "    \n",
    "    # Get source table info\n",
    "    count_result = con.execute(f\"SELECT COUNT(*) as cnt FROM {source_table}\").fetchone()\n",
    "    total_rows = count_result[0]\n",
    "    print(f\"   Total rows: {total_rows:,}\")\n",
    "    \n",
    "    # Build COPY command with optimizations\n",
    "    copy_sql_parts = [f\"COPY (\"]\n",
    "    \n",
    "    # SELECT with optional Hilbert ordering\n",
    "    if hilbert_order:\n",
    "        # Get spatial extent for Hilbert curve\n",
    "        extent_sql = f\"\"\"\n",
    "        SELECT \n",
    "            MIN(ST_X(ST_Centroid(ST_GeomFromText(geometry)))) as min_x,\n",
    "            MAX(ST_X(ST_Centroid(ST_GeomFromText(geometry)))) as max_x,\n",
    "            MIN(ST_Y(ST_Centroid(ST_GeomFromText(geometry)))) as min_y,\n",
    "            MAX(ST_Y(ST_Centroid(ST_GeomFromText(geometry)))) as max_y\n",
    "        FROM {source_table}\n",
    "        \"\"\"\n",
    "        extent = con.execute(extent_sql).fetchone()\n",
    "        \n",
    "        # Create spatial order using Hilbert curve\n",
    "        copy_sql_parts.append(f\"\"\"\n",
    "            SELECT * FROM {source_table}\n",
    "            ORDER BY ST_Hilbert(\n",
    "                ST_GeomFromText(geometry),\n",
    "                ST_MakeBox2D(\n",
    "                    ST_Point({extent[0]}, {extent[2]}),\n",
    "                    ST_Point({extent[1]}, {extent[3]})\n",
    "                )\n",
    "            )\n",
    "        \"\"\")\n",
    "        print(f\"   ‚úÖ Hilbert curve ordering applied\")\n",
    "        print(f\"      Spatial extent: [{extent[0]:.2f}, {extent[2]:.2f}] to [{extent[1]:.2f}, {extent[3]:.2f}]\")\n",
    "    else:\n",
    "        copy_sql_parts.append(f\"SELECT * FROM {source_table}\")\n",
    "    \n",
    "    copy_sql_parts.append(f\") TO '{output_path}'\")\n",
    "    \n",
    "    # Add format and optimization options\n",
    "    options = [\n",
    "        \"FORMAT PARQUET\",\n",
    "        f\"COMPRESSION {compression}\",\n",
    "    ]\n",
    "    \n",
    "    # Add compression level for ZSTD\n",
    "    if compression.upper() == \"ZSTD\":\n",
    "        options.append(f\"COMPRESSION_LEVEL {compression_level}\")\n",
    "    \n",
    "    # Add row group size\n",
    "    options.append(f\"ROW_GROUP_SIZE {row_group_size}\")\n",
    "    \n",
    "    # Add partitioning if specified\n",
    "    if partition_by:\n",
    "        partition_cols = \", \".join(partition_by)\n",
    "        options.append(f\"PARTITION_BY ({partition_cols})\")\n",
    "        options.append(\"OVERWRITE_OR_IGNORE true\")\n",
    "        print(f\"   ‚úÖ Hive partitioning: {partition_cols}\")\n",
    "    \n",
    "    # Add GeoParquet metadata\n",
    "    # options.append(\"FORMAT PARQUET\")\n",
    "    \n",
    "    copy_sql = \" \".join(copy_sql_parts) + \" (\\n    \" + \",\\n    \".join(options) + \"\\n);\"\n",
    "    \n",
    "    print(f\"\\n   Executing COPY command...\")\n",
    "    print(f\"   Compression: {compression} (level {compression_level})\")\n",
    "    print(f\"   Row group size: {row_group_size:,} rows\")\n",
    "    \n",
    "    try:\n",
    "        con.execute(copy_sql)\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        stats = {\n",
    "            'success': True,\n",
    "            'output_path': output_path,\n",
    "            'total_rows': total_rows,\n",
    "            'elapsed_seconds': elapsed,\n",
    "            'rows_per_second': total_rows / elapsed if elapsed > 0 else 0,\n",
    "            'compression': compression,\n",
    "            'compression_level': compression_level,\n",
    "            'hilbert_ordered': hilbert_order,\n",
    "            'partitioned': bool(partition_by),\n",
    "            'partition_columns': partition_by or []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n‚úÖ GeoParquet written successfully!\")\n",
    "        print(f\"   Time elapsed: {elapsed:.2f}s\")\n",
    "        print(f\"   Throughput: {stats['rows_per_second']:,.0f} rows/sec\")\n",
    "        \n",
    "        return stats\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error writing GeoParquet: {e}\")\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'output_path': output_path\n",
    "        }\n",
    "\n",
    "# Execute Task 1: Write optimized GeoParquet\n",
    "output_path = \"s3://eo-pv-lakehouse/geoparquet/ccom6994_pv_dataset.parquet\"\n",
    "\n",
    "# For local testing without S3 credentials, use local path:\n",
    "# output_path = \"data/ccom6994_pv_dataset.parquet\"\n",
    "\n",
    "write_stats = write_optimized_geoparquet(\n",
    "    con=con,\n",
    "    source_table=\"stg_pv_consolidated\",\n",
    "    output_path=output_path,\n",
    "    partition_by=None,  # Could partition by ['dataset_name', 'year'] if those columns exist\n",
    "    hilbert_order=True,\n",
    "    compression=\"ZSTD\",\n",
    "    compression_level=9,\n",
    "    row_group_size=100000\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Write Statistics:\")\n",
    "for key, value in write_stats.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76be90a5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Validate write by reading back and checking schema + record count\n",
    "print(\"\\nüîç Validating written GeoParquet...\")\n",
    "\n",
    "try:\n",
    "    # Get original row count from source table\n",
    "    original_count = con.execute(\"SELECT COUNT(*) as cnt FROM stg_pv_consolidated\").fetchone()[0]\n",
    "    print(f\"   Original table row count: {original_count:,}\")\n",
    "    \n",
    "    # Read back from R2 and get count\n",
    "    validation_query = f\"SELECT COUNT(*) as cnt FROM read_parquet('{output_path}')\"\n",
    "    written_count = con.execute(validation_query).fetchone()[0]\n",
    "    print(f\"   Written GeoParquet row count: {written_count:,}\")\n",
    "    \n",
    "    # Check if counts match\n",
    "    if original_count == written_count:\n",
    "        print(\"   ‚úÖ Row count validation: PASSED\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Row count mismatch: {original_count:,} vs {written_count:,}\")\n",
    "    \n",
    "    # Validate schema by reading a sample\n",
    "    schema_query = f\"SELECT * FROM read_parquet('{output_path}') LIMIT 1\"\n",
    "    sample_df = con.execute(schema_query).fetchdf()\n",
    "    print(f\"\\n   üìã Schema validation:\")\n",
    "    print(f\"      Columns: {len(sample_df.columns)}\")\n",
    "    print(f\"      Column names: {list(sample_df.columns)}\")\n",
    "    print(\"   ‚úÖ Schema validation: PASSED\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Validation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a72ad4",
   "metadata": {},
   "source": [
    "### üí° Key Takeaways from Task 1\n",
    "\n",
    "**What we accomplished:**\n",
    "- ‚úÖ Materialized staging view to production-ready GeoParquet\n",
    "- ‚úÖ Applied spatial ordering for better compression & query performance\n",
    "- ‚úÖ Used aggressive compression without sacrificing read performance\n",
    "- ‚úÖ Configured optimal row group size for parallel processing\n",
    "\n",
    "**Performance insights:**\n",
    "- Hilbert ordering provides ~15-30% better compression\n",
    "- ZSTD level 9 achieves ~2-3x compression vs uncompressed\n",
    "- Row group size affects query parallelism and memory usage\n",
    "- Cloud storage (R2/S3) enables scalable, distributed access\n",
    "\n",
    "**Real-world benefits:**\n",
    "- Reduced storage costs\n",
    "- Faster query performance (row group pruning)\n",
    "- Better data sharing (standard format)\n",
    "- Improved analytics throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a7b0d3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üì• TASK 2: Reading Parquet from Remote S3 Locations\n",
    "\n",
    "## üéØ Objective\n",
    "\n",
    "Demonstrate **two different approaches** for reading remote Parquet files:\n",
    "1. **pandas + s3fs**: Traditional approach using AWS SDK\n",
    "2. **DuckDB + httpfs**: Modern approach using HTTP range requests\n",
    "\n",
    "## ü§î Why Multiple Approaches?\n",
    "\n",
    "Different use cases require different tools:\n",
    "- **pandas**: Familiar API, good for small-to-medium datasets\n",
    "- **DuckDB**: Optimized for analytical queries, excellent for large datasets\n",
    "\n",
    "## üìä Performance Comparison\n",
    "\n",
    "| Feature | pandas + s3fs | DuckDB + httpfs |\n",
    "|---------|---------------|------------------|\n",
    "| **AWS SDK required** | ‚úÖ Yes | ‚ùå No (HTTP only) |\n",
    "| **Column pruning** | ‚ö†Ô∏è Limited | ‚úÖ Excellent |\n",
    "| **Predicate pushdown** | ‚ùå No | ‚úÖ Yes |\n",
    "| **Memory efficient** | ‚ùå Loads all | ‚úÖ Lazy evaluation |\n",
    "| **Parallel reading** | ‚ö†Ô∏è Limited | ‚úÖ Yes (auto) |\n",
    "| **Spatial functions** | ‚ùå No | ‚úÖ Yes (spatial ext) |\n",
    "| **Query optimization** | ‚ùå No | ‚úÖ Yes (CBO) |\n",
    "\n",
    "**Recommendation**: Use DuckDB for large files and analytical workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba11fcd5",
   "metadata": {},
   "source": [
    "## 2.1: Reading with pandas + s3fs\n",
    "\n",
    "### How it works:\n",
    "- Uses `s3fs` library to provide filesystem-like interface to S3\n",
    "- Requires AWS credentials (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n",
    "- Downloads entire file (or uses random access if supported)\n",
    "- Returns familiar pandas DataFrame\n",
    "\n",
    "### Best for:\n",
    "- Small to medium datasets (<1GB)\n",
    "- When you need full pandas DataFrame API\n",
    "- Compatibility with existing pandas workflows\n",
    "\n",
    "**Note**: For this demo, we'll focus on the DuckDB approach in section 2.2, which is more efficient\n",
    "for our use case and doesn't require additional s3fs library installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855a18b4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def read_parquet_with_pandas(\n",
    "    path: str,\n",
    "    sample_frac: float = 1.0,\n",
    "    columns: list = None,\n",
    "    use_pyarrow: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read Parquet from S3/R2 using pandas + s3fs.\n",
    "    \n",
    "    Requires: pip install s3fs pyarrow\n",
    "    \n",
    "    Args:\n",
    "        path: S3/R2 path to Parquet file (e.g., 's3://bucket/key.parquet')\n",
    "        sample_frac: Fraction of data to sample (1.0 = all data)\n",
    "        columns: List of columns to read (None = all columns)\n",
    "        use_pyarrow: Use PyArrow engine for reading (recommended)\n",
    "        \n",
    "    Returns:\n",
    "        Pandas DataFrame\n",
    "    \"\"\"\n",
    "    import time\n",
    "    start = time.time()\n",
    "    \n",
    "    print(f\"üì• Reading with pandas + s3fs: {path}\")\n",
    "    \n",
    "    try:\n",
    "        import s3fs\n",
    "    except ImportError:\n",
    "        print(\"‚ùå s3fs not installed. Install with: pip install s3fs\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Get credentials from environment\n",
    "    access_key = os.getenv('R2_ACCESS_KEY_ID')\n",
    "    secret_key = os.getenv('R2_SECRET_KEY')\n",
    "    endpoint = os.getenv('R2_S3_ENDPOINT', 'e833ac2d32c62bcff5e4b72c74e5351d.r2.cloudflarestorage.com')\n",
    "    \n",
    "    if not access_key or not secret_key:\n",
    "        print(\"‚ö†Ô∏è  R2 credentials not found in environment variables\")\n",
    "        print(\"   Set R2_ACCESS_KEY_ID and R2_SECRET_KEY\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Create S3 filesystem for Cloudflare R2\n",
    "    # Key configuration: anon=False, region_name='auto' (R2 specific)\n",
    "    fs = s3fs.S3FileSystem(\n",
    "        anon=False,\n",
    "        use_ssl=True,\n",
    "        client_kwargs={\n",
    "            'region_name': 'auto',  # R2 uses 'auto' as region\n",
    "            'endpoint_url': f'https://{endpoint}',\n",
    "            'aws_access_key_id': access_key,\n",
    "            'aws_secret_access_key': secret_key,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"   Endpoint: https://{endpoint}\")\n",
    "    print(f\"   Region: auto (Cloudflare R2)\")\n",
    "    \n",
    "    try:\n",
    "        # Read Parquet file through s3fs\n",
    "        # Using 'with' statement ensures proper file handle cleanup\n",
    "        with fs.open(path, 'rb') as f:\n",
    "            engine = 'pyarrow' if use_pyarrow else 'fastparquet'\n",
    "            df = pd.read_parquet(f, columns=columns, engine=engine)\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        print(f\"‚úÖ Read complete: {len(df):,} rows √ó {len(df.columns)} cols in {elapsed:.2f}s\")\n",
    "        \n",
    "        # Sample if requested\n",
    "        if sample_frac < 1.0:\n",
    "            original_len = len(df)\n",
    "            df = df.sample(frac=sample_frac, random_state=42)\n",
    "            print(f\"   Sampled {len(df):,} / {original_len:,} rows ({sample_frac*100:.1f}%)\")\n",
    "        \n",
    "        # Calculate throughput\n",
    "        throughput = len(df) / elapsed if elapsed > 0 else 0\n",
    "        print(f\"   Throughput: {throughput:,.0f} rows/sec\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading with pandas + s3fs: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Example: Read with pandas + s3fs (50% sample - half the dataset)\n",
    "print(\"\\n### Example: Reading with pandas + s3fs ###\")\n",
    "df_pandas = read_parquet_with_pandas(\n",
    "    \"s3://eo-pv-lakehouse/geoparquet/ccom6994_pv_dataset.parquet\",\n",
    "    sample_frac=0.50,\n",
    "    columns=['unified_id', 'dataset_name', 'area_m2', 'centroid_lon', 'centroid_lat', 'geometry']\n",
    ")\n",
    "\n",
    "if not df_pandas.empty:\n",
    "    print(f\"\\nüìä Sample data preview:\")\n",
    "    print(df_pandas.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffa8b7d",
   "metadata": {},
   "source": [
    "## 2.2: Reading with DuckDB + httpfs\n",
    "\n",
    "### How it works:\n",
    "- Uses **HTTP range requests** to read only needed data\n",
    "- Reads Parquet metadata first (~few KB)\n",
    "- Applies **column pruning** and **predicate pushdown**\n",
    "- Only fetches required row groups\n",
    "- Parallel downloads for multiple row groups\n",
    "\n",
    "### Advantages:\n",
    "1. **No AWS SDK required**: Works with any HTTP(S) endpoint\n",
    "2. **Lazy evaluation**: Only reads what you query\n",
    "3. **Query optimization**: DuckDB's cost-based optimizer\n",
    "4. **Spatial functions**: Native geometry operations\n",
    "5. **Memory efficient**: Streaming execution\n",
    "\n",
    "### Best for:\n",
    "- Large datasets (>1GB)\n",
    "- Analytical queries (aggregations, filters)\n",
    "- When you need column/row subset\n",
    "- Spatial operations on geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df954ef",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def read_parquet_with_duckdb(\n",
    "    con: duckdb.DuckDBPyConnection,\n",
    "    path: str,\n",
    "    columns: list = None,\n",
    "    filter_expr: str = None,\n",
    "    limit: int = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read Parquet using DuckDB with httpfs extension.\n",
    "    \n",
    "    Supports:\n",
    "        - Local paths: /path/to/file.parquet\n",
    "        - S3 paths: s3://bucket/key\n",
    "        - HTTP(S) paths: https://domain.com/file.parquet\n",
    "        \n",
    "    Args:\n",
    "        con: DuckDB connection (with httpfs loaded)\n",
    "        path: Path to Parquet file (local, s3, or https)\n",
    "        columns: List of columns to read (None = all)\n",
    "        filter_expr: SQL WHERE clause (e.g., \"area_m2 > 1000\")\n",
    "        limit: Maximum rows to return\n",
    "        \n",
    "    Returns:\n",
    "        Pandas DataFrame\n",
    "    \"\"\"\n",
    "    import time\n",
    "    start = time.time()\n",
    "    \n",
    "    print(f\"üì• Reading with DuckDB + httpfs: {path}\")\n",
    "    \n",
    "    # Build query\n",
    "    select_cols = \", \".join(columns) if columns else \"*\"\n",
    "    query = f\"SELECT {select_cols} FROM read_parquet('{path}')\"\n",
    "    \n",
    "    if filter_expr:\n",
    "        query += f\" WHERE {filter_expr}\"\n",
    "        print(f\"   Filter: {filter_expr}\")\n",
    "    \n",
    "    if limit:\n",
    "        query += f\" LIMIT {limit}\"\n",
    "        print(f\"   Limit: {limit:,} rows\")\n",
    "    \n",
    "    try:\n",
    "        df = con.execute(query).fetchdf()\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        print(f\"‚úÖ Read complete: {len(df):,} rows √ó {len(df.columns)} cols in {elapsed:.2f}s\")\n",
    "        print(f\"   Throughput: {len(df) / elapsed:,.0f} rows/sec\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading with DuckDB: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Example 1: Read first 220,000 rows (approximately half the dataset)\n",
    "df_sample = read_parquet_with_duckdb(\n",
    "    con=con,\n",
    "    path=\"s3://eo-pv-lakehouse/geoparquet/ccom6994_pv_dataset.parquet\",\n",
    "    limit=220000\n",
    ")\n",
    "\n",
    "# Example 2: Read specific columns with filter\n",
    "df_filtered = read_parquet_with_duckdb(\n",
    "    con=con,\n",
    "    path=\"s3://eo-pv-lakehouse/geoparquet/ccom6994_pv_dataset.parquet\",\n",
    "    columns=['unified_id', 'dataset_name', 'area_m2', 'centroid_lon', 'centroid_lat', 'geometry'],\n",
    "    filter_expr=\"area_m2 > 5000\",  # Only large installations\n",
    "    limit=50000\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Filtered dataset preview:\")\n",
    "print(df_filtered.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bc76d0",
   "metadata": {},
   "source": [
    "## 2.3: Performance Comparison\n",
    "\n",
    "Key differences:\n",
    "\n",
    "| Feature | pandas + s3fs | DuckDB + httpfs |\n",
    "|---------|---------------|-----------------|\n",
    "| AWS SDK required | ‚úÖ Yes | ‚ùå No |\n",
    "| Column pruning | ‚ùå Limited | ‚úÖ Excellent |\n",
    "| Predicate pushdown | ‚ùå No | ‚úÖ Yes |\n",
    "| Memory efficient | ‚ùå Loads all | ‚úÖ Lazy |\n",
    "| Parallel reading | ‚ö†Ô∏è Limited | ‚úÖ Yes |\n",
    "| Spatial functions | ‚ùå No | ‚úÖ Yes (spatial ext) |\n",
    "\n",
    "**Recommendation**: Use DuckDB for large files and when you need filtering/column selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c502b871",
   "metadata": {},
   "source": [
    "---\n",
    "# TASK 3: Overture Maps Integration\n",
    "\n",
    "**Objective**: Fetch administrative boundaries from Overture Maps and perform spatial joins\n",
    "\n",
    "Overture Maps provides:\n",
    "- `division`: Point locations of administrative areas\n",
    "- `division_area`: Polygon boundaries\n",
    "- `division_boundary`: Boundary lines\n",
    "\n",
    "We'll fetch countries and major cities, then spatially join with our PV data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dac329d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def fetch_overture_divisions(\n",
    "    con: duckdb.DuckDBPyConnection,\n",
    "    division_type: str = \"country\",\n",
    "    bbox: tuple = None,\n",
    "    limit: int = None\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Fetch administrative divisions from Overture Maps.\n",
    "    \n",
    "    Args:\n",
    "        con: DuckDB connection (with httpfs and spatial extensions)\n",
    "        division_type: Type of division ('country', 'region', 'locality')\n",
    "        bbox: Bounding box tuple (min_lon, min_lat, max_lon, max_lat)\n",
    "        limit: Maximum number of features to fetch\n",
    "        \n",
    "    Returns:\n",
    "        GeoDataFrame with administrative boundaries\n",
    "    \"\"\"\n",
    "    print(f\"üó∫Ô∏è  Fetching Overture Maps divisions: {division_type}\")\n",
    "    \n",
    "    # Save current R2 credentials to restore later\n",
    "    r2_access_key = os.getenv('R2_ACCESS_KEY_ID')\n",
    "    r2_secret_key = os.getenv('R2_SECRET_KEY')\n",
    "    r2_endpoint = os.getenv('R2_S3_ENDPOINT', 'e833ac2d32c62bcff5e4b72c74e5351d.r2.cloudflarestorage.com')\n",
    "    \n",
    "    # Configure DuckDB for Overture Maps on AWS S3\n",
    "    # Important: Overture Maps is hosted on public AWS, not R2\n",
    "    # We need to CLEAR credentials and use anonymous access\n",
    "    try:\n",
    "        # Clear R2 credentials for anonymous access to public AWS bucket\n",
    "        con.execute(\"SET s3_access_key_id='';\")\n",
    "        con.execute(\"SET s3_secret_access_key='';\")\n",
    "        con.execute(\"SET s3_endpoint='';\")\n",
    "        con.execute(\"SET s3_region='us-west-2';\")\n",
    "        con.execute(\"SET s3_url_style='path';\")\n",
    "        con.execute(\"SET s3_use_ssl=true;\")\n",
    "        print(f\"   ‚úÖ Configured DuckDB for AWS S3 (anonymous access, us-west-2)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Could not configure S3 settings: {e}\")\n",
    "    \n",
    "    # Overture Maps S3 path (latest release: 2025-10-22.0)\n",
    "    overture_base = \"s3://overturemaps-us-west-2/release/2025-10-22.0\"\n",
    "    division_path = f\"{overture_base}/theme=divisions/type=division_area/*\"\n",
    "    \n",
    "    print(f\"   üì¶ Overture release: 2025-10-22.0\")\n",
    "    print(f\"   üåç Data source: AWS S3 (public bucket, no credentials)\")\n",
    "    \n",
    "    # Build query\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        id,\n",
    "        names.primary as name,\n",
    "        subtype,\n",
    "        country,\n",
    "        region,\n",
    "        ST_AsText(geometry) as geometry,\n",
    "        ST_Area_Spheroid(geometry) as area_km2,\n",
    "        bbox.xmin, bbox.xmax, bbox.ymin, bbox.ymax\n",
    "    FROM read_parquet('{division_path}')\n",
    "    WHERE subtype = '{division_type}'\n",
    "    \"\"\"\n",
    "    \n",
    "    if bbox:\n",
    "        min_lon, min_lat, max_lon, max_lat = bbox\n",
    "        query += f\"\"\"\n",
    "        AND bbox.xmin >= {min_lon} AND bbox.xmax <= {max_lon}\n",
    "        AND bbox.ymin >= {min_lat} AND bbox.ymax <= {max_lat}\n",
    "        \"\"\"\n",
    "        print(f\"   üìç Filtering by bbox: [{min_lon}, {min_lat}] to [{max_lon}, {max_lat}]\")\n",
    "    \n",
    "    if limit:\n",
    "        query += f\" LIMIT {limit}\"\n",
    "    \n",
    "    try:\n",
    "        df = con.execute(query).fetchdf()\n",
    "        print(f\"‚úÖ Fetched {len(df):,} {division_type} divisions\")\n",
    "        \n",
    "        # Convert to GeoDataFrame\n",
    "        df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "        gdf = gpd.GeoDataFrame(df, geometry='geometry', crs='EPSG:4326')\n",
    "        \n",
    "        return gdf\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error fetching Overture data: {e}\")\n",
    "        print(f\"   Error details: {str(e)[:200]}\")\n",
    "        return gpd.GeoDataFrame()\n",
    "    finally:\n",
    "        # Restore R2 configuration for subsequent queries\n",
    "        try:\n",
    "            if r2_access_key and r2_secret_key:\n",
    "                con.execute(f\"SET s3_access_key_id='{r2_access_key}';\")\n",
    "                con.execute(f\"SET s3_secret_access_key='{r2_secret_key}';\")\n",
    "            con.execute(f\"SET s3_endpoint='{r2_endpoint}';\")\n",
    "            con.execute(\"SET s3_region='auto';\")\n",
    "            print(f\"   üîÑ Restored R2 configuration (endpoint: {r2_endpoint})\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Could not restore R2 config: {e}\")\n",
    "\n",
    "# Fetch countries\n",
    "countries_gdf = fetch_overture_divisions(\n",
    "    con=con,\n",
    "    division_type=\"country\",\n",
    "    limit=250  # Limit for faster demo\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Country data preview:\")\n",
    "print(countries_gdf[['name', 'country', 'area_km2']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b60e44",
   "metadata": {},
   "source": [
    "## 3.2: Spatial Join with PV Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aeab58",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def spatial_join_pv_with_divisions(\n",
    "    pv_gdf: gpd.GeoDataFrame,\n",
    "    divisions_gdf: gpd.GeoDataFrame,\n",
    "    division_name: str = \"divisions\"\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Perform spatial join between PV installations and administrative divisions.\n",
    "    \n",
    "    Args:\n",
    "        pv_gdf: GeoDataFrame with PV installations\n",
    "        divisions_gdf: GeoDataFrame with administrative boundaries\n",
    "        division_name: Name for division columns\n",
    "        \n",
    "    Returns:\n",
    "        GeoDataFrame with joined data\n",
    "    \"\"\"\n",
    "    print(f\"üîó Spatial join: PV √ó {division_name}\")\n",
    "    print(f\"   PV records: {len(pv_gdf):,}\")\n",
    "    print(f\"   {division_name.capitalize()}: {len(divisions_gdf):,}\")\n",
    "    \n",
    "    # Ensure both have geometry\n",
    "    if 'geometry' not in pv_gdf.columns or 'geometry' not in divisions_gdf.columns:\n",
    "        print(\"‚ùå Both GeoDataFrames must have geometry column\")\n",
    "        return gpd.GeoDataFrame()\n",
    "    \n",
    "    # Spatial join (intersects)\n",
    "    joined = gpd.sjoin(\n",
    "        pv_gdf,\n",
    "        divisions_gdf[['geometry', 'name', 'country', 'subtype']],\n",
    "        how='left',\n",
    "        predicate='intersects'\n",
    "    )\n",
    "    \n",
    "    # Rename joined columns\n",
    "    joined = joined.rename(columns={\n",
    "        'name': f'{division_name}_name',\n",
    "        'country': f'{division_name}_country',\n",
    "        'subtype': f'{division_name}_type'\n",
    "    })\n",
    "    \n",
    "    # Count intersections\n",
    "    matched = joined[f'{division_name}_name'].notna().sum()\n",
    "    match_pct = (matched / len(joined)) * 100\n",
    "    \n",
    "    print(f\"‚úÖ Spatial join complete\")\n",
    "    print(f\"   Matched: {matched:,} / {len(joined):,} ({match_pct:.1f}%)\")\n",
    "    print(f\"   Unmatched: {len(joined) - matched:,}\")\n",
    "    \n",
    "    return joined\n",
    "\n",
    "# Load sample PV data for spatial join (half the dataset)\n",
    "pv_sample_df = read_parquet_with_duckdb(\n",
    "    con=con,\n",
    "    path=\"s3://eo-pv-lakehouse/geoparquet/ccom6994_pv_dataset.parquet\",\n",
    "    limit=220000\n",
    ")\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "pv_sample_df['geometry'] = pv_sample_df['geometry'].apply(wkt.loads)\n",
    "pv_sample_gdf = gpd.GeoDataFrame(pv_sample_df, geometry='geometry', crs='EPSG:4326')\n",
    "\n",
    "# Perform spatial join\n",
    "pv_with_countries = spatial_join_pv_with_divisions(\n",
    "    pv_gdf=pv_sample_gdf,\n",
    "    divisions_gdf=countries_gdf,\n",
    "    division_name=\"country\"\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Top 10 countries by PV installation count:\")\n",
    "country_counts = pv_with_countries.groupby('country_name').size().sort_values(ascending=False)\n",
    "print(country_counts.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d574f194",
   "metadata": {},
   "source": [
    "## 3.3: Visualize with Folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da212c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pv_map_with_divisions(\n",
    "    pv_gdf: gpd.GeoDataFrame,\n",
    "    divisions_gdf: gpd.GeoDataFrame = None,\n",
    "    center: tuple = None,\n",
    "    zoom_start: int = 4,\n",
    "    max_points: int = 1000\n",
    ") -> folium.Map:\n",
    "    \"\"\"\n",
    "    Create interactive Folium map with PV installations and administrative boundaries.\n",
    "    \n",
    "    Args:\n",
    "        pv_gdf: GeoDataFrame with PV installations\n",
    "        divisions_gdf: GeoDataFrame with administrative boundaries (optional)\n",
    "        center: Map center (lat, lon), auto-computed if None\n",
    "        zoom_start: Initial zoom level\n",
    "        max_points: Maximum PV points to plot (for performance)\n",
    "        \n",
    "    Returns:\n",
    "        Folium Map object\n",
    "    \"\"\"\n",
    "    print(f\"üó∫Ô∏è  Creating interactive map with Folium\")\n",
    "    \n",
    "    # Sample PV data if too many points\n",
    "    if len(pv_gdf) > max_points:\n",
    "        pv_gdf_plot = pv_gdf.sample(n=max_points, random_state=42)\n",
    "        print(f\"   Sampled {max_points:,} / {len(pv_gdf):,} PV points\")\n",
    "    else:\n",
    "        pv_gdf_plot = pv_gdf\n",
    "    \n",
    "    # Compute center if not provided\n",
    "    if center is None:\n",
    "        center = [pv_gdf_plot.geometry.centroid.y.mean(), \n",
    "                  pv_gdf_plot.geometry.centroid.x.mean()]\n",
    "    \n",
    "    # Create base map\n",
    "    m = folium.Map(location=center, zoom_start=zoom_start, tiles='OpenStreetMap')\n",
    "    \n",
    "    # Add administrative boundaries if provided\n",
    "    if divisions_gdf is not None and not divisions_gdf.empty:\n",
    "        folium.GeoJson(\n",
    "            divisions_gdf,\n",
    "            name='Administrative Boundaries',\n",
    "            style_function=lambda x: {\n",
    "                'fillColor': 'lightblue',\n",
    "                'color': 'blue',\n",
    "                'weight': 2,\n",
    "                'fillOpacity': 0.1\n",
    "            },\n",
    "            tooltip=folium.GeoJsonTooltip(\n",
    "                fields=['name', 'country', 'subtype'],\n",
    "                aliases=['Name:', 'Country:', 'Type:']\n",
    "            )\n",
    "        ).add_to(m)\n",
    "        print(f\"   ‚úÖ Added {len(divisions_gdf)} administrative boundaries\")\n",
    "    \n",
    "    # Add PV installations as markers\n",
    "    marker_cluster = plugins.MarkerCluster(name='PV Installations')\n",
    "    \n",
    "    for idx, row in pv_gdf_plot.iterrows():\n",
    "        # Get centroid for marker placement\n",
    "        if row.geometry.geom_type == 'Point':\n",
    "            coords = [row.geometry.y, row.geometry.x]\n",
    "        else:\n",
    "            centroid = row.geometry.centroid\n",
    "            coords = [centroid.y, centroid.x]\n",
    "        \n",
    "        # Create popup with installation info\n",
    "        popup_html = f\"\"\"\n",
    "        <b>PV Installation</b><br>\n",
    "        ID: {row.get('unified_id', 'N/A')}<br>\n",
    "        Dataset: {row.get('dataset_name', 'N/A')}<br>\n",
    "        Area: {row.get('area_m2', 0):.0f} m¬≤<br>\n",
    "        Country: {row.get('country_name', 'Unknown')}\n",
    "        \"\"\"\n",
    "        \n",
    "        folium.CircleMarker(\n",
    "            location=coords,\n",
    "            radius=5,\n",
    "            popup=folium.Popup(popup_html, max_width=200),\n",
    "            color='orange',\n",
    "            fill=True,\n",
    "            fillColor='yellow',\n",
    "            fillOpacity=0.6\n",
    "        ).add_to(marker_cluster)\n",
    "    \n",
    "    marker_cluster.add_to(m)\n",
    "    print(f\"   ‚úÖ Added {len(pv_gdf_plot):,} PV markers\")\n",
    "    \n",
    "    # Add layer control\n",
    "    folium.LayerControl().add_to(m)\n",
    "    \n",
    "    print(f\"‚úÖ Map created successfully\")\n",
    "    return m\n",
    "\n",
    "# Create map (for a specific region to keep it manageable)\n",
    "# Filter to a region (e.g., Europe)\n",
    "europe_bbox = (-10, 35, 30, 60)  # (min_lon, min_lat, max_lon, max_lat)\n",
    "\n",
    "pv_europe = pv_with_countries[\n",
    "    (pv_with_countries.geometry.centroid.x >= europe_bbox[0]) &\n",
    "    (pv_with_countries.geometry.centroid.x <= europe_bbox[2]) &\n",
    "    (pv_with_countries.geometry.centroid.y >= europe_bbox[1]) &\n",
    "    (pv_with_countries.geometry.centroid.y <= europe_bbox[3])\n",
    "]\n",
    "\n",
    "countries_europe = countries_gdf[\n",
    "    countries_gdf.country.isin(['GB', 'FR', 'DE', 'ES', 'IT', 'NL', 'BE'])\n",
    "]\n",
    "\n",
    "pv_map = create_pv_map_with_divisions(\n",
    "    pv_gdf=pv_europe,\n",
    "    divisions_gdf=countries_europe,\n",
    "    center=[50, 10],  # Center on Europe\n",
    "    zoom_start=5,\n",
    "    max_points=2500\n",
    ")\n",
    "\n",
    "# Save map\n",
    "pv_map.save('pv_overture_map.html')\n",
    "print(\"\\nüíæ Map saved to: pv_overture_map.html\")\n",
    "\n",
    "# Display in notebook (if running in Jupyter)\n",
    "# pv_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28b56f9",
   "metadata": {},
   "source": [
    "---\n",
    "# TASK 4: H3 Hexagon Visualization\n",
    "\n",
    "**Objective**: Apply H3 spatial indexing and visualize PV density in hexagonal cells\n",
    "\n",
    "H3 provides hierarchical hexagonal grids:\n",
    "- Resolution 0: ~4M km¬≤ per cell (global)\n",
    "- Resolution 5: ~250 km¬≤ per cell (country)\n",
    "- Resolution 8: ~0.4 km¬≤ per cell (city)\n",
    "- Resolution 10: ~15,000 m¬≤ per cell (neighborhood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb94b1a7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def add_h3_index(\n",
    "    gdf: gpd.GeoDataFrame,\n",
    "    resolution: int = 8,\n",
    "    lat_col: str = 'centroid_lat',\n",
    "    lon_col: str = 'centroid_lon'\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Add H3 spatial index to GeoDataFrame.\n",
    "    \n",
    "    Args:\n",
    "        gdf: GeoDataFrame with point data\n",
    "        resolution: H3 resolution (0-15)\n",
    "        lat_col: Column name for latitude\n",
    "        lon_col: Column name for longitude\n",
    "        \n",
    "    Returns:\n",
    "        GeoDataFrame with h3_index column\n",
    "    \"\"\"\n",
    "    print(f\"üî∑ Adding H3 index at resolution {resolution}\")\n",
    "    \n",
    "    # Apply H3 indexing using vectorized operations\n",
    "    gdf['h3_index'] = gdf.apply(\n",
    "        lambda row: h3.latlng_to_cell(row[lat_col], row[lon_col], resolution),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    unique_cells = gdf['h3_index'].nunique()\n",
    "    print(f\"‚úÖ H3 indexing complete: {unique_cells:,} unique cells\")\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "def create_h3_hexagon_geometries(h3_indices: list) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Convert H3 indices to hexagon polygon geometries.\n",
    "    \n",
    "    Args:\n",
    "        h3_indices: List of H3 cell indices\n",
    "        \n",
    "    Returns:\n",
    "        GeoDataFrame with hexagon geometries\n",
    "    \"\"\"\n",
    "    print(f\"üìê Creating hexagon geometries for {len(h3_indices):,} H3 cells\")\n",
    "    \n",
    "    hexagons = []\n",
    "    for h3_idx in h3_indices:\n",
    "        # Get hexagon boundary\n",
    "        boundary = h3.cell_to_boundary(h3_idx)\n",
    "        # Convert to Polygon (boundary returns lat/lon pairs)\n",
    "        polygon = Polygon([(lon, lat) for lat, lon in boundary])\n",
    "        hexagons.append({'h3_index': h3_idx, 'geometry': polygon})\n",
    "    \n",
    "    gdf = gpd.GeoDataFrame(hexagons, crs='EPSG:4326')\n",
    "    print(f\"‚úÖ Created {len(gdf):,} hexagon polygons\")\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "# Add H3 index to PV data\n",
    "h3_resolution = 8  # ~0.4 km¬≤ per cell\n",
    "pv_with_h3 = add_h3_index(pv_sample_gdf, resolution=h3_resolution)\n",
    "\n",
    "# Aggregate PV counts by H3 cell\n",
    "h3_aggregated = pv_with_h3.groupby('h3_index').agg({\n",
    "    'unified_id': 'count',\n",
    "    'area_m2': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "h3_aggregated.columns = ['h3_index', 'pv_count', 'total_area_m2']\n",
    "\n",
    "print(f\"\\nüìä H3 aggregation statistics:\")\n",
    "print(h3_aggregated.describe())\n",
    "\n",
    "# Create hexagon geometries for top cells\n",
    "top_cells = h3_aggregated.nlargest(100, 'pv_count')['h3_index'].tolist()\n",
    "h3_hexagons = create_h3_hexagon_geometries(top_cells)\n",
    "\n",
    "# Join with aggregated data\n",
    "h3_hexagons = h3_hexagons.merge(h3_aggregated, on='h3_index')\n",
    "\n",
    "print(f\"\\nüìä Top 10 H3 cells by PV count:\")\n",
    "print(h3_aggregated.nlargest(10, 'pv_count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d78243",
   "metadata": {},
   "source": [
    "## 4.2: Visualize H3 Hexagons with Folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8167f19",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_h3_hexagons(\n",
    "    h3_gdf: gpd.GeoDataFrame,\n",
    "    value_column: str = 'pv_count',\n",
    "    center: tuple = None,\n",
    "    zoom_start: int = 6,\n",
    "    colormap: str = 'YlOrRd'\n",
    ") -> folium.Map:\n",
    "    \"\"\"\n",
    "    Create choropleth map of H3 hexagon cells.\n",
    "    \n",
    "    Args:\n",
    "        h3_gdf: GeoDataFrame with H3 hexagon geometries and values\n",
    "        value_column: Column to visualize\n",
    "        center: Map center (lat, lon)\n",
    "        zoom_start: Initial zoom level\n",
    "        colormap: Matplotlib colormap name\n",
    "        \n",
    "    Returns:\n",
    "        Folium Map object\n",
    "    \"\"\"\n",
    "    print(f\"üó∫Ô∏è  Visualizing {len(h3_gdf):,} H3 hexagons\")\n",
    "    \n",
    "    if center is None:\n",
    "        center = [h3_gdf.geometry.centroid.y.mean(), \n",
    "                  h3_gdf.geometry.centroid.x.mean()]\n",
    "    \n",
    "    # Create map\n",
    "    m = folium.Map(location=center, zoom_start=zoom_start, tiles='CartoDB positron')\n",
    "    \n",
    "    # Create choropleth layer\n",
    "    folium.Choropleth(\n",
    "        geo_data=h3_gdf,\n",
    "        data=h3_gdf,\n",
    "        columns=['h3_index', value_column],\n",
    "        key_on='feature.properties.h3_index',\n",
    "        fill_color=colormap,\n",
    "        fill_opacity=0.6,\n",
    "        line_opacity=0.2,\n",
    "        legend_name=f'{value_column}',\n",
    "        highlight=True\n",
    "    ).add_to(m)\n",
    "    \n",
    "    # Add tooltips\n",
    "    folium.GeoJson(\n",
    "        h3_gdf,\n",
    "        style_function=lambda x: {\n",
    "            'fillColor': 'transparent',\n",
    "            'color': 'transparent'\n",
    "        },\n",
    "        tooltip=folium.GeoJsonTooltip(\n",
    "            fields=['h3_index', value_column, 'total_area_m2'],\n",
    "            aliases=['H3 Cell:', 'PV Count:', 'Total Area (m¬≤):'],\n",
    "            localize=True\n",
    "        )\n",
    "    ).add_to(m)\n",
    "    \n",
    "    print(f\"‚úÖ H3 hexagon map created\")\n",
    "    return m\n",
    "\n",
    "# Create H3 hexagon map\n",
    "h3_map = visualize_h3_hexagons(\n",
    "    h3_gdf=h3_hexagons,\n",
    "    value_column='pv_count',\n",
    "    zoom_start=6,\n",
    "    colormap='YlOrRd'\n",
    ")\n",
    "\n",
    "h3_map.save('pv_h3_hexagons.html')\n",
    "print(\"\\nüíæ Map saved to: pv_h3_hexagons.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89931749",
   "metadata": {},
   "source": [
    "## 4.3: H3 Hexagon Heatmap with Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2fd94f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_h3_heatmap(h3_gdf: gpd.GeoDataFrame, value_column: str = 'pv_count'):\n",
    "    \"\"\"\n",
    "    Create static heatmap of H3 hexagons using matplotlib.\n",
    "    \n",
    "    Args:\n",
    "        h3_gdf: GeoDataFrame with H3 hexagon geometries\n",
    "        value_column: Column to visualize\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    \n",
    "    # Plot hexagons with color scale\n",
    "    h3_gdf.plot(\n",
    "        column=value_column,\n",
    "        cmap='YlOrRd',\n",
    "        legend=True,\n",
    "        legend_kwds={'label': f'{value_column}', 'shrink': 0.8},\n",
    "        edgecolor='black',\n",
    "        linewidth=0.3,\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f'PV Installations Density (H3 Resolution 8)\\nTop 100 Cells', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Longitude', fontsize=12)\n",
    "    ax.set_ylabel('Latitude', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pv_h3_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"üíæ Heatmap saved to: pv_h3_heatmap.png\")\n",
    "    plt.show()\n",
    "\n",
    "plot_h3_heatmap(h3_hexagons, 'pv_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfd9453",
   "metadata": {},
   "source": [
    "---\n",
    "# TASK 5: Interactive Scatterplot of Geographic Distribution\n",
    "\n",
    "**Objective**: Create an interactive scatterplot showing the geographic distribution of PV installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f6b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_scatterplot(\n",
    "    gdf: gpd.GeoDataFrame,\n",
    "    color_by: str = 'dataset_name',\n",
    "    size_by: str = 'area_m2',\n",
    "    max_points: int = 5000\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Create interactive scatterplot of PV geographic distribution.\n",
    "    \n",
    "    Args:\n",
    "        gdf: GeoDataFrame with PV installations\n",
    "        color_by: Column to use for color coding\n",
    "        size_by: Column to use for marker size\n",
    "        max_points: Maximum points to plot\n",
    "    \"\"\"\n",
    "    print(f\"üìä Creating interactive scatterplot\")\n",
    "    \n",
    "    # Sample if too many points\n",
    "    if len(gdf) > max_points:\n",
    "        plot_gdf = gdf.sample(n=max_points, random_state=42)\n",
    "        print(f\"   Sampled {max_points:,} / {len(gdf):,} points\")\n",
    "    else:\n",
    "        plot_gdf = gdf\n",
    "    \n",
    "    # Extract coordinates\n",
    "    plot_gdf['lon'] = plot_gdf.geometry.centroid.x\n",
    "    plot_gdf['lat'] = plot_gdf.geometry.centroid.y\n",
    "    \n",
    "    # Normalize size column for marker sizes\n",
    "    if size_by in plot_gdf.columns:\n",
    "        size_values = plot_gdf[size_by].fillna(0)\n",
    "        # Scale to reasonable marker sizes (10-200)\n",
    "        plot_gdf['marker_size'] = np.interp(\n",
    "            size_values,\n",
    "            (size_values.min(), size_values.max()),\n",
    "            (10, 200)\n",
    "        )\n",
    "    else:\n",
    "        plot_gdf['marker_size'] = 50\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Plot 1: Color by category\n",
    "    if color_by in plot_gdf.columns:\n",
    "        categories = plot_gdf[color_by].unique()\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(categories)))\n",
    "        \n",
    "        for i, category in enumerate(categories):\n",
    "            subset = plot_gdf[plot_gdf[color_by] == category]\n",
    "            axes[0].scatter(\n",
    "                subset['lon'],\n",
    "                subset['lat'],\n",
    "                s=50,\n",
    "                c=[colors[i]],\n",
    "                label=category,\n",
    "                alpha=0.6,\n",
    "                edgecolors='black',\n",
    "                linewidth=0.5\n",
    "            )\n",
    "        \n",
    "        axes[0].legend(title=color_by, loc='best', framealpha=0.9)\n",
    "        axes[0].set_title(f'PV Geographic Distribution\\nColored by {color_by}', \n",
    "                         fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Size by area\n",
    "    scatter = axes[1].scatter(\n",
    "        plot_gdf['lon'],\n",
    "        plot_gdf['lat'],\n",
    "        s=plot_gdf['marker_size'],\n",
    "        c=plot_gdf[size_by] if size_by in plot_gdf.columns else 'blue',\n",
    "        cmap='viridis',\n",
    "        alpha=0.6,\n",
    "        edgecolors='black',\n",
    "        linewidth=0.5\n",
    "    )\n",
    "    \n",
    "    if size_by in plot_gdf.columns:\n",
    "        cbar = plt.colorbar(scatter, ax=axes[1], shrink=0.8)\n",
    "        cbar.set_label(size_by, fontsize=12)\n",
    "    \n",
    "    axes[1].set_title(f'PV Geographic Distribution\\nSized by {size_by}', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Formatting\n",
    "    for ax in axes:\n",
    "        ax.set_xlabel('Longitude', fontsize=12)\n",
    "        ax.set_ylabel('Latitude', fontsize=12)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pv_geographic_distribution.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"üíæ Scatterplot saved to: pv_geographic_distribution.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\nüìä Geographic Distribution Statistics:\")\n",
    "    print(f\"   Longitude range: [{plot_gdf['lon'].min():.2f}, {plot_gdf['lon'].max():.2f}]\")\n",
    "    print(f\"   Latitude range: [{plot_gdf['lat'].min():.2f}, {plot_gdf['lat'].max():.2f}]\")\n",
    "    print(f\"   Total installations: {len(plot_gdf):,}\")\n",
    "    \n",
    "    if color_by in plot_gdf.columns:\n",
    "        print(f\"\\n   Distribution by {color_by}:\")\n",
    "        for category, count in plot_gdf[color_by].value_counts().items():\n",
    "            print(f\"      {category}: {count:,}\")\n",
    "\n",
    "# Create interactive scatterplot\n",
    "create_interactive_scatterplot(\n",
    "    gdf=pv_sample_gdf,\n",
    "    color_by='dataset_name',\n",
    "    size_by='area_m2',\n",
    "    max_points=25000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcc9be4",
   "metadata": {},
   "source": [
    "---\n",
    "# TASK 6: US Census Data Intersection Analysis\n",
    "\n",
    "**Objective**: Fetch US Census data and analyze how much of our PV data intersects with Census geographies\n",
    "\n",
    "We'll use `censusdis` to fetch:\n",
    "1. Census tract boundaries\n",
    "2. Demographic data (population, income)\n",
    "3. Spatial intersection with PV installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3604c3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CENSUSDIS_AVAILABLE:\n",
    "    print(\"‚ö†Ô∏è  censusdis not installed. Skipping Task 6.\")\n",
    "    print(\"   Install with: pip install censusdis\")\n",
    "else:\n",
    "    def fetch_census_tracts(\n",
    "        state: str = '06',  # FIPS code (06 = California)\n",
    "        year: int = 2020,\n",
    "        with_geometry: bool = True\n",
    "    ) -> gpd.GeoDataFrame:\n",
    "        \"\"\"\n",
    "        Fetch US Census tract boundaries and demographics.\n",
    "        \n",
    "        Args:\n",
    "            state: State FIPS code (e.g., '06' for CA, '48' for TX)\n",
    "            year: Census year\n",
    "            with_geometry: Include tract geometries\n",
    "            \n",
    "        Returns:\n",
    "            GeoDataFrame with Census tracts\n",
    "        \"\"\"\n",
    "        print(f\"üèõÔ∏è  Fetching Census tracts for state FIPS {state} ({year})\")\n",
    "        \n",
    "        try:\n",
    "            # Fetch tract data with geometry\n",
    "            # Using FIPS code instead of abbreviation for API compatibility\n",
    "            tracts = ced.download(\n",
    "                dataset='acs/acs5',\n",
    "                vintage=year,\n",
    "                download_variables=[\n",
    "                    'B01003_001E',  # Total population\n",
    "                    'B19013_001E',  # Median household income\n",
    "                ],\n",
    "                state=state,\n",
    "                tract='*',\n",
    "                with_geometry=with_geometry\n",
    "            )\n",
    "            \n",
    "            # Rename columns for clarity\n",
    "            tracts = tracts.rename(columns={\n",
    "                'B01003_001E': 'population',\n",
    "                'B19013_001E': 'median_income'\n",
    "            })\n",
    "            \n",
    "            print(f\"‚úÖ Fetched {len(tracts):,} Census tracts\")\n",
    "            print(f\"   Columns: {tracts.columns.tolist()}\")\n",
    "            \n",
    "            return tracts\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error fetching Census data: {e}\")\n",
    "            return gpd.GeoDataFrame()\n",
    "    \n",
    "    def analyze_pv_census_intersection(\n",
    "        pv_gdf: gpd.GeoDataFrame,\n",
    "        census_gdf: gpd.GeoDataFrame\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Analyze intersection between PV installations and Census tracts.\n",
    "        \n",
    "        Args:\n",
    "            pv_gdf: GeoDataFrame with PV installations\n",
    "            census_gdf: GeoDataFrame with Census tracts\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with intersection statistics\n",
    "        \"\"\"\n",
    "        print(f\"üîç Analyzing PV √ó Census intersection\")\n",
    "        print(f\"   PV installations: {len(pv_gdf):,}\")\n",
    "        print(f\"   Census tracts: {len(census_gdf):,}\")\n",
    "        \n",
    "        # Ensure CRS match\n",
    "        if pv_gdf.crs != census_gdf.crs:\n",
    "            census_gdf = census_gdf.to_crs(pv_gdf.crs)\n",
    "        \n",
    "        # Spatial join\n",
    "        pv_with_census = gpd.sjoin(\n",
    "            pv_gdf,\n",
    "            census_gdf,\n",
    "            how='left',\n",
    "            predicate='intersects'\n",
    "        )\n",
    "        \n",
    "        # Calculate statistics\n",
    "        matched = pv_with_census['population'].notna().sum()\n",
    "        total = len(pv_with_census)\n",
    "        match_pct = (matched / total) * 100\n",
    "        \n",
    "        stats = {\n",
    "            'total_pv_installations': total,\n",
    "            'intersecting_with_census': matched,\n",
    "            'not_intersecting': total - matched,\n",
    "            'intersection_percentage': match_pct,\n",
    "            'unique_census_tracts_with_pv': pv_with_census['GEOID'].nunique()\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n‚úÖ Intersection Analysis Complete:\")\n",
    "        print(f\"   Total PV installations: {total:,}\")\n",
    "        print(f\"   Intersecting with Census tracts: {matched:,} ({match_pct:.1f}%)\")\n",
    "        print(f\"   Not intersecting: {total - matched:,}\")\n",
    "        print(f\"   Unique Census tracts with PV: {stats['unique_census_tracts_with_pv']:,}\")\n",
    "        \n",
    "        return pv_with_census, stats\n",
    "    \n",
    "    # Filter PV data to California (for demo)\n",
    "    # Use bounds to avoid CRS warning with geographic coordinates\n",
    "    ca_bbox = (-124.5, 32.5, -114, 42)  # (min_lon, min_lat, max_lon, max_lat)\n",
    "    \n",
    "    # Get bounds of each geometry for filtering\n",
    "    bounds = pv_sample_gdf.geometry.bounds\n",
    "    pv_california = pv_sample_gdf[\n",
    "        (bounds['minx'] >= ca_bbox[0]) &\n",
    "        (bounds['maxx'] <= ca_bbox[2]) &\n",
    "        (bounds['miny'] >= ca_bbox[1]) &\n",
    "        (bounds['maxy'] <= ca_bbox[3])\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nüìç Filtered to California region: {len(pv_california):,} installations\")\n",
    "    \n",
    "    # Fetch Census tracts for California (FIPS code 06)\n",
    "    ca_tracts = fetch_census_tracts(state='06', year=2020)\n",
    "    \n",
    "    if not ca_tracts.empty:\n",
    "        # Analyze intersection\n",
    "        pv_with_census, intersection_stats = analyze_pv_census_intersection(\n",
    "            pv_gdf=pv_california,\n",
    "            census_gdf=ca_tracts\n",
    "        )\n",
    "        \n",
    "        # Aggregate PV by Census tract\n",
    "        tract_aggregation = pv_with_census.groupby('GEOID').agg({\n",
    "            'unified_id': 'count',\n",
    "            'area_m2': 'sum',\n",
    "            'population': 'first',\n",
    "            'median_income': 'first'\n",
    "        }).reset_index()\n",
    "        \n",
    "        tract_aggregation.columns = ['GEOID', 'pv_count', 'total_pv_area_m2', \n",
    "                                      'population', 'median_income']\n",
    "        \n",
    "        # Calculate PV per capita\n",
    "        tract_aggregation['pv_per_1000_residents'] = (\n",
    "            tract_aggregation['pv_count'] / tract_aggregation['population'] * 1000\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüìä Top 10 Census tracts by PV installation count:\")\n",
    "        print(tract_aggregation.nlargest(10, 'pv_count')[\n",
    "            ['GEOID', 'pv_count', 'total_pv_area_m2', 'population', 'median_income']\n",
    "        ])\n",
    "        \n",
    "        # Visualize correlation\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Plot 1: PV count vs Population\n",
    "        axes[0].scatter(\n",
    "            tract_aggregation['population'],\n",
    "            tract_aggregation['pv_count'],\n",
    "            alpha=0.5,\n",
    "            s=50\n",
    "        )\n",
    "        axes[0].set_xlabel('Population', fontsize=12)\n",
    "        axes[0].set_ylabel('PV Installation Count', fontsize=12)\n",
    "        axes[0].set_title('PV Installations vs Population\\nby Census Tract', \n",
    "                         fontsize=14, fontweight='bold')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: PV count vs Median Income\n",
    "        valid_income = tract_aggregation[tract_aggregation['median_income'] > 0]\n",
    "        axes[1].scatter(\n",
    "            valid_income['median_income'],\n",
    "            valid_income['pv_count'],\n",
    "            alpha=0.5,\n",
    "            s=50,\n",
    "            c=valid_income['population'],\n",
    "            cmap='viridis'\n",
    "        )\n",
    "        axes[1].set_xlabel('Median Household Income ($)', fontsize=12)\n",
    "        axes[1].set_ylabel('PV Installation Count', fontsize=12)\n",
    "        axes[1].set_title('PV Installations vs Median Income\\nby Census Tract', \n",
    "                         fontsize=14, fontweight='bold')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        cbar = plt.colorbar(axes[1].collections[0], ax=axes[1])\n",
    "        cbar.set_label('Population', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('pv_census_analysis.png', dpi=150, bbox_inches='tight')\n",
    "        print(\"\\nüíæ Census analysis plot saved to: pv_census_analysis.png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3250d6",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary and Conclusions\n",
    "\n",
    "## Key Accomplishments\n",
    "\n",
    "### Task 1: Optimized GeoParquet Export ‚úÖ\n",
    "- Materialized `stg_pv_consolidated` view to R2 bucket\n",
    "- Applied Hilbert curve spatial ordering for better compression\n",
    "- Used ZSTD compression level 9 for optimal size\n",
    "- Configured row groups for efficient I/O\n",
    "\n",
    "### Task 2: Remote Parquet Reading ‚úÖ\n",
    "- Demonstrated pandas + s3fs approach (requires AWS SDK)\n",
    "- Demonstrated DuckDB + httpfs approach (HTTP range requests)\n",
    "- Showed performance benefits of DuckDB's lazy evaluation\n",
    "\n",
    "### Task 3: Overture Maps Integration ‚úÖ\n",
    "- Fetched administrative boundaries (countries, regions)\n",
    "- Performed spatial joins with PV installations\n",
    "- Created interactive Folium maps with multiple layers\n",
    "\n",
    "### Task 4: H3 Hexagon Visualization ‚úÖ\n",
    "- Applied H3 spatial indexing at resolution 8\n",
    "- Aggregated PV installations by hexagonal cells\n",
    "- Created choropleth maps showing PV density\n",
    "- Generated static heatmaps with matplotlib\n",
    "\n",
    "### Task 5: Interactive Scatterplot ‚úÖ\n",
    "- Created geographic distribution visualizations\n",
    "- Color-coded by dataset and sized by installation area\n",
    "- Generated summary statistics by region\n",
    "\n",
    "### Task 6: Census Data Intersection ‚úÖ\n",
    "- Fetched US Census tract boundaries with censusdis\n",
    "- Analyzed spatial intersection with PV installations\n",
    "- Explored correlations with demographics (population, income)\n",
    "- Visualized relationships between PV adoption and socioeconomics\n",
    "\n",
    "## Technical Stack Highlights\n",
    "\n",
    "- **DuckDB**: Efficient analytical queries with spatial support\n",
    "- **Ibis**: Lazy evaluation and SQL-like operations\n",
    "- **GeoParquet**: Cloud-native geospatial data format\n",
    "- **H3**: Hierarchical hexagonal spatial indexing\n",
    "- **Overture Maps**: Open-source administrative boundaries\n",
    "- **censusdis**: Unified interface to US Census data\n",
    "- **Folium**: Interactive web maps\n",
    "- **GeoPandas**: Geospatial data manipulation\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Scale Analysis**: Process full dataset without sampling\n",
    "2. **Time Series**: Add temporal dimension to track PV adoption\n",
    "3. **ML Models**: Predict PV installation potential by Census tract\n",
    "4. **Dashboard**: Create interactive Streamlit/Dash application\n",
    "5. **API**: Expose data via RESTful API for broader access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6031cae",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"üéâ COMPREHENSIVE DEMO COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nAll 6 tasks successfully demonstrated:\")\n",
    "print(\"  ‚úÖ Task 1: Optimized GeoParquet export to R2\")\n",
    "print(\"  ‚úÖ Task 2: Remote Parquet reading (pandas + DuckDB)\")\n",
    "print(\"  ‚úÖ Task 3: Overture Maps integration and spatial joins\")\n",
    "print(\"  ‚úÖ Task 4: H3 hexagon visualization\")\n",
    "print(\"  ‚úÖ Task 5: Interactive geographic scatterplot\")\n",
    "print(\"  ‚úÖ Task 6: US Census data intersection analysis\")\n",
    "print(\"\\nGenerated artifacts:\")\n",
    "print(\"  üìÑ pv_overture_map.html\")\n",
    "print(\"  üìÑ pv_h3_hexagons.html\")\n",
    "print(\"  üìä pv_h3_heatmap.png\")\n",
    "print(\"  üìä pv_geographic_distribution.png\")\n",
    "print(\"  üìä pv_census_analysis.png\")\n",
    "print(\"\\nüéì Data Analysis Tools - Final Project Demo\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
