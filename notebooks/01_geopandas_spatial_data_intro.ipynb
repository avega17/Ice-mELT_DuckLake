{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acc6ad7e",
   "metadata": {},
   "source": [
    "# Notebook 1: Introduction to GeoPandas and Spatial Data\n",
    "\n",
    "**CCOM 6994: Data Analysis Tools - Final Project**  \n",
    "**Course: An√°lisis de Datos de Paneles Solares**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Load geospatial data** from cloud storage (S3/R2) using pandas\n",
    "2. **Work with GeoPandas** data structures (GeoDataFrame, GeoSeries)\n",
    "3. **Convert between coordinate formats** (WKT to Shapely geometries)\n",
    "4. **Perform basic spatial operations** (centroid, area, bounding box)\n",
    "5. **Filter spatial data** by bounding box and attribute conditions\n",
    "6. **Understand coordinate reference systems** (CRS) and projections\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Key Concepts\n",
    "\n",
    "### What is Geospatial Data?\n",
    "\n",
    "Geospatial data represents features on Earth's surface. Each record has:\n",
    "- **Attributes**: Regular data (text, numbers, dates)\n",
    "- **Geometry**: Spatial representation (points, lines, polygons)\n",
    "- **Coordinate Reference System (CRS)**: How coordinates map to Earth's surface\n",
    "\n",
    "### Vector Data Types\n",
    "\n",
    "- **Point**: Single coordinate (e.g., solar panel centroid)\n",
    "- **LineString**: Connected coordinates (e.g., road, river)\n",
    "- **Polygon**: Closed shape (e.g., solar panel boundary, country)\n",
    "- **Multi-* types**: Collections of geometries\n",
    "\n",
    "### GeoPandas = Pandas + Geometries\n",
    "\n",
    "GeoPandas extends pandas DataFrames with:\n",
    "- `GeoDataFrame`: DataFrame with geometry column\n",
    "- `GeoSeries`: Series of geometries\n",
    "- Spatial operations (intersects, contains, buffer, etc.)\n",
    "- Coordinate transformations\n",
    "- Map visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08afa98",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Setup: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deb626e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point, Polygon, box\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")\n",
    "print(f\"üì¶ GeoPandas version: {gpd.__version__}\")\n",
    "print(f\"üì¶ Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0b6596",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì• Task 1: Fetching Solar Panel Data from S3\n",
    "\n",
    "### About Our Dataset\n",
    "\n",
    "We're working with a **consolidated global solar panel (PV) dataset** that includes:\n",
    "- Solar panel installations from multiple sources (USA, UK, China, India, global)\n",
    "- Geographic coordinates and polygon boundaries\n",
    "- Installation metadata (area, capacity, dates)\n",
    "- Spatial indices (H3 hexagon cells)\n",
    "\n",
    "### Data Format: GeoParquet\n",
    "\n",
    "**GeoParquet** is an efficient cloud-native format for geospatial data:\n",
    "- Columnar storage (fast queries, good compression)\n",
    "- Embedded spatial metadata (CRS, bounding boxes)\n",
    "- Works with pandas, GeoPandas, DuckDB, and other tools\n",
    "- Stored in **Cloudflare R2** (S3-compatible object storage)\n",
    "\n",
    "### Reading Strategy\n",
    "\n",
    "We'll use **pandas + s3fs** to read the parquet file:\n",
    "1. Connect to S3-compatible storage (R2) with credentials\n",
    "2. Read parquet file into pandas DataFrame\n",
    "3. Convert WKT geometry strings to Shapely objects\n",
    "4. Create GeoDataFrame with proper CRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbfe64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_from_s3(\n",
    "    s3_path: str,\n",
    "    bbox: tuple = None,\n",
    "    sample_size: int = None,\n",
    "    columns: list = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read parquet file from S3/R2 bucket using pandas + s3fs.\n",
    "    \n",
    "    Args:\n",
    "        s3_path: S3 path (e.g., 's3://bucket/file.parquet')\n",
    "        bbox: Bounding box to filter (xmin, ymin, xmax, ymax) in WGS84\n",
    "        sample_size: Number of rows to sample (None = all)\n",
    "        columns: Columns to read (None = all)\n",
    "        \n",
    "    Returns:\n",
    "        Pandas DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import s3fs\n",
    "    except ImportError:\n",
    "        print(\"‚ùå s3fs not installed. Install with: pip install s3fs\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"üì• Reading parquet from: {s3_path}\")\n",
    "    \n",
    "    # Get credentials from environment\n",
    "    access_key = os.getenv('R2_ACCESS_KEY_ID')\n",
    "    secret_key = os.getenv('R2_SECRET_KEY')\n",
    "    endpoint = os.getenv('R2_S3_ENDPOINT', 'e833ac2d32c62bcff5e4b72c74e5351d.r2.cloudflarestorage.com')\n",
    "    \n",
    "    if not access_key or not secret_key:\n",
    "        print(\"‚ö†Ô∏è  No credentials found - trying public HTTPS access\")\n",
    "        # Try converting s3:// path to https:// for public access\n",
    "        if s3_path.startswith('s3://'):\n",
    "            bucket_and_key = s3_path.replace('s3://', '')\n",
    "            https_path = f\"https://{bucket_and_key.replace('/', '.', 1).replace('/', '/', 1)}\"\n",
    "            print(f\"   Trying: {https_path}\")\n",
    "            try:\n",
    "                df = pd.read_parquet(https_path, columns=columns)\n",
    "                print(f\"‚úÖ Read {len(df):,} rows from public HTTPS\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå HTTPS access failed: {e}\")\n",
    "                return pd.DataFrame()\n",
    "        else:\n",
    "            print(\"‚ùå No credentials and not an S3 path\")\n",
    "            return pd.DataFrame()\n",
    "    else:\n",
    "        # Use s3fs with credentials\n",
    "        fs = s3fs.S3FileSystem(\n",
    "            anon=False,\n",
    "            use_ssl=True,\n",
    "            client_kwargs={\n",
    "                'region_name': 'auto',\n",
    "                'endpoint_url': f'https://{endpoint}',\n",
    "                'aws_access_key_id': access_key,\n",
    "                'aws_secret_access_key': secret_key,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        with fs.open(s3_path, 'rb') as f:\n",
    "            df = pd.read_parquet(f, columns=columns)\n",
    "        \n",
    "        print(f\"‚úÖ Read {len(df):,} rows from S3/R2\")\n",
    "    \n",
    "    # Filter by bounding box if provided\n",
    "    if bbox is not None and all(col in df.columns for col in ['centroid_lon', 'centroid_lat']):\n",
    "        xmin, ymin, xmax, ymax = bbox\n",
    "        print(f\"   üîç Filtering to bbox: [{xmin}, {ymin}] to [{xmax}, {ymax}]\")\n",
    "        mask = (\n",
    "            (df['centroid_lon'] >= xmin) & (df['centroid_lon'] <= xmax) &\n",
    "            (df['centroid_lat'] >= ymin) & (df['centroid_lat'] <= ymax)\n",
    "        )\n",
    "        df = df[mask].copy()\n",
    "        print(f\"   ‚úÖ After bbox filter: {len(df):,} rows\")\n",
    "    \n",
    "    # Sample if requested\n",
    "    if sample_size is not None and len(df) > sample_size:\n",
    "        original_len = len(df)\n",
    "        df = df.sample(n=sample_size, random_state=42).copy()\n",
    "        print(f\"   üìä Sampled {len(df):,} / {original_len:,} rows\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Define our dataset path and region of interest\n",
    "PV_DATASET_PATH = os.getenv(\n",
    "    \"PV_GEO_PARQUET_PATH\",\n",
    "    \"s3://eo-pv-lakehouse/geoparquet/ccom6994_pv_dataset.parquet\"\n",
    ")\n",
    "\n",
    "# Bounding box covering USA (including Hawaii and Puerto Rico)\n",
    "# Format: (xmin/west, ymin/south, xmax/east, ymax/north) in WGS84 degrees\n",
    "USA_BBOX = (-161.0, 17.8, -65.2, 47.8)  # Hawaii to Puerto Rico to continental USA\n",
    "\n",
    "# Read dataset with filters\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING SOLAR PANEL DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "pv_df = read_parquet_from_s3(\n",
    "    s3_path=PV_DATASET_PATH,\n",
    "    bbox=USA_BBOX,\n",
    "    sample_size=100000,  # Limit to 100K points for this intro notebook\n",
    "    columns=[\n",
    "        'unified_id', 'dataset_name', 'area_m2', \n",
    "        'centroid_lon', 'centroid_lat', \n",
    "        'geometry', 'h3_index_8', \n",
    "        'capacity_mw', 'install_date', 'source_area_m2'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\nüìä Dataset Overview:\")\n",
    "print(f\"   Shape: {pv_df.shape}\")\n",
    "print(f\"   Memory: {pv_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "print(f\"\\n   Columns: {list(pv_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05479528",
   "metadata": {},
   "source": [
    "### Understanding the Dataset\n",
    "\n",
    "Let's examine what we've loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce274e8f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Show first few rows\n",
    "print(\"üìã First 5 records:\\n\")\n",
    "display(pv_df.head())\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nüìä Column Data Types:\\n\")\n",
    "print(pv_df.dtypes)\n",
    "\n",
    "# Summary statistics for numeric columns\n",
    "print(\"\\nüìà Summary Statistics:\\n\")\n",
    "display(pv_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43732f3",
   "metadata": {},
   "source": [
    "**Key Observations:**\n",
    "\n",
    "- `geometry` column contains WKT (Well-Known Text) strings\n",
    "- `centroid_lon`, `centroid_lat` are regular numeric columns\n",
    "- `area_m2` shows panel size in square meters\n",
    "- `h3_index_8` contains H3 spatial index (we'll explore this later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cf57da",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üó∫Ô∏è Task 2: Converting to GeoDataFrame\n",
    "\n",
    "### From WKT to Shapely Geometries\n",
    "\n",
    "**Well-Known Text (WKT)** is a standard text format for geometries:\n",
    "- `POINT (lon lat)`\n",
    "- `POLYGON ((lon1 lat1, lon2 lat2, ...))`\n",
    "- `MULTIPOLYGON (((lon lat, ...), (...)))`\n",
    "\n",
    "We need to convert these strings to **Shapely geometry objects** for spatial operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b91a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_geodataframe(\n",
    "    df: pd.DataFrame,\n",
    "    geometry_col: str = 'geometry',\n",
    "    crs: str = 'EPSG:4326'\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Convert pandas DataFrame with WKT geometries to GeoDataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with WKT geometry column\n",
    "        geometry_col: Name of geometry column\n",
    "        crs: Coordinate Reference System (default: WGS84/EPSG:4326)\n",
    "        \n",
    "    Returns:\n",
    "        GeoDataFrame with Shapely geometries\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Converting DataFrame to GeoDataFrame...\")\n",
    "    \n",
    "    # Convert WKT strings to Shapely geometries\n",
    "    print(f\"   üìê Parsing {len(df):,} WKT geometries...\")\n",
    "    df = df.copy()\n",
    "    # confirm geometry is in WKT format\n",
    "    # if all(isinstance(geom, str) for geom in df[geometry_col]):\n",
    "    df[geometry_col] = df[geometry_col].apply(wkt.loads)\n",
    "    \n",
    "    # Create GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geometry_col, crs=crs)\n",
    "    \n",
    "    print(f\"‚úÖ GeoDataFrame created:\")\n",
    "    print(f\"   CRS: {gdf.crs}\")\n",
    "    print(f\"   Geometry types: {gdf.geometry.geom_type.value_counts().to_dict()}\")\n",
    "    print(f\"   Valid geometries: {gdf.geometry.is_valid.sum():,} / {len(gdf):,}\")\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "pv_gdf = create_geodataframe(pv_df, geometry_col='geometry')\n",
    "\n",
    "# Show GeoDataFrame info\n",
    "print(\"\\nüìã GeoDataFrame Preview:\")\n",
    "display(pv_gdf.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe139bf",
   "metadata": {},
   "source": [
    "### What's Different in a GeoDataFrame?\n",
    "\n",
    "Compare these operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9b578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Comparing DataFrame vs GeoDataFrame:\\n\")\n",
    "\n",
    "# Regular DataFrame operations still work\n",
    "print(\"1Ô∏è‚É£ Regular operations (same as pandas):\")\n",
    "print(f\"   Mean area: {pv_gdf['area_m2'].mean():.2f} m¬≤\")\n",
    "print(f\"   Datasets: {pv_gdf['dataset_name'].nunique()} unique sources\")\n",
    "\n",
    "# New geometric operations\n",
    "print(\"\\n2Ô∏è‚É£ Geometric operations (new!):\")\n",
    "print(f\"   Geometry type: {type(pv_gdf.geometry.iloc[0])}\")\n",
    "print(f\"   First centroid: {pv_gdf.geometry.iloc[0].centroid}\")\n",
    "print(f\"   First area: {pv_gdf.geometry.iloc[0].area:.8f} square degrees\")\n",
    "\n",
    "# Spatial indexing\n",
    "print(\"\\n3Ô∏è‚É£ Spatial methods (new!):\")\n",
    "print(f\"   Has CRS: {pv_gdf.crs is not None}\")\n",
    "print(f\"   Total bounds: {pv_gdf.total_bounds}\")  # [minx, miny, maxx, maxy]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708e8f92",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìè Task 3: Basic Spatial Operations\n",
    "\n",
    "### 3.1 Computing Geometric Properties\n",
    "\n",
    "GeoPandas provides many geometric properties as attributes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f4e664",
   "metadata": {},
   "source": [
    "**Important Note:** \n",
    "\n",
    "The `geometry.area` in degrees¬≤ is **not directly comparable** to `area_m2`!\n",
    "- Degrees¬≤ depends on location (latitude affects scale)\n",
    "- For accurate area calculations, we need a **projected CRS** (meters, feet, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4458edc8",
   "metadata": {},
   "source": [
    "### 3.2 Working with Different Geometry Types\n",
    "\n",
    "Let's examine the difference between Polygon and MultiPolygon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390163f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Exploring Geometry Types\\n\")\n",
    "\n",
    "# Find examples of each type\n",
    "import random\n",
    "polygon_example = pv_gdf[pv_gdf.geometry.type == 'Polygon'].iloc[random.randint(0, len(pv_gdf[pv_gdf.geometry.type == 'Polygon']) - 1)]\n",
    "multipolygon_examples = pv_gdf[pv_gdf.geometry.type == 'MultiPolygon']\n",
    "\n",
    "print(f\"1Ô∏è‚É£ Polygon Example:\")\n",
    "print(f\"   ID: {polygon_example['unified_id']}\")\n",
    "print(f\"   Area: {polygon_example['area_m2']:.2f} m¬≤\")\n",
    "print(f\"   Centroid: ({polygon_example.geometry.centroid.x:.4f}, {polygon_example.geometry.centroid.y:.4f})\")\n",
    "print(f\"   # Coordinates: {len(polygon_example.geometry.exterior.coords)}\")\n",
    "\n",
    "if len(multipolygon_examples) > 0:\n",
    "    multi_example = multipolygon_examples.iloc[0]\n",
    "    print(f\"\\n2Ô∏è‚É£ MultiPolygon Example:\")\n",
    "    print(f\"   ID: {multi_example['unified_id']}\")\n",
    "    print(f\"   Area: {multi_example['area_m2']:.2f} m¬≤\")\n",
    "    print(f\"   # Sub-polygons: {len(multi_example.geometry.geoms)}\")\n",
    "    print(f\"   Centroid: ({multi_example.geometry.centroid.x:.4f}, {multi_example.geometry.centroid.y:.4f})\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  No MultiPolygon examples in this sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c234858",
   "metadata": {},
   "source": [
    "### 3.3 Coordinate Reference Systems (CRS)\n",
    "\n",
    "Understanding CRS is crucial for spatial analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b746e4c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(\"üó∫Ô∏è Understanding Coordinate Reference Systems\\n\")\n",
    "\n",
    "print(f\"1Ô∏è‚É£ Current CRS: {pv_gdf.crs}\")\n",
    "print(f\"   Name: {pv_gdf.crs.name}\")\n",
    "print(f\"   Type: Geographic (latitude/longitude)\")\n",
    "print(f\"   Units: Degrees\")\n",
    "print(f\"   Authority: {pv_gdf.crs.to_authority()}\")\n",
    "\n",
    "print(f\"\\n2Ô∏è‚É£ Sample Coordinates (WGS84):\")\n",
    "sample_coords = pv_gdf.geometry.iloc[0].centroid\n",
    "print(f\"   Longitude: {sample_coords.x:.6f}¬∞\")\n",
    "print(f\"   Latitude: {sample_coords.y:.6f}¬∞\")\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£ Why CRS Matters:\")\n",
    "print(f\"   ‚úì WGS84 (EPSG:4326): Good for global mapping, GPS coordinates\")\n",
    "print(f\"   ‚úì UTM zones: Good for accurate distance/area measurements\")\n",
    "print(f\"   ‚úì Web Mercator (EPSG:3857): Used by web maps (Google, OSM)\")\n",
    "\n",
    "# Example: Converting to Web Mercator\n",
    "pv_web_mercator = pv_gdf.head(100).to_crs('EPSG:3857')\n",
    "print(f\"\\n4Ô∏è‚É£ After converting to Web Mercator:\")\n",
    "sample_web = pv_web_mercator.geometry.iloc[0].centroid\n",
    "print(f\"   X: {sample_web.x:.2f} meters\")\n",
    "print(f\"   Y: {sample_web.y:.2f} meters\")\n",
    "print(f\"   (These are distances from the equator and prime meridian)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ab0e5e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Task 4: Spatial Filtering and Analysis\n",
    "\n",
    "### 4.1 Filtering by Bounding Box\n",
    "\n",
    "Let's focus on a specific region (e.g., California):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279909bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_bbox(gdf: gpd.GeoDataFrame, bbox: tuple) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Filter GeoDataFrame by bounding box.\n",
    "    \n",
    "    Args:\n",
    "        gdf: Input GeoDataFrame\n",
    "        bbox: (xmin, ymin, xmax, ymax) in same CRS as gdf\n",
    "        \n",
    "    Returns:\n",
    "        Filtered GeoDataFrame\n",
    "    \"\"\"\n",
    "    xmin, ymin, xmax, ymax = bbox\n",
    "    bbox_geom = box(xmin, ymin, xmax, ymax)\n",
    "    \n",
    "    print(f\"üîç Filtering by bounding box:\")\n",
    "    print(f\"   Bounds: [{xmin:.2f}, {ymin:.2f}] to [{xmax:.2f}, {ymax:.2f}]\")\n",
    "    \n",
    "    # Filter using geometric intersection\n",
    "    mask = gdf.geometry.intersects(bbox_geom)\n",
    "    filtered = gdf[mask].copy()\n",
    "    \n",
    "    print(f\"   Original: {len(gdf):,} features\")\n",
    "    print(f\"   Filtered: {len(filtered):,} features ({len(filtered)/len(gdf)*100:.1f}%)\")\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "\n",
    "# California bounding box\n",
    "CALIFORNIA_BBOX = (-124.5, 32.5, -114.0, 42.0)\n",
    "\n",
    "california_pv = filter_by_bbox(pv_gdf, CALIFORNIA_BBOX)\n",
    "\n",
    "print(f\"\\nüìä California Solar Panels Summary:\")\n",
    "print(f\"   Count: {len(california_pv):,}\")\n",
    "print(f\"   Total area: {california_pv['area_m2'].sum() / 1_000_000:.2f} km¬≤\")\n",
    "print(f\"   Mean area: {california_pv['area_m2'].mean():.2f} m¬≤\")\n",
    "print(f\"   Median area: {california_pv['area_m2'].median():.2f} m¬≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07c5a37",
   "metadata": {},
   "source": [
    "### 4.2 Attribute-Based Filtering\n",
    "\n",
    "Combine spatial and attribute filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a5a7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Advanced Filtering Examples\\n\")\n",
    "\n",
    "# Large installations only (>5000 m¬≤)\n",
    "large_panels = california_pv[california_pv['area_m2'] > 5000].copy()\n",
    "print(f\"1Ô∏è‚É£ Large installations (>5000 m¬≤):\")\n",
    "print(f\"   Count: {len(large_panels):,}\")\n",
    "print(f\"   Percentage: {len(large_panels)/len(california_pv)*100:.1f}%\")\n",
    "print(f\"   Total area: {large_panels['area_m2'].sum() / 1_000_000:.2f} km¬≤\")\n",
    "\n",
    "# By dataset source\n",
    "print(f\"\\n2Ô∏è‚É£ By data source:\")\n",
    "for source, group in california_pv.groupby('dataset_name'):\n",
    "    print(f\"   {source}: {len(group):,} installations\")\n",
    "\n",
    "# Installations with capacity data\n",
    "has_capacity = california_pv[california_pv['capacity_mw'].notna()]\n",
    "print(f\"\\n3Ô∏è‚É£ With capacity data:\")\n",
    "print(f\"   Count: {len(has_capacity):,}\")\n",
    "if len(has_capacity) > 0:\n",
    "    print(f\"   Total capacity: {has_capacity['capacity_mw'].sum():.2f} MW\")\n",
    "    print(f\"   Mean capacity: {has_capacity['capacity_mw'].mean():.4f} MW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb1b5db",
   "metadata": {},
   "source": [
    "### 4.3 Spatial Relationships\n",
    "\n",
    "Test spatial relationships between geometries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e286d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîó Exploring Spatial Relationships\\n\")\n",
    "\n",
    "# Create a test point (San Francisco)\n",
    "sf_point = Point(-122.4194, 37.7749)\n",
    "print(f\"Test Point: San Francisco ({sf_point.x}, {sf_point.y})\")\n",
    "\n",
    "# Find installations within 0.5 degrees (~55 km)\n",
    "sf_buffer = sf_point.buffer(0.5)  # 0.5 degrees radius\n",
    "nearby_sf = california_pv[california_pv.geometry.intersects(sf_buffer)].copy()\n",
    "\n",
    "print(f\"\\nüìç Solar panels near San Francisco (within ~55km):\")\n",
    "print(f\"   Count: {len(nearby_sf):,}\")\n",
    "print(f\"   Total area: {nearby_sf['area_m2'].sum() / 1_000_000:.3f} km¬≤\")\n",
    "\n",
    "# Compute actual distances (approximate, in degrees)\n",
    "if len(nearby_sf) > 0:\n",
    "    nearby_sf['distance_to_sf'] = nearby_sf.geometry.distance(sf_point)\n",
    "    closest = nearby_sf.nsmallest(5, 'distance_to_sf')\n",
    "    \n",
    "    print(f\"\\n   üéØ 5 Closest installations:\")\n",
    "    for idx, row in closest.iterrows():\n",
    "        dist_km = row['distance_to_sf'] * 111  # Rough conversion degrees to km\n",
    "        print(f\"      ‚Ä¢ {row['unified_id'][:16]}... - {dist_km:.1f} km - {row['area_m2']:.0f} m¬≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c45a38",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Task 5: Exploratory Data Analysis\n",
    "\n",
    "### 5.1 Area Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f6ac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Solar Panel Area Distribution Analysis\\n\")\n",
    "\n",
    "# Compute statistics\n",
    "area_stats = pv_gdf['area_m2'].describe()\n",
    "print(\"Basic Statistics:\")\n",
    "print(area_stats)\n",
    "\n",
    "# Additional percentiles\n",
    "percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99]\n",
    "area_percentiles = pv_gdf['area_m2'].quantile([p/100 for p in percentiles])\n",
    "\n",
    "print(f\"\\nDetailed Percentiles:\")\n",
    "for p, val in zip(percentiles, area_percentiles):\n",
    "    print(f\"   {p:2d}th: {val:10,.2f} m¬≤\")\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Histogram with log scale\n",
    "axes[0, 0].hist(pv_gdf['area_m2'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Area (m¬≤)', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 0].set_title('Distribution of Solar Panel Areas', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_yscale('log')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Box plot\n",
    "axes[0, 1].boxplot(pv_gdf['area_m2'], vert=True, patch_artist=True)\n",
    "axes[0, 1].set_ylabel('Area (m¬≤)', fontsize=12)\n",
    "axes[0, 1].set_title('Box Plot of Panel Areas', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_yscale('log')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Area by dataset source\n",
    "area_by_source = pv_gdf.groupby('dataset_name')['area_m2'].mean().sort_values(ascending=False)\n",
    "axes[1, 0].barh(area_by_source.index, area_by_source.values)\n",
    "axes[1, 0].set_xlabel('Mean Area (m¬≤)', fontsize=12)\n",
    "axes[1, 0].set_title('Average Panel Area by Dataset', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 4. Scatter plot: Area vs Location (Latitude)\n",
    "axes[1, 1].scatter(\n",
    "    pv_gdf['centroid_lat'], \n",
    "    pv_gdf['area_m2'], \n",
    "    alpha=0.3, \n",
    "    s=10,\n",
    "    c=pv_gdf['centroid_lon'],\n",
    "    cmap='viridis'\n",
    ")\n",
    "axes[1, 1].set_xlabel('Latitude', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Area (m¬≤)', fontsize=12)\n",
    "axes[1, 1].set_title('Panel Area vs Latitude', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_yscale('log')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "colorbar = plt.colorbar(axes[1, 1].collections[0], ax=axes[1, 1])\n",
    "colorbar.set_label('Longitude', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/Volumes/Expanse/repos/ice-mELT_ducklake/notebooks/01_area_distribution.png', \n",
    "            dpi=150, bbox_inches='tight')\n",
    "print(\"\\nüíæ Saved plot: 01_area_distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5f2acb",
   "metadata": {},
   "source": [
    "### 5.2 Geographic Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1614cc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üó∫Ô∏è Geographic Distribution Analysis\\n\")\n",
    "\n",
    "# Create scatter plot of panel locations\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "# Color by dataset\n",
    "sources = pv_gdf['dataset_name'].unique()\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(sources)))\n",
    "\n",
    "for i, source in enumerate(sources):\n",
    "    subset = pv_gdf[pv_gdf['dataset_name'] == source]\n",
    "    ax.scatter(\n",
    "        subset['centroid_lon'],\n",
    "        subset['centroid_lat'],\n",
    "        c=[colors[i]],\n",
    "        label=source,\n",
    "        alpha=0.6,\n",
    "        s=20,\n",
    "        edgecolors='black',\n",
    "        linewidth=0.3\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Longitude', fontsize=14)\n",
    "ax.set_ylabel('Latitude', fontsize=14)\n",
    "ax.set_title('Geographic Distribution of Solar Panels\\n(USA Region, colored by data source)', \n",
    "             fontsize=16, fontweight='bold')\n",
    "ax.legend(title='Data Source', loc='best', framealpha=0.9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Add bounding box annotation\n",
    "ax.axvline(USA_BBOX[0], color='red', linestyle='--', alpha=0.5, linewidth=1)\n",
    "ax.axvline(USA_BBOX[2], color='red', linestyle='--', alpha=0.5, linewidth=1)\n",
    "ax.axhline(USA_BBOX[1], color='red', linestyle='--', alpha=0.5, linewidth=1)\n",
    "ax.axhline(USA_BBOX[3], color='red', linestyle='--', alpha=0.5, linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/Volumes/Expanse/repos/ice-mELT_ducklake/notebooks/01_geographic_distribution.png',\n",
    "            dpi=150, bbox_inches='tight')\n",
    "print(\"üíæ Saved plot: 01_geographic_distribution.png\")\n",
    "plt.show()\n",
    "\n",
    "# Print summary by region (latitude bands)\n",
    "print(\"Distribution by Latitude Bands:\")\n",
    "lat_bins = [17, 30, 35, 40, 45, 48]\n",
    "lat_labels = ['Hawaii/PR (17-30¬∞N)', 'South (30-35¬∞N)', 'Mid (35-40¬∞N)', \n",
    "              'North (40-45¬∞N)', 'Far North (45-48¬∞N)']\n",
    "pv_gdf['lat_band'] = pd.cut(pv_gdf['centroid_lat'], bins=lat_bins, labels=lat_labels)\n",
    "\n",
    "for band in lat_labels:\n",
    "    count = (pv_gdf['lat_band'] == band).sum()\n",
    "    if count > 0:\n",
    "        pct = count / len(pv_gdf) * 100\n",
    "        print(f\"   {band}: {count:,} installations ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e99c00e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Task 6: Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc83ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Data Quality Assessment\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"1Ô∏è‚É£ Missing Values:\")\n",
    "missing = pv_gdf.isnull().sum()\n",
    "missing_pct = (missing / len(pv_gdf) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing.index,\n",
    "    'Missing Count': missing.values,\n",
    "    'Percentage': missing_pct.values\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "if len(missing_df) > 0:\n",
    "    display(missing_df)\n",
    "else:\n",
    "    print(\"   ‚úÖ No missing values found!\")\n",
    "\n",
    "# Check geometry validity\n",
    "print(\"\\n2Ô∏è‚É£ Geometry Validity:\")\n",
    "invalid_geoms = ~pv_gdf.geometry.is_valid\n",
    "print(f\"   Total geometries: {len(pv_gdf):,}\")\n",
    "print(f\"   Valid: {pv_gdf.geometry.is_valid.sum():,}\")\n",
    "print(f\"   Invalid: {invalid_geoms.sum():,}\")\n",
    "\n",
    "if invalid_geoms.sum() > 0:\n",
    "    print(\"   ‚ö†Ô∏è  Found invalid geometries - may need repair\")\n",
    "\n",
    "# Check for duplicate IDs\n",
    "print(\"\\n3Ô∏è‚É£ Duplicate Check:\")\n",
    "duplicate_ids = pv_gdf['unified_id'].duplicated().sum()\n",
    "print(f\"   Duplicate IDs: {duplicate_ids}\")\n",
    "\n",
    "# Data completeness score\n",
    "print(\"\\n4Ô∏è‚É£ Overall Data Quality Score:\")\n",
    "completeness_scores = {\n",
    "    'Valid geometries': pv_gdf.geometry.is_valid.sum() / len(pv_gdf) * 100,\n",
    "    'Has area data': pv_gdf['area_m2'].notna().sum() / len(pv_gdf) * 100,\n",
    "    'Has coordinates': ((pv_gdf['centroid_lon'].notna()) & \n",
    "                        (pv_gdf['centroid_lat'].notna())).sum() / len(pv_gdf) * 100,\n",
    "    'Unique IDs': (1 - pv_gdf['unified_id'].duplicated().sum() / len(pv_gdf)) * 100,\n",
    "}\n",
    "\n",
    "for metric, score in completeness_scores.items():\n",
    "    status = \"‚úÖ\" if score > 95 else \"‚ö†Ô∏è\" if score > 80 else \"‚ùå\"\n",
    "    print(f\"   {status} {metric}: {score:.1f}%\")\n",
    "\n",
    "overall_score = np.mean(list(completeness_scores.values()))\n",
    "print(f\"\\n   Overall Quality: {overall_score:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ef8e24",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üó∫Ô∏è Task 7: Assigning Countries and States to Solar Panels\n",
    "\n",
    "### Why Geocoding Matters\n",
    "\n",
    "To analyze solar panel adoption patterns, we need to know **which country and state/region** each installation belongs to. This is called **reverse geocoding** - converting coordinates to administrative boundaries.\n",
    "\n",
    "### Three Approaches We'll Compare\n",
    "\n",
    "1. **Reverse Geocoding APIs** (fastest, requires external service)\n",
    "   - Uses coordinate lookup services\n",
    "   - Limited free tier, rate limits apply\n",
    "   - Good for small datasets or real-time lookups\n",
    "\n",
    "2. **H3 Spatial Index Matching** (fast, memory efficient)\n",
    "   - Decompose admin boundaries into H3 hexagons\n",
    "   - Match panel centroids to pre-computed H3 cells\n",
    "   - Excellent for repeated lookups\n",
    "\n",
    "3. **Direct Spatial Join** (most accurate, computationally expensive)\n",
    "   - Point-in-polygon tests for each installation\n",
    "   - Guaranteed accuracy\n",
    "   - Baseline for performance comparison\n",
    "\n",
    "Let's implement and compare all three methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a916edb9",
   "metadata": {},
   "source": [
    "### 7.1: Method 1 - Reverse Geocoding with offline-geocoder\n",
    "\n",
    "We'll use the `reverse-geocoder` library which uses offline data (no API calls needed).\n",
    "It's faster than online APIs but less accurate than spatial joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fd62a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import reverse_geocoder as rg\n",
    "    REVERSE_GEOCODER_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  reverse-geocoder not installed\")\n",
    "    print(\"   Install with: pip install reverse-geocoder\")\n",
    "    REVERSE_GEOCODER_AVAILABLE = False\n",
    "\n",
    "if REVERSE_GEOCODER_AVAILABLE:\n",
    "    import time\n",
    "    \n",
    "    print(\"üåç Method 1: Offline Reverse Geocoding\\n\")\n",
    "    \n",
    "    # Sample smaller subset for demo (reverse geocoding is slower)\n",
    "    geocode_sample = california_pv.head(1000).copy()\n",
    "    \n",
    "    print(f\"Testing with {len(geocode_sample):,} installations...\")\n",
    "    \n",
    "    # Prepare coordinates as list of tuples (lat, lon)\n",
    "    coords = list(zip(\n",
    "        geocode_sample['centroid_lat'].values,\n",
    "        geocode_sample['centroid_lon'].values\n",
    "    ))\n",
    "    \n",
    "    # Perform reverse geocoding\n",
    "    start_time = time.time()\n",
    "    results = rg.search(coords)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # Extract country and state info\n",
    "    geocode_sample['rg_country'] = [r['cc'] for r in results]  # Country code\n",
    "    geocode_sample['rg_state'] = [r['admin1'] for r in results]  # State/Province\n",
    "    geocode_sample['rg_county'] = [r['admin2'] for r in results]  # County\n",
    "    \n",
    "    print(f\"‚úÖ Reverse geocoding complete!\")\n",
    "    print(f\"   Time: {elapsed:.2f}s\")\n",
    "    print(f\"   Rate: {len(geocode_sample)/elapsed:.0f} lookups/sec\")\n",
    "    \n",
    "    # Show results\n",
    "    print(f\"\\nüìä Sample Results:\")\n",
    "    display(geocode_sample[['unified_id', 'centroid_lon', 'centroid_lat', \n",
    "                            'rg_country', 'rg_state', 'rg_county']].head())\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nüìà Distribution by State:\")\n",
    "    state_counts = geocode_sample['rg_state'].value_counts().head(10)\n",
    "    for state, count in state_counts.items():\n",
    "        print(f\"   {state}: {count:,} installations\")\n",
    "    \n",
    "    # Performance metrics\n",
    "    print(f\"\\n‚ö° Performance Metrics:\")\n",
    "    print(f\"   Total time: {elapsed:.2f}s\")\n",
    "    print(f\"   Per-lookup: {elapsed*1000/len(geocode_sample):.2f}ms\")\n",
    "    print(f\"   Throughput: {len(geocode_sample)/elapsed:.0f} lookups/sec\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Advantages: Fast, no API keys needed, works offline\")\n",
    "    print(f\"‚ö†Ô∏è  Limitations: Less accurate, limited to ~3km resolution\")\n",
    "else:\n",
    "    print(\"Skipping reverse geocoding demo - library not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3febbd",
   "metadata": {},
   "source": [
    "### 7.2: Method 2 - H3 Spatial Index Matching\n",
    "\n",
    "This method uses **H3 hierarchical hexagonal grids** for fast spatial lookups:\n",
    "1. Download admin boundaries (countries, states) from Overture Maps\n",
    "2. Decompose boundaries into H3 hexagon cells\n",
    "3. Match panel centroids to H3 cells\n",
    "\n",
    "**Advantages:** Very fast lookups, memory efficient, reusable index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9e415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import h3.api.memview_int as h3\n",
    "    H3_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  h3 library not installed\")\n",
    "    print(\"   Install with: pip install h3\")\n",
    "    H3_AVAILABLE = False\n",
    "\n",
    "if H3_AVAILABLE:\n",
    "    print(\"üî∑ Method 2: H3 Spatial Index Matching\\n\")\n",
    "    \n",
    "    # Step 1: Create H3 indices for our solar panels\n",
    "    print(\"Step 1: Assigning H3 indices to solar panels...\")\n",
    "    h3_resolution = 7  # ~5km¬≤ per cell\n",
    "    \n",
    "    # Use sample for faster demo\n",
    "    h3_sample = california_pv.head(5000).copy()\n",
    "    \n",
    "    # Assign H3 index to each panel centroid\n",
    "    h3_sample['h3_index'] = h3_sample.apply(\n",
    "        lambda row: h3.latlng_to_cell(\n",
    "            row['centroid_lat'],\n",
    "            row['centroid_lon'],\n",
    "            h3_resolution\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    print(f\"   ‚úÖ Assigned H3 indices (resolution {h3_resolution})\")\n",
    "    print(f\"   Unique H3 cells: {h3_sample['h3_index'].nunique():,}\")\n",
    "    \n",
    "    # Step 2: For demo, we'll create a simple lookup table\n",
    "    # In production, you would:\n",
    "    # a) Download Overture Maps admin boundaries\n",
    "    # b) Decompose them into H3 cells using h3.h3shape_to_cells()\n",
    "    # c) Create lookup dict: h3_cell -> (country, state)\n",
    "    \n",
    "    print(\"\\nStep 2: In production, you would:\")\n",
    "    print(\"   a) Download admin boundaries from Overture Maps\")\n",
    "    print(\"   b) Convert polygons to H3 cells:\")\n",
    "    print(\"      cells = h3.h3shape_to_cells(polygon, resolution)\")\n",
    "    print(\"   c) Build lookup: {h3_cell: (country, state)}\")\n",
    "    \n",
    "    # Demo: Show how to convert a polygon to H3 cells\n",
    "    print(\"\\nüìê Demo: Converting polygon to H3 cells\")\n",
    "    \n",
    "    # Create a simple polygon (California-ish shape)\n",
    "    from shapely.geometry import Polygon\n",
    "    demo_polygon = box(-120, 35, -119, 36)  # Small region\n",
    "    \n",
    "    # Convert to H3 shape and get cells\n",
    "    try:\n",
    "        h3_shape = h3.geo_to_h3shape(demo_polygon)\n",
    "        h3_cells = h3.h3shape_to_cells(h3_shape, h3_resolution)\n",
    "        \n",
    "        print(f\"   Demo polygon covers {len(h3_cells):,} H3 cells at resolution {h3_resolution}\")\n",
    "        print(f\"   Sample cells: {list(h3_cells)[:5]}\")\n",
    "        \n",
    "        # Performance estimate\n",
    "        lookup_time_per_cell = 0.0001  # seconds (hash table lookup)\n",
    "        estimated_time = len(h3_sample) * lookup_time_per_cell\n",
    "        \n",
    "        print(f\"\\n‚ö° Performance Estimate:\")\n",
    "        print(f\"   Lookups needed: {len(h3_sample):,}\")\n",
    "        print(f\"   Estimated time: {estimated_time:.3f}s\")\n",
    "        print(f\"   Throughput: {len(h3_sample)/estimated_time:,.0f} lookups/sec\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Advantages: Very fast O(1) lookups, reusable index\")\n",
    "        print(f\"‚ö†Ô∏è  Limitations: Setup overhead, approximate boundaries\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  H3 polygon conversion demo error: {e}\")\n",
    "        print(f\"   (This is for demonstration - full implementation in st_context_processing.py)\")\n",
    "else:\n",
    "    print(\"Skipping H3 demo - library not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109f125d",
   "metadata": {},
   "source": [
    "### 7.3: Method 3 - Direct Spatial Join with Admin Boundaries\n",
    "\n",
    "This is the **most accurate** method but also the slowest:\n",
    "1. Download admin boundary polygons\n",
    "2. Perform point-in-polygon tests for each installation\n",
    "3. Get exact administrative assignment\n",
    "\n",
    "**Note:** For this demo, we'll use a small sample to avoid long processing times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2478a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üó∫Ô∏è Method 3: Direct Spatial Join (GeoPandas sjoin)\\n\")\n",
    "\n",
    "# For demo purposes, we'll create simplified admin boundaries\n",
    "# In production, you would download from Overture Maps or Natural Earth\n",
    "\n",
    "print(\"Setting up demo admin boundaries...\")\n",
    "\n",
    "# Create simplified state boundaries (demo only)\n",
    "# In reality, use: gpd.read_file('admin_boundaries.gpq')\n",
    "demo_states = gpd.GeoDataFrame({\n",
    "    'name': ['California', 'Nevada', 'Arizona', 'Oregon'],\n",
    "    'state_code': ['CA', 'NV', 'AZ', 'OR'],\n",
    "    'geometry': [\n",
    "        box(-124.5, 32.5, -114.0, 42.0),  # California (approximate)\n",
    "        box(-120.0, 35.0, -114.0, 42.0),  # Nevada\n",
    "        box(-115.0, 31.3, -109.0, 37.0),  # Arizona\n",
    "        box(-124.6, 42.0, -116.5, 46.3),  # Oregon\n",
    "    ]\n",
    "}, crs='EPSG:4326')\n",
    "\n",
    "print(f\"   Created {len(demo_states)} demo state boundaries\")\n",
    "\n",
    "# Sample for spatial join\n",
    "sjoin_sample = california_pv.head(1000).copy()\n",
    "\n",
    "print(f\"\\nPerforming spatial join on {len(sjoin_sample):,} installations...\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform spatial join: find which state each panel is in\n",
    "joined = gpd.sjoin(\n",
    "    sjoin_sample,\n",
    "    demo_states,\n",
    "    how='left',\n",
    "    predicate='within'  # Point within polygon\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"‚úÖ Spatial join complete!\")\n",
    "print(f\"   Time: {elapsed:.2f}s\")\n",
    "print(f\"   Rate: {len(sjoin_sample)/elapsed:.0f} joins/sec\")\n",
    "\n",
    "# Show results\n",
    "print(f\"\\nüìä Sample Results:\")\n",
    "display(joined[['unified_id', 'centroid_lon', 'centroid_lat', \n",
    "                'name', 'state_code', 'area_m2']].head())\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nüìà Distribution by State:\")\n",
    "state_counts = joined['name'].value_counts()\n",
    "for state, count in state_counts.items():\n",
    "    if pd.notna(state):\n",
    "        print(f\"   {state}: {count:,} installations\")\n",
    "\n",
    "unmatched = joined['name'].isna().sum()\n",
    "if unmatched > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  Unmatched: {unmatched:,} ({unmatched/len(joined)*100:.1f}%)\")\n",
    "\n",
    "# Performance metrics\n",
    "print(f\"\\n‚ö° Performance Metrics:\")\n",
    "print(f\"   Total time: {elapsed:.2f}s\")\n",
    "print(f\"   Per-join: {elapsed*1000/len(sjoin_sample):.2f}ms\")\n",
    "print(f\"   Throughput: {len(sjoin_sample)/elapsed:.0f} joins/sec\")\n",
    "\n",
    "print(f\"\\n‚úÖ Advantages: Most accurate, uses exact boundaries\")\n",
    "print(f\"‚ö†Ô∏è  Limitations: Slow for large datasets, O(n*m) complexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497294e2",
   "metadata": {},
   "source": [
    "### 7.4: Performance Comparison Summary\n",
    "\n",
    "Let's compare all three methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fa8e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä PERFORMANCE COMPARISON SUMMARY\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    'Method': [\n",
    "        '1. Reverse Geocoding',\n",
    "        '2. H3 Index Matching',\n",
    "        '3. Spatial Join (sjoin)'\n",
    "    ],\n",
    "    'Speed': [\n",
    "        '~100-1000/sec',\n",
    "        '~10,000-100,000/sec',\n",
    "        '~100-1000/sec'\n",
    "    ],\n",
    "    'Accuracy': [\n",
    "        'Good (~3km)',\n",
    "        'Very Good (~cell size)',\n",
    "        'Exact'\n",
    "    ],\n",
    "    'Setup': [\n",
    "        'None (offline data)',\n",
    "        'Medium (build H3 index)',\n",
    "        'Low (download boundaries)'\n",
    "    ],\n",
    "    'Memory': [\n",
    "        'Low',\n",
    "        'Medium (H3 lookup table)',\n",
    "        'High (geometry objects)'\n",
    "    ],\n",
    "    'Best For': [\n",
    "        'Small datasets, quick lookups',\n",
    "        'Large datasets, repeated queries',\n",
    "        'Highest accuracy needed'\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nüí° Recommendations:\")\n",
    "print(\"\\n  ‚Ä¢ For < 10K points: Use Method 1 (Reverse Geocoding) or Method 3 (Spatial Join)\")\n",
    "print(\"  ‚Ä¢ For 10K - 1M points: Use Method 2 (H3 Index) for best performance\")\n",
    "print(\"  ‚Ä¢ For > 1M points: Definitely use Method 2 (H3 Index)\")\n",
    "print(\"  ‚Ä¢ For critical accuracy: Use Method 3 (Spatial Join) as ground truth\")\n",
    "\n",
    "print(\"\\nüîó For production implementation:\")\n",
    "print(\"  ‚Ä¢ Download Overture Maps admin boundaries:\")\n",
    "print(\"    https://docs.overturemaps.org/guides/divisions/\")\n",
    "print(\"  ‚Ä¢ See st_context_processing.py for full H3 implementation\")\n",
    "print(\"  ‚Ä¢ Consider caching H3 lookups for repeated analyses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7ad999",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üíæ Task 8: Exporting Results\n",
    "\n",
    "Save our filtered and processed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6344b5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ Exporting Processed Data\\n\")\n",
    "\n",
    "# Export California subset as GeoParquet\n",
    "output_dir = '/Volumes/Expanse/repos/ice-mELT_ducklake/notebooks'\n",
    "california_output = f'{output_dir}/01_california_solar_panels.parquet'\n",
    "\n",
    "print(f\"1Ô∏è‚É£ Exporting California data to GeoParquet...\")\n",
    "california_pv.to_parquet(california_output, index=False)\n",
    "file_size = os.path.getsize(california_output) / 1024**2\n",
    "print(f\"   ‚úÖ Saved: {california_output}\")\n",
    "print(f\"   Size: {file_size:.2f} MB\")\n",
    "print(f\"   Records: {len(california_pv):,}\")\n",
    "\n",
    "# Export summary statistics as CSV\n",
    "print(f\"\\n2Ô∏è‚É£ Exporting summary statistics...\")\n",
    "summary_stats = {\n",
    "    'metric': [\n",
    "        'Total Installations',\n",
    "        'Total Area (km¬≤)',\n",
    "        'Mean Area (m¬≤)',\n",
    "        'Median Area (m¬≤)',\n",
    "        'Data Sources',\n",
    "        'Latitude Range',\n",
    "        'Longitude Range',\n",
    "    ],\n",
    "    'value': [\n",
    "        f\"{len(california_pv):,}\",\n",
    "        f\"{california_pv['area_m2'].sum() / 1_000_000:.2f}\",\n",
    "        f\"{california_pv['area_m2'].mean():.2f}\",\n",
    "        f\"{california_pv['area_m2'].median():.2f}\",\n",
    "        f\"{california_pv['dataset_name'].nunique()}\",\n",
    "        f\"{california_pv['centroid_lat'].min():.2f} to {california_pv['centroid_lat'].max():.2f}\",\n",
    "        f\"{california_pv['centroid_lon'].min():.2f} to {california_pv['centroid_lon'].max():.2f}\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_stats)\n",
    "summary_output = f'{output_dir}/01_california_summary.csv'\n",
    "summary_df.to_csv(summary_output, index=False)\n",
    "print(f\"   ‚úÖ Saved: {summary_output}\")\n",
    "\n",
    "print(\"\\n‚úÖ All exports complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed369c5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Summary: What We Learned\n",
    "\n",
    "### Key Concepts Covered\n",
    "\n",
    "1. **Loading Geospatial Data**\n",
    "   - Reading GeoParquet from cloud storage (S3/R2)\n",
    "   - Using pandas + s3fs for data access\n",
    "   - Filtering data by bounding box and sampling\n",
    "\n",
    "2. **GeoPandas Fundamentals**\n",
    "   - Converting WKT strings to Shapely geometries\n",
    "   - Creating GeoDataFrames with proper CRS\n",
    "   - Understanding geometry types (Polygon, MultiPolygon)\n",
    "\n",
    "3. **Spatial Operations**\n",
    "   - Computing geometric properties (area, centroid, bounds)\n",
    "   - Working with different geometry types\n",
    "   - Coordinate Reference Systems (CRS)\n",
    "   - Transforming between projections\n",
    "\n",
    "4. **Spatial Analysis**\n",
    "   - Filtering by bounding box\n",
    "   - Testing spatial relationships (intersects, distance)\n",
    "   - Combining spatial and attribute filters\n",
    "\n",
    "5. **Exploratory Analysis**\n",
    "   - Distribution analysis (area, location)\n",
    "   - Visualization with matplotlib\n",
    "   - Summary statistics by region\n",
    "\n",
    "6. **Data Quality**\n",
    "   - Checking for missing values\n",
    "   - Validating geometries\n",
    "   - Assessing data completeness\n",
    "\n",
    "7. **Geocoding & Spatial Joins**\n",
    "   - Assigning countries/states to installations\n",
    "   - Comparing three methods:\n",
    "     * Reverse geocoding (offline-geocoder)\n",
    "     * H3 spatial indexing (h3-py)\n",
    "     * Direct spatial joins (GeoPandas sjoin)\n",
    "   - Performance benchmarking\n",
    "   - Accuracy vs speed tradeoffs\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In **Notebook 2**, we'll explore:\n",
    "- Interactive visualizations with Folium\n",
    "- Creating choropleth maps\n",
    "- Adding popups and tooltips\n",
    "- Layering multiple datasets\n",
    "- Export to HTML for web viewing\n",
    "\n",
    "### Resources for Further Learning\n",
    "\n",
    "- [GeoPandas Documentation](https://geopandas.org/en/stable/)\n",
    "- [Shapely User Manual](https://shapely.readthedocs.io/)\n",
    "- [Working with Geospatial Data in Python](https://geographicdata.science/book/)\n",
    "- [GeoParquet Specification](https://geoparquet.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09287a28",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "source": [
    "---\n",
    "\n",
    "## üìö Exercises for Practice\n",
    "\n",
    "Try these exercises to reinforce what you learned:\n",
    "\n",
    "### Exercise 1: Different Region\n",
    "Filter the dataset to a different region (e.g., Texas, Florida) and:\n",
    "- Count installations\n",
    "- Calculate total area\n",
    "- Create a scatter plot\n",
    "\n",
    "### Exercise 2: Size Categories\n",
    "Categorize panels by size:\n",
    "- Small: < 100 m¬≤\n",
    "- Medium: 100-1000 m¬≤\n",
    "- Large: 1000-10000 m¬≤\n",
    "- Utility: > 10000 m¬≤\n",
    "\n",
    "Calculate statistics for each category.\n",
    "\n",
    "### Exercise 3: Buffer Analysis\n",
    "For a major city of your choice:\n",
    "1. Create a point at the city center\n",
    "2. Create buffers of 25km, 50km, 100km\n",
    "3. Count installations in each buffer\n",
    "4. Plot the results\n",
    "\n",
    "### Exercise 4: Data Export\n",
    "Export the top 1000 largest installations as:\n",
    "- GeoJSON for web mapping\n",
    "- CSV with WKT geometries\n",
    "- Shapefile for GIS software"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
